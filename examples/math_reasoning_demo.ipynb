{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLAS Math Reasoning Demo\n",
    "\n",
    "This notebook demonstrates the ATLAS two-pass inference protocol improving math problem solving accuracy.\n",
    "\n",
    "## Overview\n",
    "\n",
    "ATLAS uses a teacher model to guide student models through:\n",
    "1. **Diagnostic Probing**: Teacher assesses student capability (~50 tokens)\n",
    "2. **Adaptive Learning**: Teacher provides targeted guidance based on assessment\n",
    "3. **Enhanced Response**: Student generates improved solution using guidance\n",
    "\n",
    "Expected improvement: **15.7% accuracy gain** with near-zero degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (for Google Colab)\nimport sys\nif 'google.colab' in sys.modules:\n    !pip install -q transformers torch accelerate datasets matplotlib pandas numpy\n    print(\"Packages installed for Google Colab\")\nelse:\n    print(\"Using local environment\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, HTML\nimport warnings\nimport json\nimport random\nimport re\nfrom typing import List, Dict, Any, Optional\nwarnings.filterwarnings('ignore')\n\n# Import ATLAS utilities\nfrom utils.atlas_inference import ATLASInference, load_atlas_models\nfrom utils.evaluation import calculate_metrics\nfrom utils.visualization import plot_comparison, display_results_table, show_example_comparisons\n\nprint(\"Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Configuration\nDEFAULT_STUDENT_MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\nDEFAULT_TEACHER_THINKING = \"Arc-Intelligence/ATLAS-8B-Thinking\"\nDEFAULT_TEACHER_INSTRUCT = \"Arc-Intelligence/ATLAS-8B-Instruct\"\n\n# Token Limits\nPROBE_TOKEN_LIMIT = 50  # Maximum tokens for diagnostic probing\nLEARNING_RESPONSE_LIMIT = 200  # Maximum tokens for adaptive learning guidance\nSTUDENT_RESPONSE_LIMIT = 300  # Maximum tokens for student responses\n\n# Capability Score Thresholds\nCAPABILITY_HIGH_THRESHOLD = 4  # Scores 4-5: Light intervention\nCAPABILITY_MEDIUM_THRESHOLD = 2  # Scores 2-3: Medium guidance\n# Score 1: Heavy support\n\n# Evaluation Settings\nDEGRADATION_PENALTY_MULTIPLIER = 2.0\nIMPROVEMENT_REWARD = 1.0\nNO_CHANGE_REWARD = 0.0\n\n# Memory Requirements\nMIN_GPU_MEMORY_GB = 12\nRECOMMENDED_GPU_MEMORY_GB = 16\n\n# Dataset Settings\nDEFAULT_NUM_SAMPLES = 20\nDEFAULT_DATASET_SPLIT = \"train\"\n\nprint(\"Configuration loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_atlas_teach_dataset(split: str = \"train\", num_samples: Optional[int] = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load Arc-Intelligence/Arc-ATLAS-Teach-v0 dataset.\"\"\"\n",
    "    print(\"Loading Arc-Intelligence/Arc-ATLAS-Teach-v0 dataset...\")\n",
    "    \n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        \n",
    "        # Download the RL training file\n",
    "        file_path = hf_hub_download(\n",
    "            repo_id=\"Arc-Intelligence/Arc-ATLAS-Teach-v0\",\n",
    "            filename=\"training/rl.jsonl\",\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        \n",
    "        # Load the JSONL file\n",
    "        problems = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    item = json.loads(line)\n",
    "                    \n",
    "                    problem_text = item.get(\"prompt\", \"\")\n",
    "                    ground_truth = item.get(\"ground_truth\", \"\")\n",
    "                    \n",
    "                    if problem_text:\n",
    "                        problem_dict = {\n",
    "                            \"problem\": problem_text,\n",
    "                            \"solution\": ground_truth,\n",
    "                            \"source\": \"Arc-ATLAS-Teach-v0\",\n",
    "                            \"problem_id\": item.get(\"problem_id\", \"\"),\n",
    "                            \"student_level\": item.get(\"student_level\", \"\"),\n",
    "                            \"baseline_score\": item.get(\"baseline_score\", 0),\n",
    "                            \"with_teaching_score\": item.get(\"with_teaching_score\", 0),\n",
    "                            \"teaching\": item.get(\"teaching\", \"\"),\n",
    "                            \"reward\": item.get(\"reward\", 0)\n",
    "                        }\n",
    "                        \n",
    "                        # Extract numerical answer\n",
    "                        if ground_truth:\n",
    "                            numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", str(ground_truth))\n",
    "                            if numbers:\n",
    "                                try:\n",
    "                                    problem_dict[\"answer\"] = float(numbers[-1])\n",
    "                                except:\n",
    "                                    pass\n",
    "                        \n",
    "                        problems.append(problem_dict)\n",
    "        \n",
    "        # Sample if requested\n",
    "        if num_samples and len(problems) > num_samples:\n",
    "            problems = random.sample(problems, num_samples)\n",
    "        \n",
    "        print(f\"Loaded {len(problems)} problems from Arc-ATLAS-Teach dataset\")\n",
    "        return problems\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Arc-ATLAS-Teach dataset: {e}\")\n",
    "        print(\"Falling back to sample problems...\")\n",
    "        return get_sample_math_problems()\n",
    "\n",
    "def get_sample_math_problems() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fallback sample math problems.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"problem\": \"Sarah has 24 apples. She gives 1/3 of them to her brother and 1/4 of the remaining apples to her sister. How many apples does Sarah have left?\",\n",
    "            \"answer\": 12,\n",
    "            \"solution\": \"Sarah starts with 24 apples. She gives 1/3 to her brother: 24 × 1/3 = 8 apples. Remaining: 24 - 8 = 16 apples. She gives 1/4 of remaining to her sister: 16 × 1/4 = 4 apples. Final amount: 16 - 4 = 12 apples.\",\n",
    "            \"source\": \"sample\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"A train travels 120 miles in 2 hours. If it maintains the same speed, how far will it travel in 5 hours?\",\n",
    "            \"answer\": 300,\n",
    "            \"solution\": \"Speed = Distance ÷ Time = 120 miles ÷ 2 hours = 60 miles per hour. Distance in 5 hours = Speed × Time = 60 mph × 5 hours = 300 miles.\",\n",
    "            \"source\": \"sample\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"The sum of two consecutive even numbers is 46. What are the two numbers?\",\n",
    "            \"answer\": \"22 and 24\",\n",
    "            \"solution\": \"Let the first even number be x. The next consecutive even number is x + 2. Sum: x + (x + 2) = 46. Solving: 2x + 2 = 46, so 2x = 44, and x = 22. The two numbers are 22 and 24.\",\n",
    "            \"source\": \"sample\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load math problems\nprint(\"Loading math problems...\\n\")\n\ntry:\n    # Try Arc-ATLAS-Teach dataset first\n    problems = load_atlas_teach_dataset(num_samples=DEFAULT_NUM_SAMPLES)\n    print(f\"Loaded {len(problems)} problems\")\nexcept Exception as e:\n    print(f\"Dataset loading failed: {e}\")\n    print(\"Using sample problems...\")\n    problems = get_sample_math_problems()\n\n# Display sample problems\nprint(f\"\\nSample problem:\")\nprint(f\"Problem: {problems[0]['problem'][:200]}...\")\nprint(f\"Answer: {problems[0].get('answer', 'N/A')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {gpu_memory:.1f} GB\")\n    \n    if gpu_memory < MIN_GPU_MEMORY_GB:\n        print(f\"Warning: GPU memory ({gpu_memory:.1f} GB) below recommended {MIN_GPU_MEMORY_GB} GB\")\n        print(\"   Consider using 8-bit quantization or smaller models\")\nelse:\n    print(\"No GPU detected. Using CPU (will be slower)\")\n    print(\"   For better performance, use Google Colab with GPU runtime\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load models with ATLAS wrapper\nprint(\"\\nLoading models...\")\nprint(f\"Student: {DEFAULT_STUDENT_MODEL}\")\nprint(f\"Teacher: {DEFAULT_TEACHER_THINKING}\\n\")\n\ntry:\n    atlas, models = load_atlas_models(\n        student_model_name=DEFAULT_STUDENT_MODEL,\n        teacher_thinking_name=DEFAULT_TEACHER_THINKING,\n        device=device,\n        load_in_8bit=(gpu_memory < RECOMMENDED_GPU_MEMORY_GB) if device == \"cuda\" else False\n    )\n    print(\"Models loaded successfully\")\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Check internet connection\")\n    print(\"2. Verify HuggingFace access (some models require authentication)\")\n    print(\"3. Try with smaller models or enable 8-bit quantization\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ATLAS Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run inference on all problems\nresults = []\nprint(\"\\nRunning ATLAS protocol on problems...\\n\")\n\nfor i, problem in enumerate(problems[:5]):  # Run on first 5 for demo\n    print(f\"Problem {i+1}/{min(5, len(problems))}...\")\n    \n    try:\n        # Run full ATLAS protocol\n        result = atlas.run_full_protocol(\n            problem[\"problem\"],\n            ground_truth=problem.get(\"answer\"),\n            max_student_tokens=STUDENT_RESPONSE_LIMIT\n        )\n        \n        # Store results\n        result[\"problem_id\"] = i\n        result[\"ground_truth\"] = problem.get(\"answer\")\n        results.append(result)\n        \n        # Show improvement\n        if result.get(\"improvement_category\") == \"improved\":\n            print(f\"  Improved: {result.get('baseline_correct', False)} -> {result.get('guided_correct', True)}\")\n        elif result.get(\"improvement_category\") == \"degraded\":\n            print(f\"  Degraded: {result.get('baseline_correct', True)} -> {result.get('guided_correct', False)}\")\n        else:\n            print(f\"  No change\")\n            \n    except Exception as e:\n        print(f\"  Error: {e}\")\n        continue\n\nprint(f\"\\nCompleted {len(results)} problems\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate metrics\nif results:\n    metrics = calculate_metrics(results)\n    \n    print(\"\\nPerformance Summary:\")\n    print(\"=\" * 50)\n    print(f\"Baseline Accuracy: {metrics['baseline_accuracy']:.1%}\")\n    print(f\"With ATLAS:        {metrics['guided_accuracy']:.1%}\")\n    print(f\"Improvement:       {metrics['improvement_rate']:.1%}\")\n    print(f\"Degradation:       {metrics['degradation_rate']:.1%}\")\n    print(f\"Non-degradation:   {metrics['non_degradation_rate']:.1%}\")\n    print(\"=\" * 50)\n    \n    # Visualize results\n    plot_comparison(results, metrics)\nelse:\n    print(\"No results to analyze\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed comparisons for improved cases\n",
    "if results:\n",
    "    show_example_comparisons(results, problems, n_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with your own problem\ndef test_custom_problem(problem_text: str):\n    \"\"\"Test ATLAS with a custom math problem.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing custom problem\")\n    print(\"=\"*60)\n    print(f\"\\nProblem: {problem_text}\\n\")\n    \n    result = atlas.run_full_protocol(problem_text)\n    \n    print(\"\\nStudent Response (Alone):\")\n    print(\"-\" * 40)\n    print(result['baseline_response'])\n    \n    print(\"\\nTeacher Guidance:\")\n    print(\"-\" * 40)\n    print(f\"Strategy: {result['learning']['strategy']}\")\n    print(f\"Guidance: {result['learning']['response'][:200]}...\")\n    \n    print(\"\\nStudent Response (With ATLAS):\")\n    print(\"-\" * 40)\n    print(result['guided_response'])\n    \n    return result\n\n# Example usage\ncustom_problem = \"A store offers a 20% discount on all items. If a jacket originally costs $80, how much will it cost after the discount?\"\ncustom_result = test_custom_problem(custom_problem)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze token usage\nif results:\n    total_probe_tokens = sum(r.get('probe', {}).get('tokens_used', 0) for r in results)\n    total_learning_tokens = sum(r.get('learning', {}).get('tokens_used', 0) for r in results)\n    total_baseline_tokens = sum(r.get('baseline_tokens', 0) for r in results)\n    total_guided_tokens = sum(r.get('guided_tokens', 0) for r in results)\n    \n    avg_probe = total_probe_tokens / len(results)\n    avg_learning = total_learning_tokens / len(results)\n    avg_overhead = avg_probe + avg_learning\n    \n    print(\"\\nToken Efficiency:\")\n    print(\"=\" * 50)\n    print(f\"Average probe tokens:     {avg_probe:.0f}\")\n    print(f\"Average learning tokens:  {avg_learning:.0f}\")\n    print(f\"Total teacher overhead:   {avg_overhead:.0f}\")\n    print(f\"\\nBaseline response avg:    {total_baseline_tokens/len(results):.0f}\")\n    print(f\"Guided response avg:      {total_guided_tokens/len(results):.0f}\")\n    print(\"=\" * 50)\n    \n    efficiency_ratio = avg_overhead / (total_guided_tokens/len(results))\n    print(f\"\\nEfficiency ratio: {efficiency_ratio:.1%} overhead for {metrics['improvement_rate']:.1%} gain\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results for further analysis\nif results:\n    output_file = \"atlas_math_results.json\"\n    with open(output_file, 'w') as f:\n        json.dump({\n            \"metrics\": metrics,\n            \"results\": results,\n            \"config\": {\n                \"student_model\": DEFAULT_STUDENT_MODEL,\n                \"teacher_model\": DEFAULT_TEACHER_THINKING,\n                \"num_problems\": len(results)\n            }\n        }, f, indent=2)\n    \n    print(f\"\\nResults saved to {output_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demo showed how ATLAS improves math problem solving through:\n",
    "\n",
    "1. **Diagnostic Probing**: Teacher assesses student capability in ~50 tokens\n",
    "2. **Adaptive Teaching**: Conditional guidance based on diagnosed strength\n",
    "3. **Performance Gains**: ~15.7% accuracy improvement with minimal overhead\n",
    "\n",
    "### Key Takeaways\n",
    "- ATLAS works with any student model (4B-8B parameters)\n",
    "- Teacher overhead is minimal (~250 tokens total)\n",
    "- Non-degradation rate of 97% ensures reliability\n",
    "- Drop-in replacement for existing inference pipelines\n",
    "\n",
    "### Next Steps\n",
    "- Try with your own student models\n",
    "- Test on different problem types\n",
    "- Fine-tune teacher models for specific domains\n",
    "- Deploy in production with the vLLM server\n",
    "\n",
    "For more information, see the [main repository](https://github.com/Arc-Intelligence/RCL)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}