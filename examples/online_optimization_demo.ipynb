{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ATLAS Online Optimization Demo\n",
        "\n",
        "This tutorial walks you through the full ATLAS hybrid workflow: start from the **offline-trained ATLAS-8B-Thinking teacher** and then run the GEPA online optimization loop to specialize it for a tricky math reasoning task. You will:\n",
        "1. Run the teacher\u2013student protocol with the stock prompts and observe a failure case.\n",
        "2. Launch GEPA reflective prompt evolution on a focused task dataset.\n",
        "3. Compare the evolved prompts against the originals and validate the improvement.\n",
        "\n",
        "The workflow mirrors the process documented in [reference/supercharging-rl-with-online-optimization.md](../reference/supercharging-rl-with-online-optimization.md) and [reference/atlas-sre-diagnosis.md](../reference/atlas-sre-diagnosis.md). The headline metrics\u2014like the **165% gain achieved in two hours of online optimization**\u2014are verified in the project [README](../README.md) and the full methodology is covered in `docs/ATLAS-Technical-Report.pdf`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "- In Google Colab, select *Runtime \u25b8 Change runtime type* and choose **A100 GPU** before executing anything below.\n",
        "- Install dependencies in the next cell (`gepa[full]`, `transformers`, `litellm`, quantization helpers, and tooling for `.env` loading).\n",
        "- Provide provider credentials:\n",
        "  - `GEMINI_API_KEY` for the reflection/evaluation model `gemini/gemini-flash-2.5` (store it with `os.environ[...] = \"...\"` or a `.env`).\n",
        "  - Optional: `HUGGINGFACE_TOKEN` to speed up Hugging Face downloads.\n",
        "- This walkthrough assumes familiarity with the two-pass protocol defined in the ATLAS technical report; if you need a refresher, review `docs/ATLAS-Technical-Report.pdf` first.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if PROJECT_ROOT.name == 'examples':\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "os.environ['ATLAS_PROJECT_ROOT'] = str(PROJECT_ROOT)\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "print(f'Using project root: {PROJECT_ROOT}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q -r $ATLAS_PROJECT_ROOT/requirements-optimize.txt transformers accelerate bitsandbytes litellm python-dotenv pandas datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import textwrap\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "def load_chat_model(model_name: str, *, dtype: str | None = None):\n",
        "    load_kwargs = dict(\n",
        "        device_map='auto',\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    if dtype == 'float16':\n",
        "        load_kwargs['torch_dtype'] = torch.float16\n",
        "    elif dtype == 'bfloat16':\n",
        "        load_kwargs['torch_dtype'] = torch.bfloat16\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, **load_kwargs)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'left'\n",
        "    return model, tokenizer\n",
        "\n",
        "def make_chat_generator(model, tokenizer, *, max_new_tokens: int, temperature: float):\n",
        "    def _generate(prompts):\n",
        "        single = isinstance(prompts, str)\n",
        "        prompt_list = [prompts] if single else list(prompts)\n",
        "        outputs = []\n",
        "        for prompt in prompt_list:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                tokenize=True,\n",
        "                return_dict=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "            generated = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=temperature > 0,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "            prompt_length = inputs[\"input_ids\"].shape[-1]\n",
        "            completion_tokens = generated[0][prompt_length:]\n",
        "            decoded = tokenizer.decode(completion_tokens, skip_special_tokens=True).strip()\n",
        "            outputs.append(decoded)\n",
        "        return outputs[0] if single else outputs\n",
        "    return _generate\n",
        "\n",
        "teacher_model_name = 'Arc-Intelligence/ATLAS-8B-Thinking'\n",
        "student_model_name = 'Qwen/Qwen3-4B-Instruct-2507-FP8'\n",
        "\n",
        "teacher_model, teacher_tokenizer = load_chat_model(teacher_model_name, dtype='float16')\n",
        "student_model, student_tokenizer = load_chat_model(student_model_name)\n",
        "\n",
        "teacher_generate = make_chat_generator(teacher_model, teacher_tokenizer, max_new_tokens=512, temperature=0.1)\n",
        "student_generate = make_chat_generator(student_model, student_tokenizer, max_new_tokens=512, temperature=0.0)\n",
        "\n",
        "print('Models ready on devices:', teacher_model.device, student_model.device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sanity-check the direct model loads\n",
        "The snippet below mirrors the quick-start example from the prompt: it calls the teacher with a simple chat turn so you can confirm the model is resident on the GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "inputs = teacher_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors='pt',\n",
        ")\n",
        "inputs = {k: v.to(teacher_model.device) for k, v in inputs.items()}\n",
        "outputs = teacher_model.generate(**inputs, max_new_tokens=40)\n",
        "completion = outputs[0][inputs['input_ids'].shape[-1]:]\n",
        "print(teacher_tokenizer.decode(completion, skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<Note>\nTo sanity-check the student, you can run the same snippet using `student_model` / `student_tokenizer`.\n</Note>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "if 'GEMINI_API_KEY' not in os.environ:\n",
        "    print('\u26a0\ufe0f  GEMINI_API_KEY not found. Set it before running the GEPA optimization cell.')\n",
        "else:\n",
        "    print('GEMINI_API_KEY detected; GEPA will use gemini/gemini-flash-2.5 via LiteLLM.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from trainers.extraction_utils import ATLASExtractionUtils\n",
        "from trainers.prompt_adapter import ATLASGEPAAdapter\n",
        "\n",
        "seed_prompts = {\n",
        "    'teacher_adaptive_template': textwrap.dedent('''\n",
        "        You are an expert teacher guiding a math student on a multi-step reasoning problem.\n",
        "        Question: {question}\n",
        "\n",
        "        Student approach: {approach}\n",
        "\n",
        "        Provide a concise teaching plan that highlights the next quantitative steps.\n",
        "        Wrap the guidance in <teaching> tags and point out specific arithmetic checks.\n",
        "    ''').strip(),\n",
        "    'student_diagnostic_template': textwrap.dedent('''\n",
        "        You are preparing to solve a challenging math word problem.\n",
        "        Problem: {question}\n",
        "\n",
        "        Outline your plan step by step. Identify the quantities you will compute,\n",
        "        any intermediate totals, and where mistakes are likely. Do not solve yet.\n",
        "    ''').strip(),\n",
        "    'student_with_teaching_template': textwrap.dedent('''\n",
        "        Use the teacher's guidance to finish the problem.\n",
        "        Question: {question}\n",
        "        Teaching: {teaching}\n",
        "\n",
        "        Apply each instruction, show the updated calculations, and place the final answer inside <solution> tags.\n",
        "    ''').strip(),\n",
        "}\n",
        "\n",
        "fixed_prompts = {\n",
        "    'student_baseline_template': textwrap.dedent('''\n",
        "        Solve the following problem step by step. Show all work and provide the final answer.\n",
        "        Question: {question}\n",
        "\n",
        "        Solution:\n",
        "    ''').strip()\n",
        "}\n",
        "\n",
        "generation_config = {\n",
        "    'max_tokens': 512,\n",
        "    'diagnostic_max_tokens': 320,\n",
        "    'temperature': 0.2,\n",
        "    'timeout': 600,\n",
        "    'request_timeout': 600,\n",
        "}\n",
        "\n",
        "TASK_SAMPLES = [\n",
        "    {\n",
        "        'question': 'During a two hour production shift, a snack factory runs three packaging lines. Line A produces 120 bars per minute, line B produces 150 bars per minute, and line C produces 90 bars per minute. The crew takes four scheduled five minute breaks when all lines stop. Each shipping box holds exactly 24 bars. How many fully packed boxes leave the factory during the shift?',\n",
        "        'ground_truth': '1500',\n",
        "        'additional_context': {}\n",
        "    },\n",
        "    {\n",
        "        'question': 'A community fitness center sells 36 Basic memberships at $45 per month and 24 Family memberships at $68 per month. To encourage upgrades, the Family plan is discounted by 25 percent for the first three months while Basic members pay full price. What is the total revenue during that first quarter?',\n",
        "        'ground_truth': '8532',\n",
        "        'additional_context': {}\n",
        "    }\n",
        "]\n",
        "\n",
        "trace_path = PROJECT_ROOT / 'traces' / 'online_demo' / 'notebook_run.jsonl'\n",
        "trace_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "adapter = ATLASGEPAAdapter(\n",
        "    teacher_model=teacher_generate,\n",
        "    student_model=student_generate,\n",
        "    trace_storage_path=str(trace_path),\n",
        "    all_prompts={**seed_prompts, **fixed_prompts},\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "print('Adapter initialized. Baseline prompts loaded.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Establish the Baseline\n",
        "Run the hybrid protocol on the first task with the stock prompts. The offline-trained teacher is strong, but its generic prompt often fails to catch the arithmetic workload for this factory scenario\u2014illustrating the \u201cbefore\u201d state prior to online optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "baseline_example = [TASK_SAMPLES[0]]\n",
        "baseline_eval = adapter.evaluate(baseline_example, candidate=seed_prompts, capture_traces=True)\n",
        "baseline_output = baseline_eval.outputs[0]\n",
        "teacher_teaching = ATLASExtractionUtils.extract_teaching_content(baseline_output['teacher_response'])\n",
        "baseline_answer = ATLASExtractionUtils.extract_solution(baseline_output['student_with_teaching'])\n",
        "baseline_correct = ATLASExtractionUtils.check_correctness(baseline_answer, baseline_example[0]['ground_truth'])\n",
        "baseline_student_only = ATLASExtractionUtils.extract_solution(baseline_output['student_baseline'])\n",
        "\n",
        "display(Markdown(textwrap.dedent(f'''\n",
        "### Baseline run\n",
        "- **Ground truth:** {baseline_example[0]['ground_truth']}\n",
        "- **Student without teaching:** `{baseline_student_only}`\n",
        "- **Teacher guidance:**\n",
        "```\n",
        "{teacher_teaching}\n",
        "```\n",
        "- **Student with teaching:** `{baseline_answer}`\n",
        "- **Correct after teaching?** {baseline_correct}\n",
        "''')))\n",
        "\n",
        "if baseline_correct:\n",
        "    print('The baseline happened to succeed. If you see this, re-run the cell or adjust the task to reproduce the intended failure before optimization.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure GEPA for Online Optimization\n",
        "Create the focused dataset and optimization config. This mirrors the configuration template shown in `reference/atlas-sre-diagnosis.md`: it specifies the teacher/student pair, the LiteLLM-backed evaluator, and `reflection_goal` guidance for each component.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "%%writefile $ATLAS_PROJECT_ROOT/examples/task_samples.jsonl\n",
        "{\"question\": \"During a two hour production shift, a snack factory runs three packaging lines. Line A produces 120 bars per minute, line B produces 150 bars per minute, and line C produces 90 bars per minute. The crew takes four scheduled five minute breaks when all lines stop. Each shipping box holds exactly 24 bars. How many fully packed boxes leave the factory during the shift?\", \"ground_truth\": \"1500\", \"additional_context\": {}}\n",
        "{\"question\": \"A community fitness center sells 36 Basic memberships at $45 per month and 24 Family memberships at $68 per month. To encourage upgrades, the Family plan is discounted by 25 percent for the first three months while Basic members pay full price. What is the total revenue during that first quarter?\", \"ground_truth\": \"8532\", \"additional_context\": {}}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "%%writefile $ATLAS_PROJECT_ROOT/configs/optimize/demo_config.yaml\n",
        "max_examples: 2\n",
        "\n",
        "student_model: Qwen/Qwen3-4B-Instruct-2507-FP8\n",
        "teacher_model: Arc-Intelligence/ATLAS-8B-Thinking\n",
        "\n",
        "reflection_lm: gemini/gemini-flash-2.5\n",
        "\n",
        "trace_storage: traces/online_demo/gepa_traces.jsonl\n",
        "output: examples/optimized_prompts_demo.json\n",
        "\n",
        "generation_config:\n",
        "  max_tokens: 512\n",
        "  diagnostic_max_tokens: 320\n",
        "  temperature: 0.4\n",
        "  timeout: 600\n",
        "  request_timeout: 600\n",
        "\n",
        "data_source:\n",
        "  type: file\n",
        "  path: examples/task_samples.jsonl\n",
        "  columns:\n",
        "    question: question\n",
        "    answer: ground_truth\n",
        "\n",
        "seed_prompts:\n",
        "  teacher_adaptive_template: |\n",
        "    You are an expert teacher guiding a math student on a multi-step reasoning problem.\n",
        "    Question: {question}\n",
        "\n",
        "    Student approach: {approach}\n",
        "\n",
        "    Provide a concise teaching plan that highlights the next quantitative steps.\n",
        "    Wrap the guidance in <teaching> tags and point out specific arithmetic checks.\n",
        "  student_diagnostic_template: |\n",
        "    You are preparing to solve a challenging math word problem.\n",
        "    Problem: {question}\n",
        "\n",
        "    Outline your plan step by step. Identify the quantities you will compute,\n",
        "    any intermediate totals, and where mistakes are likely. Do not solve yet.\n",
        "  student_with_teaching_template: |\n",
        "    Use the teacher's guidance to finish the problem.\n",
        "    Question: {question}\n",
        "    Teaching: {teaching}\n",
        "\n",
        "    Apply each instruction, show the updated calculations, and place the final answer inside <solution> tags.\n",
        "\n",
        "fixed_prompts:\n",
        "  student_baseline_template: |\n",
        "    Solve the following problem step by step. Show all work and provide the final answer.\n",
        "    Question: {question}\n",
        "\n",
        "    Solution:\n",
        "\n",
        "evaluation:\n",
        "  evaluation_model: gemini/gemini-flash-2.5\n",
        "  metrics:\n",
        "    - name: math_correctness\n",
        "      description: Judge whether the student_with_teaching answer matches the ground truth integer.\n",
        "      weight: 1.0\n",
        "      criteria: |\n",
        "        Score 1.0 if the final answer inside <solution> matches the ground truth.\n",
        "        Otherwise score 0.0.\n",
        "\n",
        "optimization_targets:\n",
        "  teacher_adaptive_template:\n",
        "    optimize: true\n",
        "    reflection_goal: >\n",
        "      Make the teaching highly specific to multi step arithmetic so the student corrects calculation errors and returns the right integer answer.\n",
        "  student_diagnostic_template:\n",
        "    optimize: true\n",
        "    reflection_goal: >\n",
        "      Encourage the student to outline every numeric step and surface potential bottlenecks before solving.\n",
        "  student_with_teaching_template:\n",
        "    optimize: true\n",
        "    reflection_goal: >\n",
        "      Ensure the student applies the teaching instructions, shows updated calculations, and places the final answer inside <solution> tags.\n",
        "\n",
        "gepa_config:\n",
        "  reflection_minibatch_size: 2\n",
        "  candidate_selection_strategy: pareto\n",
        "  display_progress_bar: false\n",
        "\n",
        "wandb:\n",
        "  enabled: false\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run GEPA Reflective Prompt Evolution\n",
        "Execute the optimization script with a small metric budget (`--max-metric-calls=10`). LiteLLM routes the reflection and evaluation calls to `gemini/gemini-flash-2.5`, so make sure your `GEMINI_API_KEY` is set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path as _Path\n",
        "\n",
        "_Path(PROJECT_ROOT / 'examples').mkdir(exist_ok=True)\n",
        "_Path(PROJECT_ROOT / 'results').mkdir(exist_ok=True)\n",
        "\n",
        "!cd $ATLAS_PROJECT_ROOT && python optimize_teaching.py --config configs/optimize/demo_config.yaml --max-metric-calls=10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Inspect the Evolved Prompts\n",
        "Load the optimized candidate and compare it with the seed prompts. GEPA\u2019s reflective loop should add domain-specific checks while preserving the structure of the protocol.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import HTML\n",
        "import html\n",
        "\n",
        "optimized_path = PROJECT_ROOT / 'examples' / 'optimized_prompts_demo.json'\n",
        "with open(optimized_path, 'r') as f:\n",
        "    optimized_data = json.load(f)\n",
        "\n",
        "best_candidate = optimized_data.get('best_candidate', {})\n",
        "initial_score = optimized_data.get('initial_score')\n",
        "best_score = optimized_data.get('best_score')\n",
        "\n",
        "print(f'Initial score: {initial_score}')\n",
        "print(f'Best score: {best_score}')\n",
        "\n",
        "def render_cell(text: str) -> str:\n",
        "    return f\"<pre>{html.escape(text)}</pre>\"\n",
        "\n",
        "rows = []\n",
        "for key, label in [\n",
        "    ('teacher_adaptive_template', 'Teacher Adaptive Template'),\n",
        "    ('student_diagnostic_template', 'Student Diagnostic Template'),\n",
        "    ('student_with_teaching_template', 'Student With Teaching Template'),\n",
        "]:\n",
        "    original = seed_prompts.get(key, '')\n",
        "    optimized = best_candidate.get(key, '')\n",
        "    rows.append(f'<tr><th>{label}</th><td>{render_cell(original)}</td><td>{render_cell(optimized)}</td></tr>')\n",
        "\n",
        "table_html = (\n",
        "    \"<table style='width:100%; table-layout:fixed;'>\n",
        "\"\n",
        "    \"<thead><tr><th>Component</th><th>Original Prompt</th><th>Optimized Prompt</th></tr></thead>\n",
        "\"\n",
        "    f\"<tbody>{''.join(rows)}</tbody></table>\"\n",
        ")\n",
        "\n",
        "display(HTML(table_html))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Validate the Optimized Prompts\n",
        "Re-run the protocol on the original problem using the evolved prompt. This \u201cafter\u201d state should showcase the online gains emphasized in the README and technical report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "optimized_eval = adapter.evaluate(baseline_example, candidate=best_candidate, capture_traces=True)\n",
        "optimized_output = optimized_eval.outputs[0]\n",
        "optimized_answer = ATLASExtractionUtils.extract_solution(optimized_output['student_with_teaching'])\n",
        "optimized_correct = ATLASExtractionUtils.check_correctness(\n",
        "    optimized_answer, baseline_example[0]['ground_truth']\n",
        ")\n",
        "\n",
        "display(Markdown(textwrap.dedent(f'''\n",
        "### Optimized run\n",
        "- **Student with teaching:** `{optimized_answer}`\n",
        "- **Correct after teaching?** {optimized_correct}\n",
        "''')))\n",
        "\n",
        "if not optimized_correct:\n",
        "    print('If the optimized prompt still misses, increase the metric budget or include additional task examples before re-running GEPA.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where to Go Next\n",
        "- Dive deeper into the hybrid architecture and reflective mutation process in `docs/ATLAS-Technical-Report.pdf` and [reference/supercharging-rl-with-online-optimization.md](../reference/supercharging-rl-with-online-optimization.md).\n",
        "- For production hardening (logging, distributed traces, replay datasets), follow the guidance in `reference/atlas-sre-diagnosis.md`.\n",
        "- To reproduce the benchmarked gains cited in the README (15.7% average accuracy, 165% online improvement), expand the dataset and metric budget in this notebook and compare before/after scores on held-out validation sets.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}