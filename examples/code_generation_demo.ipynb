{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLAS Code Generation Demo\n",
    "\n",
    "This notebook demonstrates ATLAS improving code generation and explanation quality.\n",
    "\n",
    "## Overview\n",
    "\n",
    "ATLAS enhances code generation through:\n",
    "1. **Diagnostic Assessment**: Teacher evaluates student's coding capability\n",
    "2. **Adaptive Guidance**: Tailored instruction based on coding skill level\n",
    "3. **Enhanced Generation**: Student produces better code with teacher support\n",
    "\n",
    "Expected improvements: Better code quality, clearer explanations, and more complete solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (for Google Colab)\nimport sys\nif 'google.colab' in sys.modules:\n    !pip install -q transformers torch accelerate datasets matplotlib pandas numpy\n    print(\"Packages installed for Google Colab\")\nelse:\n    print(\"Using local environment\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display, HTML, Markdown\nimport warnings\nimport json\nimport random\nimport re\nfrom typing import List, Dict, Any, Optional\nwarnings.filterwarnings('ignore')\n\n# Import ATLAS utilities\nfrom utils.atlas_inference import ATLASInference, load_atlas_models\nfrom utils.evaluation import evaluate_code_quality, calculate_code_metrics\nfrom utils.visualization import plot_code_metrics, display_code_comparison\n\nprint(\"Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Configuration\nDEFAULT_STUDENT_MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\nDEFAULT_TEACHER_THINKING = \"Arc-Intelligence/ATLAS-8B-Thinking\"\nDEFAULT_TEACHER_INSTRUCT = \"Arc-Intelligence/ATLAS-8B-Instruct\"\n\n# Token Limits\nPROBE_TOKEN_LIMIT = 50  # Maximum tokens for diagnostic probing\nLEARNING_RESPONSE_LIMIT = 200  # Maximum tokens for adaptive learning guidance\nSTUDENT_RESPONSE_LIMIT = 500  # Higher limit for code generation\n\n# Capability Score Thresholds\nCAPABILITY_HIGH_THRESHOLD = 4  # Scores 4-5: Light intervention\nCAPABILITY_MEDIUM_THRESHOLD = 2  # Scores 2-3: Medium guidance\n\n# Memory Requirements\nMIN_GPU_MEMORY_GB = 12\nRECOMMENDED_GPU_MEMORY_GB = 16\n\n# Dataset Settings\nDEFAULT_NUM_SAMPLES = 15\n\nprint(\"Configuration loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code_problems(num_samples: Optional[int] = 15) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load coding problems for code generation demo.\"\"\"\n",
    "    print(\"Loading coding problems...\")\n",
    "    \n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        # Try loading from HumanEval dataset\n",
    "        dataset = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "        \n",
    "        problems = []\n",
    "        for item in dataset:\n",
    "            problem_dict = {\n",
    "                \"problem\": item.get(\"prompt\", \"\"),\n",
    "                \"expected_behavior\": item.get(\"docstring\", \"\"),\n",
    "                \"test_cases\": item.get(\"test\", \"\"),\n",
    "                \"canonical_solution\": item.get(\"canonical_solution\", \"\"),\n",
    "                \"source\": \"HumanEval\"\n",
    "            }\n",
    "            problems.append(problem_dict)\n",
    "        \n",
    "        # Sample if requested\n",
    "        if num_samples and len(problems) > num_samples:\n",
    "            problems = random.sample(problems, num_samples)\n",
    "        \n",
    "        print(f\"Loaded {len(problems)} coding problems\")\n",
    "        return problems\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading coding dataset: {e}\")\n",
    "        print(\"Using sample coding problems...\")\n",
    "        return get_sample_code_problems()\n",
    "\n",
    "def get_sample_code_problems() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fallback sample coding problems.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"problem\": \"Write a function that returns the factorial of a positive integer n.\",\n",
    "            \"expected_behavior\": \"factorial(5) should return 120, factorial(0) should return 1\",\n",
    "            \"canonical_solution\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\",\n",
    "            \"source\": \"sample\",\n",
    "            \"difficulty\": \"easy\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Write a function that checks if a string is a palindrome (reads the same forwards and backwards).\",\n",
    "            \"expected_behavior\": \"is_palindrome('racecar') should return True, is_palindrome('hello') should return False\",\n",
    "            \"canonical_solution\": \"def is_palindrome(s):\\n    s = s.lower().replace(' ', '')\\n    return s == s[::-1]\",\n",
    "            \"source\": \"sample\",\n",
    "            \"difficulty\": \"easy\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Write a function that finds the longest common subsequence of two strings.\",\n",
    "            \"expected_behavior\": \"lcs('ABCDGH', 'AEDFHR') should return 'ADH', lcs('AGGTAB', 'GXTXAYB') should return 'GTAB'\",\n",
    "            \"canonical_solution\": \"def lcs(X, Y):\\n    m, n = len(X), len(Y)\\n    L = [[0] * (n + 1) for _ in range(m + 1)]\\n    \\n    for i in range(m + 1):\\n        for j in range(n + 1):\\n            if i == 0 or j == 0:\\n                L[i][j] = 0\\n            elif X[i-1] == Y[j-1]:\\n                L[i][j] = L[i-1][j-1] + 1\\n            else:\\n                L[i][j] = max(L[i-1][j], L[i][j-1])\\n    \\n    # Reconstruct LCS\\n    i, j = m, n\\n    lcs_str = []\\n    while i > 0 and j > 0:\\n        if X[i-1] == Y[j-1]:\\n            lcs_str.append(X[i-1])\\n            i -= 1\\n            j -= 1\\n        elif L[i-1][j] > L[i][j-1]:\\n            i -= 1\\n        else:\\n            j -= 1\\n    \\n    return ''.join(reversed(lcs_str))\",\n",
    "            \"source\": \"sample\",\n",
    "            \"difficulty\": \"hard\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load coding problems\nprint(\"Loading coding problems...\\n\")\n\ntry:\n    problems = load_code_problems(num_samples=DEFAULT_NUM_SAMPLES)\n    print(f\"Loaded {len(problems)} coding problems\")\nexcept Exception as e:\n    print(f\"Dataset loading failed: {e}\")\n    print(\"Using sample problems...\")\n    problems = get_sample_code_problems()\n\n# Display sample problem\nprint(f\"\\nSample problem:\")\nprint(f\"Problem: {problems[0]['problem'][:200]}...\")\nprint(f\"Expected: {problems[0].get('expected_behavior', 'N/A')[:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {gpu_memory:.1f} GB\")\n    \n    if gpu_memory < MIN_GPU_MEMORY_GB:\n        print(f\"Warning: GPU memory ({gpu_memory:.1f} GB) below recommended {MIN_GPU_MEMORY_GB} GB\")\n        print(\"   Consider using 8-bit quantization or smaller models\")\nelse:\n    print(\"No GPU detected. Using CPU (will be slower)\")\n    print(\"   For better performance, use Google Colab with GPU runtime\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load models with ATLAS wrapper\nprint(\"\\nLoading models...\")\nprint(f\"Student: {DEFAULT_STUDENT_MODEL}\")\nprint(f\"Teacher: {DEFAULT_TEACHER_INSTRUCT}\\n\")\n\ntry:\n    # For code generation, use ATLAS-8B-Instruct\n    atlas, models = load_atlas_models(\n        student_model_name=DEFAULT_STUDENT_MODEL,\n        teacher_thinking_name=DEFAULT_TEACHER_INSTRUCT,  # Using Instruct for code\n        device=device,\n        load_in_8bit=(gpu_memory < RECOMMENDED_GPU_MEMORY_GB) if device == \"cuda\" else False\n    )\n    print(\"Models loaded successfully\")\nexcept Exception as e:\n    print(f\"Error loading models: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Check internet connection\")\n    print(\"2. Verify HuggingFace access\")\n    print(\"3. Try with smaller models or enable 8-bit quantization\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ATLAS Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run inference on coding problems\nresults = []\nprint(\"\\nRunning ATLAS protocol on coding problems...\\n\")\n\nfor i, problem in enumerate(problems[:3]):  # Run on first 3 for demo\n    print(f\"Problem {i+1}/{min(3, len(problems))}...\")\n    \n    try:\n        # Format problem for code generation\n        prompt = f\"{problem['problem']}\\n\\nExpected behavior: {problem.get('expected_behavior', '')}\\n\\nPlease provide a Python implementation.\"\n        \n        # Run full ATLAS protocol\n        result = atlas.run_full_protocol(\n            prompt,\n            ground_truth=problem.get('canonical_solution'),\n            max_student_tokens=STUDENT_RESPONSE_LIMIT\n        )\n        \n        # Store results\n        result[\"problem_id\"] = i\n        result[\"problem_text\"] = problem['problem']\n        result[\"canonical_solution\"] = problem.get('canonical_solution')\n        results.append(result)\n        \n        # Show strategy used\n        strategy = result.get('learning', {}).get('strategy', 'Unknown')\n        print(f\"  Teaching strategy: {strategy}\")\n        print(f\"  Completed\")\n            \n    except Exception as e:\n        print(f\"  Error: {e}\")\n        continue\n\nprint(f\"\\nCompleted {len(results)} problems\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Code Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Show code generation comparisons\ndef display_code_comparison(result):\n    \"\"\"Display side-by-side comparison of baseline vs guided code.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"Problem: {result['problem_text'][:150]}...\")\n    print(\"=\"*80)\n    \n    print(\"\\nStudent Code (Alone):\")\n    print(\"-\" * 40)\n    baseline_code = result.get('baseline_response', '')\n    print(baseline_code[:500] + (\"...\" if len(baseline_code) > 500 else \"\"))\n    \n    print(\"\\nTeaching Strategy: \" + result.get('learning', {}).get('strategy', 'Unknown'))\n    \n    print(\"\\nStudent Code (With ATLAS):\")\n    print(\"-\" * 40)\n    guided_code = result.get('guided_response', '')\n    print(guided_code[:500] + (\"...\" if len(guided_code) > 500 else \"\"))\n    \n    if result.get('canonical_solution'):\n        print(\"\\nReference Solution:\")\n        print(\"-\" * 40)\n        print(result['canonical_solution'][:500])\n\n# Display first result in detail\nif results:\n    display_code_comparison(results[0])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze code quality improvements\ndef analyze_code_quality(results):\n    \"\"\"Analyze improvements in code quality metrics.\"\"\"\n    metrics = {\n        \"syntax_valid\": {\"baseline\": 0, \"guided\": 0},\n        \"has_function\": {\"baseline\": 0, \"guided\": 0},\n        \"has_docstring\": {\"baseline\": 0, \"guided\": 0},\n        \"has_comments\": {\"baseline\": 0, \"guided\": 0},\n        \"avg_length\": {\"baseline\": [], \"guided\": []}\n    }\n    \n    for result in results:\n        baseline = result.get('baseline_response', '')\n        guided = result.get('guided_response', '')\n        \n        # Check syntax validity (simple heuristic)\n        if 'def ' in baseline and ':' in baseline:\n            metrics[\"syntax_valid\"][\"baseline\"] += 1\n        if 'def ' in guided and ':' in guided:\n            metrics[\"syntax_valid\"][\"guided\"] += 1\n        \n        # Check for function definition\n        if 'def ' in baseline:\n            metrics[\"has_function\"][\"baseline\"] += 1\n        if 'def ' in guided:\n            metrics[\"has_function\"][\"guided\"] += 1\n        \n        # Check for docstrings\n        if '\"\"\"' in baseline or \"'''\" in baseline:\n            metrics[\"has_docstring\"][\"baseline\"] += 1\n        if '\"\"\"' in guided or \"'''\" in guided:\n            metrics[\"has_docstring\"][\"guided\"] += 1\n        \n        # Check for comments\n        if '#' in baseline:\n            metrics[\"has_comments\"][\"baseline\"] += 1\n        if '#' in guided:\n            metrics[\"has_comments\"][\"guided\"] += 1\n        \n        # Track length\n        metrics[\"avg_length\"][\"baseline\"].append(len(baseline))\n        metrics[\"avg_length\"][\"guided\"].append(len(guided))\n    \n    # Calculate percentages\n    n = len(results)\n    if n > 0:\n        print(\"\\nCode Quality Metrics:\")\n        print(\"=\" * 50)\n        print(f\"Valid Syntax:   Baseline: {metrics['syntax_valid']['baseline']/n:.0%} | With ATLAS: {metrics['syntax_valid']['guided']/n:.0%}\")\n        print(f\"Has Function:   Baseline: {metrics['has_function']['baseline']/n:.0%} | With ATLAS: {metrics['has_function']['guided']/n:.0%}\")\n        print(f\"Has Docstring:  Baseline: {metrics['has_docstring']['baseline']/n:.0%} | With ATLAS: {metrics['has_docstring']['guided']/n:.0%}\")\n        print(f\"Has Comments:   Baseline: {metrics['has_comments']['baseline']/n:.0%} | With ATLAS: {metrics['has_comments']['guided']/n:.0%}\")\n        \n        avg_baseline = np.mean(metrics[\"avg_length\"][\"baseline\"])\n        avg_guided = np.mean(metrics[\"avg_length\"][\"guided\"])\n        print(f\"\\nAvg Length:     Baseline: {avg_baseline:.0f} chars | With ATLAS: {avg_guided:.0f} chars\")\n        print(\"=\" * 50)\n    \n    return metrics\n\nif results:\n    code_metrics = analyze_code_quality(results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with custom coding problem\ndef test_custom_code_problem(problem_text: str):\n    \"\"\"Test ATLAS with a custom coding problem.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"Testing custom coding problem\")\n    print(\"=\"*60)\n    print(f\"\\nProblem: {problem_text}\\n\")\n    \n    result = atlas.run_full_protocol(\n        problem_text,\n        max_student_tokens=STUDENT_RESPONSE_LIMIT\n    )\n    \n    print(\"\\nStudent Code (Alone):\")\n    print(\"-\" * 40)\n    print(result['baseline_response'])\n    \n    print(\"\\nTeacher Guidance:\")\n    print(\"-\" * 40)\n    print(f\"Strategy: {result['learning']['strategy']}\")\n    print(f\"Guidance preview: {result['learning']['response'][:200]}...\")\n    \n    print(\"\\nStudent Code (With ATLAS):\")\n    print(\"-\" * 40)\n    print(result['guided_response'])\n    \n    return result\n\n# Example usage\ncustom_problem = \"\"\"Write a Python function that takes a list of integers and returns \na new list containing only the unique elements while preserving the original order.\nFor example: unique_ordered([1, 2, 2, 3, 1, 4]) should return [1, 2, 3, 4]\"\"\"\n\ncustom_result = test_custom_code_problem(custom_problem)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze teaching strategies used\nif results:\n    strategies = [r.get('learning', {}).get('strategy', 'Unknown') for r in results]\n    strategy_counts = pd.Series(strategies).value_counts()\n    \n    print(\"\\nTeaching Strategies Used:\")\n    print(\"=\" * 40)\n    for strategy, count in strategy_counts.items():\n        print(f\"{strategy}: {count} ({count/len(results):.0%})\")\n    \n    # Plot strategy distribution\n    if len(strategy_counts) > 0:\n        plt.figure(figsize=(10, 6))\n        plt.pie(strategy_counts.values, labels=strategy_counts.index, autopct='%1.0f%%')\n        plt.title('Distribution of Teaching Strategies')\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Complex Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a more complex problem\n",
    "complex_problem = \"\"\"Write a Python class called 'TaskQueue' that implements a priority queue with the following features:\n",
    "1. add_task(task_name, priority) - adds a task with given priority (higher number = higher priority)\n",
    "2. get_next_task() - returns and removes the highest priority task\n",
    "3. peek() - returns the highest priority task without removing it\n",
    "4. is_empty() - returns True if queue is empty\n",
    "5. size() - returns the number of tasks in the queue\n",
    "\n",
    "The implementation should handle ties in priority by maintaining FIFO order.\"\"\"\n",
    "\n",
    "print(\"Testing with complex problem...\\n\")\n",
    "complex_result = test_custom_code_problem(complex_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze token usage for code generation\nif results:\n    total_probe_tokens = sum(r.get('probe', {}).get('tokens_used', 0) for r in results)\n    total_learning_tokens = sum(r.get('learning', {}).get('tokens_used', 0) for r in results)\n    total_baseline_tokens = sum(len(r.get('baseline_response', '').split()) for r in results)\n    total_guided_tokens = sum(len(r.get('guided_response', '').split()) for r in results)\n    \n    avg_probe = total_probe_tokens / len(results) if results else 0\n    avg_learning = total_learning_tokens / len(results) if results else 0\n    avg_overhead = avg_probe + avg_learning\n    \n    print(\"\\nToken Efficiency for Code Generation:\")\n    print(\"=\" * 50)\n    print(f\"Average probe tokens:     {avg_probe:.0f}\")\n    print(f\"Average learning tokens:  {avg_learning:.0f}\")\n    print(f\"Total teacher overhead:   {avg_overhead:.0f}\")\n    print(f\"\\nBaseline code avg words: {total_baseline_tokens/len(results):.0f}\")\n    print(f\"Guided code avg words:    {total_guided_tokens/len(results):.0f}\")\n    print(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results for further analysis\nif results:\n    output_file = \"atlas_code_results.json\"\n    with open(output_file, 'w') as f:\n        # Prepare serializable results\n        save_results = []\n        for r in results:\n            save_results.append({\n                \"problem_id\": r.get(\"problem_id\"),\n                \"problem_text\": r.get(\"problem_text\"),\n                \"baseline_response\": r.get(\"baseline_response\"),\n                \"guided_response\": r.get(\"guided_response\"),\n                \"teaching_strategy\": r.get(\"learning\", {}).get(\"strategy\"),\n                \"canonical_solution\": r.get(\"canonical_solution\")\n            })\n        \n        json.dump({\n            \"results\": save_results,\n            \"config\": {\n                \"student_model\": DEFAULT_STUDENT_MODEL,\n                \"teacher_model\": DEFAULT_TEACHER_INSTRUCT,\n                \"num_problems\": len(results)\n            }\n        }, f, indent=2)\n    \n    print(f\"\\nResults saved to {output_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demo showed how ATLAS improves code generation through:\n",
    "\n",
    "1. **Diagnostic Assessment**: Teacher evaluates student's coding capability\n",
    "2. **Adaptive Guidance**: Tailored instruction based on skill level\n",
    "3. **Quality Improvements**: Better structure, documentation, and correctness\n",
    "\n",
    "### Key Benefits for Code Generation\n",
    "- More complete and correct implementations\n",
    "- Better code structure and organization\n",
    "- Improved documentation (docstrings, comments)\n",
    "- Handling of edge cases and error conditions\n",
    "\n",
    "### Next Steps\n",
    "- Test with domain-specific coding tasks\n",
    "- Fine-tune teacher models for specific languages/frameworks\n",
    "- Integrate with code review workflows\n",
    "- Deploy as coding assistant API\n",
    "\n",
    "For more information, see the [main repository](https://github.com/Arc-Intelligence/RCL)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}