{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ATLAS Code Generation Demo\n\nThis notebook demonstrates ATLAS improving code generation and explanation quality.\n\n## Overview\n\nATLAS enhances code generation through:\n1. **Diagnostic Assessment**: Teacher evaluates student's coding capability\n2. **Adaptive Guidance**: Tailored instruction based on coding skill level\n3. **Enhanced Generation**: Student produces better code with teacher support\n\nThe system uses adaptive teaching strategies based on diagnosed capability levels."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    if not os.path.exists('/content/ATLAS'):\n",
    "        !git clone https://github.com/Arc-Computer/ATLAS.git /content/ATLAS\n",
    "    \n",
    "    os.chdir('/content/ATLAS/examples')\n",
    "    \n",
    "    if '/content/ATLAS/examples' not in sys.path:\n",
    "        sys.path.append('/content/ATLAS/examples')\n",
    "    \n",
    "    !pip install -q transformers torch accelerate datasets matplotlib pandas numpy\n",
    "    print(\"✓ Repository cloned and packages installed for Google Colab\")\n",
    "    print(f\"✓ Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    if not os.path.exists('utils'):\n",
    "        print(\"⚠️  Warning: 'utils' directory not found. Please run from the examples/ directory\")\n",
    "    print(\"Using local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.atlas_inference import ATLASInference, load_atlas_models\n",
    "from utils.evaluation import evaluate_code_responses, calculate_metrics\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_STUDENT_MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "DEFAULT_TEACHER_THINKING = \"Arc-Intelligence/ATLAS-8B-Thinking\"\n",
    "DEFAULT_TEACHER_INSTRUCT = \"Arc-Intelligence/ATLAS-8B-Instruct\"\n",
    "\n",
    "PROBE_TOKEN_LIMIT = 150 \n",
    "LEARNING_RESPONSE_LIMIT = 500 \n",
    "STUDENT_RESPONSE_LIMIT = 1000 \n",
    "\n",
    "CAPABILITY_HIGH_THRESHOLD = 4\n",
    "CAPABILITY_MEDIUM_THRESHOLD = 2\n",
    "\n",
    "MIN_GPU_MEMORY_GB = 12\n",
    "RECOMMENDED_GPU_MEMORY_GB = 16\n",
    "\n",
    "DEFAULT_NUM_SAMPLES = 15\n",
    "\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code_problems(num_samples: Optional[int] = 15) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load coding problems for code generation demo.\"\"\"\n",
    "    print(\"Loading coding problems...\")\n",
    "    \n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        # Try loading from HumanEval dataset\n",
    "        dataset = load_dataset(\"openai_humaneval\", split=\"test\")\n",
    "        \n",
    "        problems = []\n",
    "        for item in dataset:\n",
    "            problem_dict = {\n",
    "                \"problem\": item.get(\"prompt\", \"\"),\n",
    "                \"expected_behavior\": item.get(\"docstring\", \"\"),\n",
    "                \"test_cases\": item.get(\"test\", \"\"),\n",
    "                \"canonical_solution\": item.get(\"canonical_solution\", \"\"),\n",
    "                \"source\": \"HumanEval\"\n",
    "            }\n",
    "            problems.append(problem_dict)\n",
    "        \n",
    "        # Sample if requested\n",
    "        if num_samples and len(problems) > num_samples:\n",
    "            problems = random.sample(problems, num_samples)\n",
    "        \n",
    "        print(f\"Loaded {len(problems)} coding problems\")\n",
    "        return problems\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading coding dataset: {e}\")\n",
    "        print(\"Using sample coding problems...\")\n",
    "        return get_sample_code_problems()\n",
    "\n",
    "def get_sample_code_problems() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fallback sample coding problems.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"problem\": \"Write a function that returns the factorial of a positive integer n.\",\n",
    "            \"expected_behavior\": \"factorial(5) should return 120, factorial(0) should return 1\",\n",
    "            \"canonical_solution\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\",\n",
    "            \"source\": \"sample\",\n",
    "            \"difficulty\": \"easy\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Write a function that checks if a string is a palindrome (reads the same forwards and backwards).\",\n",
    "            \"expected_behavior\": \"is_palindrome('racecar') should return True, is_palindrome('hello') should return False\",\n",
    "            \"canonical_solution\": \"def is_palindrome(s):\\n    s = s.lower().replace(' ', '')\\n    return s == s[::-1]\",\n",
    "            \"source\": \"sample\",\n",
    "            \"difficulty\": \"easy\"\n",
    "        },\n",
    "        {\n",
    "            \"problem\": \"Write a function that finds the longest common subsequence of two strings.\",\n",
    "            \"expected_behavior\": \"lcs('ABCDGH', 'AEDFHR') should return 'ADH', lcs('AGGTAB', 'GXTXAYB') should return 'GTAB'\",\n",
    "            \"canonical_solution\": \"def lcs(X, Y):\\n    m, n = len(X), len(Y)\\n    L = [[0] * (n + 1) for _ in range(m + 1)]\\n    \\n    for i in range(m + 1):\\n        for j in range(n + 1):\\n            if i == 0 or j == 0:\\n                L[i][j] = 0\\n            elif X[i-1] == Y[j-1]:\\n                L[i][j] = L[i-1][j-1] + 1\\n            else:\\n                L[i][j] = max(L[i-1][j], L[i][j-1])\\n    \\n    # Reconstruct LCS\\n    i, j = m, n\\n    lcs_str = []\\n    while i > 0 and j > 0:\\n        if X[i-1] == Y[j-1]:\\n            lcs_str.append(X[i-1])\\n            i -= 1\\n            j -= 1\\n        elif L[i-1][j] > L[i][j-1]:\\n            i -= 1\\n        else:\\n            j -= 1\\n    \\n    return ''.join(reversed(lcs_str))\",\n",
    "            \"source\": \"sample\",\n",
    "            \"difficulty\": \"hard\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading coding problems...\\n\")\n",
    "\n",
    "try:\n",
    "    problems = load_code_problems(num_samples=DEFAULT_NUM_SAMPLES)\n",
    "    print(f\"Loaded {len(problems)} coding problems\")\n",
    "except Exception as e:\n",
    "    print(f\"Dataset loading failed: {e}\")\n",
    "    print(\"Using sample problems...\")\n",
    "    problems = get_sample_code_problems()\n",
    "\n",
    "print(f\"\\nSample problem:\")\n",
    "print(f\"Problem: {problems[0]['problem'][:200]}...\")\n",
    "print(f\"Expected: {problems[0].get('expected_behavior', 'N/A')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if gpu_memory < MIN_GPU_MEMORY_GB:\n",
    "        print(f\"Warning: GPU memory ({gpu_memory:.1f} GB) below recommended {MIN_GPU_MEMORY_GB} GB\")\n",
    "        print(\"   Consider using 8-bit quantization or smaller models\")\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU (will be slower)\")\n",
    "    print(\"   For better performance, use Google Colab with GPU runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading models...\")\n",
    "print(f\"Student: {DEFAULT_STUDENT_MODEL}\")\n",
    "print(f\"Teacher: {DEFAULT_TEACHER_INSTRUCT}\\n\")\n",
    "\n",
    "try:\n",
    "    _, code_atlas = load_atlas_models(\n",
    "        student_model_name=DEFAULT_STUDENT_MODEL,\n",
    "        teacher_thinking_name=DEFAULT_TEACHER_THINKING,\n",
    "        teacher_instruct_name=DEFAULT_TEACHER_INSTRUCT,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    )\n",
    "    \n",
    "    atlas = code_atlas\n",
    "    print(\"✓ Models loaded successfully\")\n",
    "    print(f\"✓ Using device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check internet connection\")\n",
    "    print(\"2. Verify HuggingFace access (run: huggingface-cli login)\")\n",
    "    print(\"3. Check GPU memory (need ~12GB for both models)\")\n",
    "    print(\"4. Try reducing batch size or using CPU\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ATLAS Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "print(\"\\nRunning ATLAS protocol on coding problems...\\n\")\n",
    "\n",
    "for i, problem in enumerate(problems[:3]):  # Focus on 3 high-quality examples\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Problem {i+1}/{min(3, len(problems))}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        prompt = f\"{problem['problem']}\\n\\nExpected behavior: {problem.get('expected_behavior', '')}\\n\\nPlease provide a Python implementation.\"\n",
    "        \n",
    "        result = atlas.run_full_protocol(prompt)\n",
    "        \n",
    "        result[\"problem_id\"] = i\n",
    "        result[\"problem_text\"] = problem['problem']\n",
    "        result[\"canonical_solution\"] = problem.get('canonical_solution')\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\nPROBLEM: {problem['problem'][:200]}...\")\n",
    "        \n",
    "        print(f\"\\nDIAGNOSTIC ASSESSMENT:\")\n",
    "        print(f\"Capability Score: {result['diagnostic']['capability_score']}/5\")\n",
    "        print(f\"Strategy Selected: {result['learning']['strategy']}\")\n",
    "        \n",
    "        print(f\"\\nCompleted\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Completed {len(results)} problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Code Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show code generation comparisons\n",
    "def display_code_comparison(result):\n",
    "    \"\"\"Display side-by-side comparison of baseline vs guided code.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Problem: {result['problem_text'][:150]}...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nStudent Code (Alone):\")\n",
    "    print(\"-\" * 40)\n",
    "    baseline_code = result.get('baseline_response', '')\n",
    "    print(baseline_code[:500] + (\"...\" if len(baseline_code) > 500 else \"\"))\n",
    "    \n",
    "    print(\"\\nTeaching Strategy: \" + result.get('learning', {}).get('strategy', 'Unknown'))\n",
    "    \n",
    "    print(\"\\nStudent Code (With ATLAS):\")\n",
    "    print(\"-\" * 40)\n",
    "    guided_code = result.get('guided_response', '')\n",
    "    print(guided_code[:500] + (\"...\" if len(guided_code) > 500 else \"\"))\n",
    "    \n",
    "    if result.get('canonical_solution'):\n",
    "        print(\"\\nReference Solution:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result['canonical_solution'][:500])\n",
    "\n",
    "# Display first result in detail\n",
    "if results:\n",
    "    display_code_comparison(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Quality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom coding problem\n",
    "def test_custom_code_problem(problem_text: str):\n",
    "    \"\"\"Test ATLAS with a custom coding problem.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Testing custom coding problem\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nProblem: {problem_text}\\n\")\n",
    "    \n",
    "    result = atlas.run_full_protocol(problem_text)\n",
    "    \n",
    "    print(\"\\nStudent Code (Alone):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['baseline_response'])\n",
    "    \n",
    "    print(\"\\nTeacher Guidance:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Strategy: {result['learning']['strategy']}\")\n",
    "    guidance = result['learning'].get('learning_guidance', '')\n",
    "    print(f\"Guidance preview: {guidance[:200]}...\")\n",
    "    \n",
    "    print(\"\\nStudent Code (With ATLAS):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result['guided_response'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "custom_problem = \"\"\"Write a Python function that takes a list of integers and returns \n",
    "a new list containing only the unique elements while preserving the original order.\n",
    "For example: unique_ordered([1, 2, 2, 3, 1, 4]) should return [1, 2, 3, 4]\"\"\"\n",
    "\n",
    "custom_result = test_custom_code_problem(custom_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    strategies = [r.get('learning', {}).get('strategy', 'Unknown') for r in results]\n",
    "    strategy_counts = pd.Series(strategies).value_counts()\n",
    "    \n",
    "    print(\"\\nTeaching Strategies Used:\")\n",
    "    print(\"=\" * 40)\n",
    "    for strategy, count in strategy_counts.items():\n",
    "        print(f\"{strategy}: {count} ({count/len(results):.0%})\")\n",
    "    \n",
    "    if len(strategy_counts) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.pie(strategy_counts.values, labels=strategy_counts.index, autopct='%1.0f%%')\n",
    "        plt.title('Distribution of Teaching Strategies')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a more complex problem\n",
    "complex_problem = \"\"\"Write a Python class called 'TaskQueue' that implements a priority queue with the following features:\n",
    "1. add_task(task_name, priority) - adds a task with given priority (higher number = higher priority)\n",
    "2. get_next_task() - returns and removes the highest priority task\n",
    "3. peek() - returns the highest priority task without removing it\n",
    "4. is_empty() - returns True if queue is empty\n",
    "5. size() - returns the number of tasks in the queue\n",
    "\n",
    "The implementation should handle ties in priority by maintaining FIFO order.\"\"\"\n",
    "\n",
    "print(\"Testing with complex problem...\\n\")\n",
    "complex_result = test_custom_code_problem(complex_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    total_probe_tokens = sum(r.get('probe', {}).get('tokens_used', 0) for r in results)\n",
    "    total_learning_tokens = sum(r.get('learning', {}).get('tokens_used', 0) for r in results)\n",
    "    total_baseline_tokens = sum(len(r.get('baseline_response', '').split()) for r in results)\n",
    "    total_guided_tokens = sum(len(r.get('guided_response', '').split()) for r in results)\n",
    "    \n",
    "    avg_probe = total_probe_tokens / len(results) if results else 0\n",
    "    avg_learning = total_learning_tokens / len(results) if results else 0\n",
    "    avg_overhead = avg_probe + avg_learning\n",
    "    \n",
    "    print(\"\\nToken Efficiency for Code Generation:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Average probe tokens:     {avg_probe:.0f}\")\n",
    "    print(f\"Average learning tokens:  {avg_learning:.0f}\")\n",
    "    print(f\"Total teacher overhead:   {avg_overhead:.0f}\")\n",
    "    print(f\"\\nBaseline code avg words: {total_baseline_tokens/len(results):.0f}\")\n",
    "    print(f\"Guided code avg words:    {total_guided_tokens/len(results):.0f}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nThis demonstration shows ATLAS improving code generation through diagnostic assessment and adaptive guidance. The framework evaluates student coding capability and provides tailored instruction based on the diagnosed capability level.\n\nThe two-pass protocol works with any student model and adds minimal overhead. ATLAS-8B-Instruct specializes in code generation tasks, though the teacher models can be fine-tuned for specific languages or frameworks.\n\nFor integration into development workflows or deployment as a coding assistant API, refer to the implementation guides at https://github.com/Arc-Computer/ATLAS."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}