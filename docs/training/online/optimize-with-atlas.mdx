---
title: Optimize with GEPA
description: Rapidly enhance teaching prompts with GEPA optimization in 2 hours (~$10)
sidebarTitle: GEPA Optimization
icon: bolt
---

## Overview

GEPA (Genetic-Pareto) optimization evolves ATLAS teaching prompts using reflective mutation to achieve +165% performance gains in ~2 hours with only $10 in API costs. This hyper-efficient online learning method adapts pre-trained teachers to your specific domain without requiring RL training infrastructure.

![Online Learning Architecture](/images/atlas-online-learning.png)

<Info>
**What is GEPA?** GEPA is an external optimization package (https://github.com/gepa-ai/gepa) that uses LLM-based reflection and Pareto-efficient evolutionary search to evolve prompts. ATLAS uses GEPA to optimize teaching strategies. See [What is GEPA?](/concepts/what-is-gepa) for details.
</Info>

## Prerequisites

- Python 3.11+
- OpenAI API key (for teacher/student models)
- Gemini API key (for RIM reward judges)
- ~$10 in API credits for ~40 optimization iterations

## Quick Start

<Steps>
  <Step title="Install ATLAS">
    Clone and install the complete framework:

    ```bash
    git clone https://github.com/Arc-Computer/ATLAS
    cd ATLAS
    pip install -r requirements-py312.txt
    ```

    This installs:
    - `gepa` package (prompt optimizer)
    - `trainers.prompt_adapter` (ATLAS adapter)
    - `RIM.reward_adapter` (reward system)
  </Step>

  <Step title="Set API Keys">
    Configure your credentials:

    ```bash
    export OPENAI_API_KEY="sk-..."
    export GEMINI_API_KEY="your-gemini-key"

    # Optionally pin specific models
    export TEACHER_MODEL=gpt-5
    export STUDENT_MODEL=gpt-4o-mini
    ```
  </Step>

  <Step title="Run GEPA Optimization">
    Execute the optimization script with a wrapper config:

    ```bash
    ./scripts/openai_agent_atlas.sh configs/wrappers/openai_existing_agent.yaml
    ```

    This runs the GEPA loop:
    1. **Evaluate** baseline teaching prompts on dataset
    2. **Reflect** on failures using GPT-4 to propose improvements
    3. **Mutate** prompts based on reflection insights
    4. **Select** best performers via Pareto frontier
    5. **Iterate** for ~40 rounds (~$10, ~2 hours)

    Output: `optimized_prompts.json` with best teaching strategies
  </Step>

  <Step title="Deploy Optimized Prompts">
    Use the evolved teaching strategies in production:

    ```python
    import json
    from trainers.prompt_adapter import ATLASGEPAAdapter

    # Load optimized prompts from GEPA
    with open("optimized_prompts.json", "r") as f:
        result = json.load(f)
        optimized_prompts = result["best_candidate"]

    # Create adapter with optimized teaching
    adapter = ATLASGEPAAdapter(
        teacher_model="gpt-5",
        student_model="gpt-4o-mini",
        all_prompts=optimized_prompts,
        generation_config={
            "max_tokens": 512,
            "diagnostic_max_tokens": 100,
            "temperature": 0.7
        }
    )

    # Run on new queries
    question = "Debug intermittent 503 errors in production"
    result = adapter.evaluate(
        batch=[{"question": question, "ground_truth": ""}],
        candidate=optimized_prompts
    )

    enhanced_response = result.outputs[0]["student_with_teaching"]
    print(f"Enhanced: {enhanced_response}")
    ```
  </Step>
</Steps>

## Real Implementation: optimize_teaching.py

The actual GEPA optimization is powered by `optimize_teaching.py` in the repository root. Here's how it works:

```python
# optimize_teaching.py (simplified)
import gepa
from trainers.prompt_adapter import ATLASGEPAAdapter
from RIM.reward_adapter import RIMReward
from datasets import load_dataset

# Load dataset
dataset = load_dataset("Arc-Intelligence/Arc-ATLAS-Teach-v1", split="train")
trainset = [
    {
        "question": ex["prompt"],
        "ground_truth": ex["ground_truth"],
        "additional_context": {}
    }
    for ex in dataset
]

# Create ATLAS adapter
adapter = ATLASGEPAAdapter(
    teacher_model="gpt-5",
    student_model="gpt-4o-mini",
    reward_function=None,  # Uses default OnlineTeachingReward
    trace_storage_path="traces/gepa_traces.jsonl",
    all_prompts={},
    generation_config={
        "max_tokens": 512,
        "diagnostic_max_tokens": 100,
        "temperature": 0.7
    }
)

# Define seed prompts (starting point)
seed_prompts = {
    "teacher_adaptive_template": """You are an expert teacher.
The student gave this response: {baseline_response}

To the question: {question}

Provide focused teaching to help them improve.
Wrap teaching in <teaching> tags.""",

    "student_diagnostic_template": """Show your approach to solving: {question}
Explain your thinking in under 500 tokens.""",

    "student_with_teaching_template": """Question: {question}

Teaching: {teaching}

Apply the teaching step-by-step. Put final answer in <solution> tags."""
}

# Define reflection LLM (analyzes failures and proposes improvements)
import litellm
def reflection_lm(prompt: str) -> str:
    response = litellm.completion(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=32768,
        temperature=0.7
    )
    return response.choices[0].message.content

# Run GEPA optimization
result = gepa.optimize(
    seed_candidate=seed_prompts,
    trainset=trainset[:10],  # Use subset for faster iteration
    valset=trainset[:10],
    adapter=adapter,
    reflection_lm=reflection_lm,
    max_metric_calls=40,  # ~$10 budget
    candidate_selection_strategy="pareto",
    skip_perfect_score=False,
    reflection_minibatch_size=5,
    module_selector="single",
    display_progress_bar=True
)

print(f"Best score: {result.best_score}")
print(f"Improvement: {result.best_score - initial_score:+.3f}")

# Save optimized prompts
with open("optimized_prompts.json", "w") as f:
    json.dump({
        "best_candidate": result.best_candidate,
        "best_score": result.best_score,
        "pareto_frontier": result.pareto_frontier
    }, f, indent=2)
```

## How GEPA Works

GEPA evolves prompts through reflective mutation:

### 1. Evaluation Phase
```python
# Adapter evaluates teaching prompts on dataset
scores = []
for example in trainset:
    # Run full ATLAS protocol
    result = adapter.evaluate(
        batch=[example],
        candidate=current_prompts
    )
    scores.append(result.scores[0])

mean_score = sum(scores) / len(scores)
```

### 2. Reflection Phase
```python
# LLM analyzes failures and proposes improvements
reflection_prompt = f"""
Current teaching prompt:
{current_prompts["teacher_adaptive_template"]}

Performance: {mean_score:.3f}

Failed examples:
{format_failures(low_scoring_examples)}

Analyze what went wrong and propose an improved prompt.
"""

improvement_suggestion = reflection_lm(reflection_prompt)
```

### 3. Mutation Phase
```python
# Generate mutated prompt based on reflection
mutation_prompt = f"""
Original prompt: {current_prompt}
Reflection: {improvement_suggestion}

Generate an improved variant that addresses the identified issues.
Output only the new prompt.
"""

mutated_prompt = reflection_lm(mutation_prompt)
```

### 4. Selection Phase
```python
# Pareto frontier: keep prompts that are best on at least one metric
population = [original, mutated_1, mutated_2, ...]
pareto_frontier = select_pareto_optimal(population, metrics=["accuracy", "helpfulness"])

# These become the parents for next generation
elite_prompts = pareto_frontier[:5]
```

## Configuration System

ATLAS uses YAML configs for all settings. The wrapper configs define how to connect your agent:

<Info>
See the [Configuration Guide](/guides/configuration-guide) for complete config examples, troubleshooting, and decision guides for choosing the right config files.
</Info>

```yaml
# configs/wrappers/openai_existing_agent.yaml
user_agent:
  type: custom
  config:
    integration_type: http_api
    endpoint: "http://localhost:8000/chat"
    prompt_field: "message"
    response_field: "response"

teacher_model: gpt-5
student_model: gpt-4o-mini

# Dataset
trainset: arc-atlas-rl
max_examples: 10

# GEPA settings
max_metric_calls: 40  # ~$10 budget
reflection_lm: gpt-4

gepa_config:
  candidate_selection_strategy: pareto
  reflection_minibatch_size: 5
  module_selector: single
  display_progress_bar: true

generation_config:
  max_tokens: 512
  diagnostic_max_tokens: 100
  temperature: 0.7
```

## Wrapper Integration Types

ATLAS can wrap any existing agent through configs:

<Tabs>
  <Tab title="HTTP API">
    ```yaml
    user_agent:
      type: custom
      config:
        integration_type: http_api
        endpoint: "http://localhost:8000/chat"
        prompt_field: "message"
        response_field: "response"
        headers:
          Authorization: "Bearer ${API_KEY}"
        timeout: 300
    ```
  </Tab>

  <Tab title="Python Function">
    ```yaml
    user_agent:
      type: custom
      config:
        integration_type: python_function
        module_path: "/path/to/agent.py"
        function_name: "generate_response"
    ```
  </Tab>

  <Tab title="CLI Command">
    ```yaml
    user_agent:
      type: custom
      config:
        integration_type: cli_command
        command: "python agent.py '{prompt}'"
    ```
  </Tab>

  <Tab title="OpenAI Model">
    ```yaml
    # No user_agent needed - just specify student model
    student_model: gpt-4o-mini
    teacher_model: gpt-5
    ```
  </Tab>
</Tabs>

## Advanced: Custom Datasets

Load your own data for domain-specific optimization:

```python
# custom_data.jsonl format
{"question": "Your task here", "ground_truth": "Expected output"}
{"question": "Another task", "ground_truth": "Another output"}
```

```yaml
# Config with custom data
data_source:
  type: file
  path: "custom_data.jsonl"
  columns:
    question: "question"
    answer: "ground_truth"
```

Or use the API dynamically:

```python
from optimize_teaching import run_gepa_optimization

# Load your custom dataset
custom_trainset = [
    {"question": q, "ground_truth": a, "additional_context": {}}
    for q, a in your_data
]

# Run optimization
result = run_gepa_optimization(
    teacher_model="gpt-5",
    student_model="gpt-4o-mini",
    trainset=custom_trainset[:20],
    valset=custom_trainset[20:30],
    max_metric_calls=40,
    reflection_lm="gpt-4",
    trace_storage_path="traces/custom_opt.jsonl",
    seed_prompts=seed_prompts,
    all_prompts={},
    gepa_config={
        "candidate_selection_strategy": "pareto",
        "skip_perfect_score": False,
        "reflection_minibatch_size": 5
    },
    generation_config={
        "max_tokens": 512,
        "diagnostic_max_tokens": 100,
        "temperature": 0.7
    }
)
```

## Monitoring Optimization Progress

The optimizer displays real-time progress:

```
Iteration 1/40: Best score: 0.342 | Mean: 0.312 | Elite: 5
Iteration 2/40: Best score: 0.389 | Mean: 0.351 | Elite: 5
...
Iteration 40/40: Best score: 0.781 | Mean: 0.712 | Elite: 5

✓ Optimization complete!
Best score: 0.781 (+0.439 improvement)
Saved to: optimized_prompts.json
```

Traces are saved to `traces/gepa_traces.jsonl` for analysis:

```bash
# View traces
tail -f traces/gepa_traces.jsonl

# Analyze prompt evolution
ls traces/gepa_traces/prompt_evolution/
# prompts_eval_0001.json
# prompts_eval_0002.json
# ...
```

## Cost Estimation

Typical costs for 40 iterations:

| Component | Model | Calls | Cost |
|-----------|-------|-------|------|
| Student baseline | gpt-4o-mini | 40 × 10 examples | ~$0.50 |
| Teacher guidance | gpt-5 | 40 × 10 examples | ~$4.00 |
| Student enhanced | gpt-4o-mini | 40 × 10 examples | ~$0.50 |
| Reflection LLM | gpt-4 | 40 iterations | ~$4.00 |
| RIM judges | Gemini Flash | 40 × 10 × 2 | ~$1.00 |
| **Total** | | | **~$10.00** |

Reduce costs by:
- Using fewer examples (`max_examples: 5`)
- Fewer iterations (`max_metric_calls: 20`)
- Smaller models for reflection (`gpt-4o-mini`)

## Performance Expectations

Real results from case study ([full details](/examples/online-optimization-case-study)):

- **Initial score**: -0.2 (generic prompt actually hurt performance)
- **Final score**: 1.479 after 40 iterations
- **Improvement**: +165% vs baseline
- **Time**: ~2 hours
- **Cost**: ~$10

Typical optimization curve:
- Iterations 1-10: Rapid initial gains (+30-50%)
- Iterations 11-30: Incremental improvements (+10-20%)
- Iterations 31-40: Refinement and convergence (+5-10%)

## Troubleshooting

<AccordionGroup>
  <Accordion title="GEPA not improving after many iterations">
    **Problem**: Scores plateau or oscillate

    **Solutions**:
    1. Check reflection LLM is analyzing failures (enable `verbose=True`)
    2. Verify seed prompts aren't already optimal (try worse starting points)
    3. Increase `reflection_minibatch_size` to show more diverse failures
    4. Try different `temperature` for reflection LLM (0.7-1.0)
  </Accordion>

  <Accordion title="ImportError: No module named 'gepa'">
    **Problem**: GEPA package not installed

    **Solution**: Install from requirements:
    ```bash
    pip install -r requirements-py312.txt
    ```
    Or directly:
    ```bash
    pip install gepa
    ```
  </Accordion>

  <Accordion title="Optimization too slow / expensive">
    **Problem**: Taking >3 hours or costing >$15

    **Solutions**:
    1. Reduce dataset size: `max_examples: 5`
    2. Reduce iterations: `max_metric_calls: 20`
    3. Use parallel evaluation: `max_litellm_workers: 20`
    4. Cache baseline responses (already implemented in adapter)
  </Accordion>

  <Accordion title="Wrapper not calling my agent">
    **Problem**: Agent endpoint not being invoked

    **Solutions**:
    1. Test endpoint manually: `curl http://localhost:8000/chat -d '{"message":"test"}'`
    2. Check field names match your API: `prompt_field`, `response_field`
    3. Verify auth headers are correct
    4. Enable debug logging: `export RIM_VERBOSE=1`
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="What is GEPA?" icon="lightbulb" href="/concepts/what-is-gepa">
    Understand the optimization algorithm
  </Card>
  <Card title="Case Study: +165% Gain" icon="chart-line" href="/examples/online-optimization-case-study">
    See real results from GEPA optimization
  </Card>
  <Card title="Inference Integration" icon="play" href="/integration/inference-only">
    Use optimized prompts in production
  </Card>
  <Card title="Full Training" icon="dumbbell" href="/first-experiment">
    Train custom teachers with GRPO
  </Card>
</CardGroup>

## Related Resources

- **GEPA Package**: https://github.com/gepa-ai/gepa
- **Research Blog**: [Supercharging RL with Online Optimization](https://www.arc.computer/blog/supercharging-rl-with-online-optimization)
- **Interactive Demo**: [Colab Notebook](https://colab.research.google.com/github/Arc-Computer/ATLAS/blob/main/examples/online_optimization_demo.ipynb)