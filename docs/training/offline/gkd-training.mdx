---
title: "GKD Training"
description: "On-policy distillation of Atlas runtime traces using Generalized Knowledge Distillation"
---

## Overview

Generalized Knowledge Distillation (GKD) enables on-policy distillation of Atlas runtime traces into smaller, faster student models. GKD is **9-30× more compute-efficient than GRPO** while preserving learning quality, making it ideal for deploying production-ready distilled models.

### Key Benefits

- **Compute Efficient**: 9-30× faster training than GRPO
- **Direct Database Access**: Streams from Postgres, eliminating JSONL export drift
- **Multi-Turn Native**: Designed for conversation-based workflows
- **baseline comparison Metrics**: Built-in tracking of success delta and token efficiency

## When to Use GKD vs GRPO

| Criterion | GKD | GRPO |
|-----------|-----|------|
| **Data source** | Atlas runtime traces | Interactive environment |
| **Compute cost** | Low (supervised + KL) | High (PPO + rollouts) |
| **Speed** | Fast (single pass) | Slow (multi-epoch) |
| **Best for** | Distill teacher → student | Train from scratch with RL |
| **Training time** | Hours | Days |

**Rule of thumb**: Use GKD when you have Atlas traces and want to distill knowledge into a smaller model. Use GRPO when training a new policy from scratch via reinforcement learning.

## Quick Start

### Prerequisites

```bash
# Ensure TRL is installed
pip install "trl>=0.12.0"

# Set database URL
export ATLAS_DB_URL="postgresql://user:pass@host:5432/atlas"
```

### Basic Training

Train a distilled student model from Atlas traces:

```bash
python train.py \
  --config-name teacher_gkd \
  teacher_model_name_or_path=Qwen/Qwen2.5-14B-Instruct \
  model.model_name_or_path=Qwen/Qwen2.5-7B-Instruct \
  trainer.min_reward=0.8
```

This will:
1. Load Atlas runtime traces from Postgres (reward ≥ 0.8)
2. Train the 7B student model to mimic the 14B teacher
3. Log baseline comparison metrics (success delta, token reduction) to WandB
4. Save checkpoints to `outputs/gkd/`

### Configuration Files

GKD training uses two main config files:

**Trainer Config** (`configs/trainer/gkd.yaml`):
- GKD-specific parameters (lmbda, beta, temperature)
- Database connection settings
- Training hyperparameters

**Run Config** (`configs/run/teacher_gkd.yaml`):
- Student and teacher model paths
- Baseline metrics for baseline comparison tracking
- Output directory and logging

## Key Parameters

### GKD Parameters

#### `lmbda` (On-Policy Fraction)

Controls the proportion of student-generated outputs vs teacher-generated outputs:

- `lmbda=1.0`: **Fully on-policy**. Student generates outputs, teacher provides feedback. Most compute-efficient (9-30× speedup).
- `lmbda=0.5`: **Mixed**. 50% student-generated, 50% teacher-generated.
- `lmbda=0.0`: **Supervised**. Only teacher-generated outputs (traditional knowledge distillation).

**Recommendation**: Start with `lmbda=1.0` (Issue #40 recommendation).

#### `beta` (KL Divergence Balance)

Controls the interpolation in the generalized Jensen-Shannon Divergence:

- `beta=0.0`: Forward KL divergence (teacher perspective)
- `beta=1.0`: Reverse KL divergence (student perspective)
- `beta=0.5` (default): Balanced interpolation

**Recommendation**: Start with `beta=0.5`, tune based on task.

#### `temperature`

Sampling temperature for student generation:

- Higher values (0.9-1.0): More diverse outputs
- Lower values (0.5-0.7): More focused outputs

**Recommendation**: Use `temperature=0.9` (default).

### Database Filtering

#### `min_reward`

Minimum session reward threshold for training data:

```yaml
trainer:
  min_reward: 0.8  # Only use high-quality traces
```

Higher values (0.8-0.9) ensure training on successful sessions only.

#### `learning_key`

Filter traces by task type:

```yaml
trainer:
  learning_key: "crm_workflows"  # Only CRM-related traces
```

Set to `null` to use all traces.

### baseline comparison Metrics

Track distillation quality against baseline:

```yaml
trainer:
  baseline_success: 0.75     # Baseline task success rate
  baseline_tokens: 1200      # Baseline tokens per episode
```

**Logged Metrics**:
- `metrics/success_delta`: Improvement over baseline (target: ≥10 pp)
- `metrics/token_reduction_pct`: Token efficiency gain (target: ≥30%)
- `metrics/meets_target`: Boolean flag for success

## Example Configurations

### High-Quality Distillation

For maximum quality, use higher reward threshold and more epochs:

```yaml
# configs/run/gkd_high_quality.yaml
defaults:
  - override /trainer@_global_: gkd

teacher_model_name_or_path: Qwen/Qwen2.5-14B-Instruct
model_name_or_path: Qwen/Qwen2.5-7B-Instruct

trainer:
  min_reward: 0.9           # Only excellent traces
  num_train_epochs: 5       # More training
  learning_rate: 3e-6       # Lower learning rate
```

### Fast Iteration

For rapid experimentation:

```yaml
# configs/run/gkd_fast.yaml
defaults:
  - override /trainer@_global_: gkd

trainer:
  min_reward: 0.7
  num_train_epochs: 1
  eval_steps: 50
  save_steps: 50
```

### Task-Specific Distillation

For a specific workflow:

```yaml
trainer:
  learning_key: "crm_contact_management"
  min_reward: 0.85
  baseline_success: 0.78
```

## Monitoring Training

### WandB Metrics

GKD training logs comprehensive metrics to WandB:

**Training Metrics**:
- `train/loss`: Generalized JSD loss
- `train/learning_rate`: Current learning rate

**Evaluation Metrics**:
- `eval/loss`: Evaluation loss
- `eval/success_rate`: Task completion rate
- `eval/avg_tokens`: Average tokens per episode

**baseline comparison Metrics**:
- `metrics/success_delta`: Improvement vs baseline
- `metrics/token_reduction_pct`: Token efficiency gain
- `metrics/meets_target`: Target achievement flag

### Command Line Output

```
Starting GKD training with baseline comparison baseline: success=75.00%, tokens=1200
Loaded datasets: train=850, eval=150 conversations
AtlasGKDTrainer initialized with lmbda=1.0, beta=0.5

Epoch 1/3
  Step 100: loss=0.245, eval_loss=0.312
  ✅ baseline comparison targets MET: success delta=12.3 pp, token reduction=35.2%

Epoch 2/3
  Step 200: loss=0.198, eval_loss=0.276
  ✅ baseline comparison targets MET: success delta=14.1 pp, token reduction=38.7%
```

## Troubleshooting

### Issue: "No conversations found in database"

**Cause**: Database connectivity or filtering too strict.

**Solutions**:
1. Verify `ATLAS_DB_URL` is correct
2. Check traces exist: Use `atlas.training_data.get_training_sessions()`
3. Lower `min_reward` threshold
4. Set `learning_key: null` to include all tasks

### Issue: "Out of memory (OOM) during training"

**Cause**: Student + teacher models exceed GPU memory.

**Solutions**:
1. Reduce batch size:
   ```yaml
   per_device_train_batch_size: 2
   gradient_accumulation_steps: 8
   ```
2. Enable gradient checkpointing (already default)
3. Load teacher in 8-bit:
   ```yaml
   teacher_model_init_kwargs:
     load_in_8bit: true
   ```
4. Use smaller teacher model

### Issue: "Metrics not improving"

**Possible causes and solutions**:

1. **Insufficient training**: Increase epochs
   ```yaml
   num_train_epochs: 5
   ```

2. **Suboptimal hyperparameters**: Try different beta
   ```yaml
   beta: 0.3  # or 0.7
   ```

3. **Low-quality data**: Increase min_reward
   ```yaml
   min_reward: 0.85
   ```

4. **Student capacity too small**: Use larger student model

### Issue: "Training too slow"

**Solutions**:
1. Use fewer training steps:
   ```yaml
   max_steps: 500  # Instead of epochs
   ```
2. Reduce evaluation frequency:
   ```yaml
   eval_steps: 200  # Default: 100
   ```
3. Use smaller dataset:
   ```yaml
   limit: 5000  # Limit conversations
   ```

## Advanced Topics

### Continual Learning with GKD

To preserve existing skills while distilling new knowledge:

```yaml
# Include rehearsal data from previous tasks
trainer:
  learning_key: null  # All tasks
  min_reward: 0.85
```

Monitor for catastrophic forgetting using regression tests.

### Multi-Stage Distillation

Distill progressively smaller models:

```
14B Teacher → 7B Student₁ → 3B Student₂ → 1.5B Student₃
```

Each stage uses the previous student as the teacher:

```bash
# Stage 1: 14B → 7B
python train.py --config-name teacher_gkd \\
  teacher_model_name_or_path=Qwen/Qwen2.5-14B-Instruct \\
  model.model_name_or_path=Qwen/Qwen2.5-7B-Instruct

# baseline comparison: 7B → 3B
python train.py --config-name teacher_gkd \\
  teacher_model_name_or_path=outputs/gkd_7b/final \\
  model.model_name_or_path=Qwen/Qwen2.5-3B-Instruct
```

### Integration with Arc-CRM-Benchmark

For Stage 3 evaluation (Issue #42):

```bash
# 1. Train distilled model from baseline comparison traces
python train.py --config-name teacher_gkd \\
  trainer.learning_key="crm_workflows" \\
  trainer.min_reward=0.8

# 2. Evaluate distilled model (no guidance)
# ... use arc-crm-benchmark evaluation scripts
```

## API Reference

### AtlasGKDTrainer

```python
from trainers import AtlasGKDTrainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GKDConfig

student = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-7B")
teacher = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-14B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B")

args = GKDConfig(
    output_dir="outputs/gkd",
    per_device_train_batch_size=4,
    lmbda=1.0,
    beta=0.5,
)

trainer = AtlasGKDTrainer(
    model=student,
    teacher_model=teacher,
    args=args,
    db_url="postgresql://localhost:5432/atlas",
    min_reward=0.8,
    processing_class=tokenizer,
)

trainer.train()
trainer.save_model("outputs/gkd/final")
```

### Dataset Functions

```python
from trainers.gkd_dataset import build_gkd_dataset

train_ds, eval_ds = build_gkd_dataset(
    db_url="postgresql://localhost:5432/atlas",
    min_reward=0.8,
    learning_key="crm_workflows",
    eval_split=0.15,
)
```

### baseline comparison Metrics

```python
from trainers.gkd_evaluator import compute_baseline_summary

summary = compute_baseline_summary(
    eval_results,
    baseline_success=0.75,
    baseline_tokens=1200,
)

print(f"Success delta: {summary['success_delta']*100:.1f} pp")
print(f"Token reduction: {summary['token_reduction_pct']:.1f}%")
print(f"Meets targets: {summary['meets_all_targets']}")
```

## Next Steps

- [Configuration Reference](./configuration.mdx) - Detailed config options
- [GRPO Training](./grpo-training.mdx) - Alternative training method
- [Evaluation](../benchmarks/evaluation.mdx) - Evaluate distilled models

## References

- [On-Policy Distillation Paper](https://arxiv.org/abs/2306.13649) - Original GKD research
- [TRL GKDTrainer Docs](https://huggingface.co/docs/trl/main/en/gkd_trainer) - TRL implementation
- [Issue #40](https://github.com/Arc-Computer/ATLAS/issues/40) - Implementation details
