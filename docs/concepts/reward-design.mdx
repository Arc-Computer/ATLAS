---
title: The ATLAS Reward System
description: How the ATLAS reward ensemble evaluates accuracy, teaching quality, process, and diagnostics.
sidebarTitle: Reward System
icon: trophy
---

The ATLAS reward system is an evaluation framework designed for robustness, explainability, and cost-efficiency. It replaces a single, monolithic reward model with a multi-agent ensemble of "judges" that work together to score agent performance. The system operates on a two-tier protocol: an ensemble of efficient, small language models performs an initial evaluation, and any contentious or low-confidence results are automatically escalated to a larger, more capable arbiter model.

This architecture provides developers with clear, principle-driven rationales for every score, calibrated uncertainty estimates, and state-of-the-art accuracy, as validated by its **93.7%** performance on the RewardBenchV2 benchmark.

## Architectural Flow

The reward system's evaluation process is a multi-step workflow designed to balance cost and accuracy, as implemented in `RIM/rim.py`.

<Steps>
  <Step title="1. Ensemble Evaluation">
    An evaluation begins with the **Tier 1 Small-Judge Ensemble**. As defined in `configs/rim_config.yaml`, a pool of efficient models (e.g., `gemini/gemini-2.5-flash`) are called in parallel. Each judge receives the same input but is run at a different temperature (e.g., `[0.2, 0.5, 0.8]`) to produce a diverse set of initial judgments.
  </Step>

  <Step title="2. Principle-Guided Rationale">
    A core feature of the system is its explainability. Judges are required by their prompts (defined in `RIM/judges.py`) to provide a structured rationale for their scores. For instance, the `AccuracyJudge` must first generate and weigh a set of evaluation principles, making its final score fully auditable, while other judges like `HelpfulnessJudge` return different structured outputs like an `evidence` list.
  </Step>

  <Step title="3. Uncertainty & Disagreement Check">
    Once the ensemble judgments are collected, the system checks for ambiguity. Escalation to the next tier is triggered if either of these conditions is met:
    - **High Disagreement:** The standard deviation of scores from the ensemble exceeds the `variance_threshold` (e.g., `0.15`).
    - **Low Confidence:** Any individual judge reports a high self-assessed `uncertainty` score (e.g., `> 0.3`).
  </Step>

  <Step title="4. Escalation to Arbiter">
    If a case is flagged, it is escalated to the **Tier 2 Arbiter**. A detailed **meta-prompt** is constructed, containing the original inputs plus the full set of principles, scores, and rationales from the Tier 1 ensemble. This gives the larger, more capable model (e.g., `gemini/gemini-2.5-pro`) complete context to make a final, informed decision. If no flags are raised, the most confident judgment from the Tier 1 ensemble is used, saving cost and time.
  </Step>
</Steps>

![ATLAS Reward System Architecture](/images/reward-system-design.png)

## Reward Dimensions

The system is designed to be flexible, evaluating interactions across four key dimensions. Each dimension is implemented as a separate `Judge` class in `RIM/judges.py` and can be toggled in the configuration.

| Judge | Purpose | ATLAS Use Case |
| :--- | :--- | :--- |
| **`AccuracyJudge`** | Measures factual correctness and alignment with ground truth. | Foundational check for all tasks; primary judge for pairwise benchmarking. |
| **`HelpfulnessJudge`** | Assesses if teacher guidance caused a positive change in student performance. | Core reward signal for training the Teacher model to be effective. |
| **`ProcessJudge`** | Evaluates the quality and coherence of a student's plan or reasoning steps. | Rewards the Teacher for guiding students toward better problem-solving strategies. |
| **`DiagnosticJudge`** | Judges whether the teacher correctly identified the student's underlying flaws. | Rewards the Teacher for accurate diagnostic capabilities. |

## Configuration for Developers

Developers can tune the reward system's behavior via `configs/rim_config.yaml` without changing code. This allows for easy adaptation to different tasks and cost constraints.

```yaml title="configs/rim_config.yaml"
rim:
  # Controls the diversity of the Tier 1 ensemble
  temperatures: [0.2, 0.5, 0.8]

  # Sets the sensitivity for escalation due to judge disagreement
  variance_threshold: 0.15

  # Defines the models for the two-tier system
  models:
    small_model: "gemini/gemini-2.5-flash"
    large_model: "gemini/gemini-2.5-pro"

  # Toggle which reward dimensions to compute for a given run
  active_judges:
    accuracy: true
    helpfulness: true
    process: true
    diagnostic: true

  # Configure parallel execution for the ensemble
  parallel_execution:
    max_workers: 8
```
For different use cases, such as offline training where only process and helpfulness matter, you can create or use an alternative configuration file (e.g., `rim_offline_config.yaml`) that sets the `accuracy` and `diagnostic` flags to `false`.

## Using the Reward System in Code

The primary entry point for developers is the `RIMReward` class in `RIM/reward_adapter.py`. This class wraps the entire evaluation workflow.

### Offline RL Training

In a typical offline training run, you don't call the reward system directly. Instead, you instantiate it and pass it as an argument to the `GRPOTrainer`. The trainer then handles calling it with batches of data during the RL loop.

```python
# In a real training script (like train.py):

from trainers.grpo import GRPOTrainer
from RIM.reward_adapter import RIMReward
from datasets import load_dataset

# 1. Load your training configuration (e.g., from Hydra)
# grpo_config = ...
# train_dataset = ...

# 2. Instantiate the reward system
reward_system = RIMReward(config_path='configs/rim_config.yaml')

# 3. Initialize the GRPOTrainer
# Note: The first argument is the model, `args` takes the config,
# and the reward system is passed in a list to `reward_funcs`.
trainer = GRPOTrainer(
    model="path/to/your/teacher_model",
    args=grpo_config,
    reward_funcs=[reward_system],
    train_dataset=train_dataset,
    # ... other standard trainer arguments
)

# 4. Start the training
# The trainer will now use the multi-agent reward system internally
trainer.train()
```

### Online Optimization

In the online optimization workflow (`optimize_teaching.py`), the reward system serves as a pluggable evaluation function. While the default process uses a `reflection_lm` to guide prompt evolution, the underlying `ATLASGEPAAdapter` is designed to use our reward system as the fitness function for its genetic algorithm. This allows you to optimize teaching prompts directly against the multi-dimensional scores produced by the reward system.

```python
# In optimize_teaching.py, the adapter can be configured
# to use the reward system as its primary metric.

from RIM.reward_adapter import RIMReward
from trainers.prompt_adapter import ATLASGEPAAdapter

# 1. Instantiate the reward system
reward_system = RIMReward(config_path='configs/rim_config.yaml')

# 2. Pass it to the adapter for online optimization
adapter = ATLASGEPAAdapter(
    teacher_model=teacher_model,
    student_model=student_model,
    reward_function=reward_system, # Using the reward system as the metric
    # ... other adapter arguments
)

# 3. The gepa.optimize function will now use the reward system's
# output to score and evolve the prompt candidates.
```

## Extending the System

The prompt-based design makes the reward system highly extensible. To add a new evaluation dimension (e.g., "Creativity"), a developer only needs to:
1.  Create a new `CreativityJudge` class in `RIM/judges.py` that defines the evaluation prompt.
2.  Add `creativity: true` to the `active_judges` section in the YAML configuration.

No changes to the core `rim.py` evaluation logic are needed. This allows for rapid adaptation to new domains and evaluation criteria.

## Performance Snapshot

Our reward system achieves a state-of-the-art **93.7%** overall accuracy on the RewardBenchV2 benchmark. This accuracy stems from our systems-based architecture, which outperforms single-model evaluators. 

The value of the architecture is best seen in the performance uplift it provides to its components. The system's Tier 1 judge, `gemini-2.5-flash`, scores 77.7% on its own. By integrating it into our ensemble-and-escalation framework, the system achieves a **16-point performance increase**. This highlights how the system's design creates a result that is greater than the sum of its parts.

![ATLAS Reward System Performance on RewardBench V2](/images/Atlas-reward-system-bench.png)

For a full analysis, see the complete [Technical Report](../REWARD_SYSTEM_TECHNICAL_REPORT).