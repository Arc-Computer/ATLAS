---
title: The ATLAS Reward System
description: How the ATLAS reward ensemble evaluates accuracy, teaching quality, process, and diagnostics.
sidebarTitle: Reward System
icon: trophy
---

The ATLAS reward system is an evaluation framework designed for robustness, explainability, and cost-efficiency. It replaces a single, monolithic reward model with a multi-agent ensemble of "judges" that work together to score agent performance. The system operates on a two-tier protocol: an ensemble of efficient, small language models performs an initial evaluation, and any contentious or low-confidence results are automatically escalated to a larger, more capable arbiter model.

This architecture provides developers with clear, principle-driven rationales for every score, calibrated uncertainty estimates, and state-of-the-art accuracy, as validated by its **93.7%** performance on the RewardBenchV2 benchmark.

## Architectural Flow

The reward system's evaluation process is a multi-step workflow designed to balance cost and accuracy, as implemented in `RIM/rim.py`.

<Steps>
  <Step title="1. Ensemble Evaluation">
    An evaluation begins with the **Tier 1 Small-Judge Ensemble**. As defined in `configs/rim_config.yaml`, a pool of efficient models (e.g., `gemini/gemini-2.5-flash`) are called in parallel. Each judge receives the same input but is run at a different temperature (e.g., `[0.2, 0.5, 0.8]`) to produce a diverse set of initial judgments.
  </Step>

  <Step title="2. Principle-Guided Rationale">
    A core feature of the system is its explainability. Judges are required by their prompts (defined in `RIM/judge_specs.py`) to provide a structured rationale for their scores. For instance, the `AccuracyJudge` must first generate and weigh a set of evaluation principles, making its final score fully auditable, while other judges like `HelpfulnessJudge` return different structured outputs like an `evidence` list.
  </Step>

  <Step title="3. Uncertainty & Disagreement Check">
    Once the ensemble judgments are collected, the system checks for ambiguity. Escalation to the next tier is triggered if either of these conditions is met:
    - **High Disagreement:** The standard deviation of scores from the ensemble exceeds the `variance_threshold` (e.g., `0.15`).
    - **Low Confidence:** Any individual judge reports a high self-assessed `uncertainty` score (e.g., `> 0.3`).
  </Step>

  <Step title="4. Escalation to Arbiter">
    If a case is flagged, it is escalated to the **Tier 2 Arbiter**. A detailed **meta-prompt** is constructed, containing the original inputs plus the full set of principles, scores, and rationales from the Tier 1 ensemble. This gives the larger, more capable model (e.g., `gemini/gemini-2.5-pro`) complete context to make a final, informed decision. If no flags are raised, the most confident judgment from the Tier 1 ensemble is used, saving cost and time.
  </Step>
</Steps>

![ATLAS Reward System Architecture](/images/reward-system-design.png)

## Reward Dimensions

The system is designed to be flexible, evaluating interactions across four key dimensions. Each dimension is implemented as a separate `Judge` class in `RIM/judges.py` and can be toggled in the configuration.

| Judge | Purpose | ATLAS Use Case |
| :--- | :--- | :--- |
| **`AccuracyJudge`** | Measures factual correctness and alignment with ground truth. | Foundational check for all tasks; primary judge for pairwise benchmarking. |
| **`HelpfulnessJudge`** | Assesses if teacher guidance caused a positive change in student performance. | Core reward signal for training the Teacher model to be effective. |
| **`ProcessJudge`** | Evaluates the quality and coherence of a student's plan or reasoning steps. | Rewards the Teacher for guiding students toward better problem-solving strategies. |
| **`DiagnosticJudge`** | Judges whether the teacher correctly identified the student's underlying flaws. | Rewards the Teacher for accurate diagnostic capabilities. |

## Configuration for Developers

Developers can tune the reward system's behavior via `configs/rim_config.yaml` without changing code. This allows for easy adaptation to different tasks and cost constraints.

```yaml title="configs/rim_config.yaml"
rim:
  # Controls the diversity of the Tier 1 ensemble
  temperatures: [0.2, 0.5, 0.8]

  # Sets the sensitivity for escalation due to judge disagreement
  variance_threshold: 0.15

  # Defines the models for the two-tier system
  models:
    small_model: "gemini/gemini-2.5-flash"
    large_model: "gemini/gemini-2.5-pro"

  # Toggle which reward dimensions to compute for a given run
  active_judges:
    accuracy: true
    helpfulness: true
    process: true
    diagnostic: true

  model_configs:
    default:
      max_tokens: 32768
      response_format: "json"
    large:
      max_tokens: 32768
      response_format: "json"

  consistency_rules:
    alignment_completeness_bound: 0.1
    completeness_helpfulness_threshold: 0.5
    understanding_helpfulness_bound: 0.1
    contradiction_penalty: 0.2
    safety_violation_penalty: 0.2

  anti_gaming:
    enabled: true
    cap_score: 0.3

  # Configure parallel execution for the ensemble
  parallel_execution:
    max_workers: 8
```
For different use cases, such as offline training where only process and helpfulness matter, you can create or use an alternative configuration file (e.g., `rim_offline_config.yaml`) that sets the `accuracy` and `diagnostic` flags to `false`.
`model_configs` lets you tune per-model generation settings without editing code, while the `consistency_rules` block enforces the score relationships we rely on (for example, process cannot exceed accuracy by more than 0.1). The `anti_gaming` guard caps outlier scores when contradictions or safety issues are detected.

## Using the Reward System in Code

The primary entry point for developers is the `RIMReward` class in `RIM/reward_adapter.py`. This class wraps the entire evaluation workflow.

### Offline RL Training

In a typical offline training run, you don't call the reward system directly. Instead, you instantiate it and pass it as an argument to the `GRPOTrainer`. The trainer then handles calling it with batches of data during the RL loop.

```python
# In a real training script (like train.py):

from trainers.grpo import GRPOTrainer
from RIM.reward_adapter import RIMReward
from datasets import load_dataset

# 1. Load your training configuration (e.g., from Hydra)
# grpo_config = ...
# train_dataset = ...

# 2. Instantiate the reward system
reward_system = RIMReward(config_path='configs/rim_config.yaml')

# 3. Initialize the GRPOTrainer
# Note: The first argument is the model, `args` takes the config,
# and the reward system is passed in a list to `reward_funcs`.
trainer = GRPOTrainer(
    model="path/to/your/teacher_model",
    args=grpo_config,
    reward_funcs=[reward_system],
    train_dataset=train_dataset,
    # ... other standard trainer arguments
)

# 4. Start the training
# The trainer will now use the multi-agent reward system internally
trainer.train()
```

### Online Optimization

In the online optimization workflow (`optimize_teaching.py`), the reward system serves as a pluggable evaluation function. While the default process uses a `reflection_lm` to guide prompt evolution, the underlying `ATLASGEPAAdapter` is designed to use our reward system as the fitness function for its genetic algorithm. This allows you to optimize teaching prompts directly against the multi-dimensional scores produced by the reward system.

```python
# In optimize_teaching.py, the adapter can be configured
# to use the reward system as its primary metric.

from RIM.reward_adapter import RIMReward
from trainers.prompt_adapter import ATLASGEPAAdapter

# 1. Instantiate the reward system
reward_system = RIMReward(config_path='configs/rim_config.yaml')

# 2. Pass it to the adapter for online optimization
adapter = ATLASGEPAAdapter(
    teacher_model=teacher_model,
    student_model=student_model,
    reward_function=reward_system, # Using the reward system as the metric
    # ... other adapter arguments
)

# 3. The gepa.optimize function will now use the reward system's
# output to score and evolve the prompt candidates.
```

## Customizing Judge Behavior

Judge behavior is controlled by **Python code in `RIM/judges.py`**, not YAML configuration. The config file only toggles which judges are active.

### Modifying Existing Judge Prompts

To customize how a judge evaluates (e.g., change what AccuracyJudge prioritizes), edit the `_build_prompt` method in `RIM/judges.py`:

```python
# RIM/judges.py
class AccuracyJudge:
    def _build_prompt(self, inputs: Dict[str, Any]) -> str:
        # Customize this string to change evaluation criteria
        return f"""Evaluate these two responses to the given prompt.

Prompt: {inputs.get('prompt', '')}

Response A: {inputs.get('response_a', '')}
Response B: {inputs.get('response_b', '')}

Step 1: Generate 2-3 principles that are most relevant for evaluating these responses.
For each principle, assign a weight (percentage from 0.0 to 1.0) based on its importance.
The weights should sum to 1.0.

Step 2: Evaluate both responses against each principle.

Step 3: Provide final scores from 0.0 to 1.0 for each response.

Output JSON only: {{"principles": [{{"name": str, "weight": float, "description": str}}], "score_a": float, "score_b": float, "explanation": str, "uncertainty": float}}"""
```

Modify the prompt text to emphasize different criteria, add constraints, or change the evaluation structure.

### Adding a New Judge

To add a new evaluation dimension (e.g., "Creativity"):

**Step 1: Create the judge class in `RIM/judges.py`:**

```python
# RIM/judges.py
class CreativityJudge:
    def __init__(self):
        self.name = 'creativity'

    def evaluate(self, inputs: Dict[str, Any], model_fn, temperature: float) -> Dict[str, Any]:
        prompt = self._build_prompt(inputs)
        response = model_fn(prompt, temperature)
        return self._parse_response(response)

    def _build_prompt(self, inputs: Dict[str, Any]) -> str:
        return f"""Evaluate the creativity of this response.

Question: {inputs.get('question', '')}
Response: {inputs.get('response', '')}

Score creativity from 0.0 (formulaic) to 1.0 (highly creative).

Output JSON only: {{"score": float, "rationale": str, "uncertainty": float}}"""

    def _parse_response(self, response: str) -> Dict[str, Any]:
        try:
            data = json.loads(response)
            return {
                'score': data.get('score', 0.0),
                'rationale': data.get('rationale', ''),
                'uncertainty': data.get('uncertainty', 0.5)
            }
        except:
            return {'score': 0.0, 'rationale': 'Parse error', 'uncertainty': 1.0}
```

**Step 2: Register the judge in `RIM/reward_adapter.py`:**

```python
# RIM/reward_adapter.py
from RIM.judges import AccuracyJudge, HelpfulnessJudge, ProcessJudge, DiagnosticJudge, CreativityJudge

class RIMReward:
    def __init__(self, ...):
        # Add your new judge here
        self.judges = {
            'accuracy': AccuracyJudge(),
            'helpfulness': HelpfulnessJudge(),
            'process': ProcessJudge(),
            'diagnostic': DiagnosticJudge(),
            'creativity': CreativityJudge()  # New judge
        }
```

**Step 3: Enable in `configs/rim_config.yaml`:**

```yaml
active_judges:
  accuracy: true
  helpfulness: true
  process: true
  diagnostic: true
  creativity: true  # Enable new judge
```

### Judge Class Structure

Every judge must implement:
- `__init__()`: Set `self.name`
- `evaluate()`: Call `_build_prompt()`, invoke model, return parsed result
- `_build_prompt()`: Return the LLM prompt string
- `_parse_response()`: Parse JSON response into dict with `score`, `rationale`, `uncertainty`

The YAML config controls which judges run, but **all prompt logic lives in Python code**.

## Performance Snapshot

Our reward system achieves a state-of-the-art **93.7%** overall accuracy on the RewardBenchV2 benchmark. This accuracy stems from our systems-based architecture, which outperforms single-model evaluators.

The value of the architecture is best seen in the performance uplift it provides to its components. The system's Tier 1 judge, `gemini-2.5-flash`, scores 77.7% on its own. By integrating it into our ensemble-and-escalation framework, the system achieves a **16-point performance increase**. This highlights how the system's design creates a result that is greater than the sum of its parts.

![ATLAS Reward System Leaderboard on RewardBench V2](/images/reward-leaderboard.png)

![ATLAS Performance by Category](/images/atlas-categories.png)

For a full analysis, see the complete [Technical Report](../REWARD_SYSTEM_TECHNICAL_REPORT).