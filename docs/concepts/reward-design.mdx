---
title: The ATLAS Reward System
description: How ATLAS measures if teaching actually works
sidebarTitle: Reward System
icon: trophy
---

How do we know if teaching actually worked? ATLAS uses a team of AI judges to score every interaction.

Instead of a single reward model that can be biased or brittle, ATLAS uses a multi-agent ensemble. Think of it like a medical panel: a team of general practitioners makes an initial diagnosis, and when they disagree, a specialist makes the final call.

This achieves **93.7% accuracy on RewardBench V2** while keeping costs low.

## The Two-Tier System

<div align="center">
  <img src="/images/reward-system-design.png" alt="ATLAS Reward System Architecture" width="800" />
  <p><em>Tier 1: Fast ensemble evaluation → Tier 2: Expert arbiter when needed</em></p>
</div>

### How It Works

**Tier 1: The Initial Team**
- Multiple efficient models (like `gemini-2.5-flash`) run in parallel
- Each runs at different temperatures for diverse perspectives
- They all score the same interaction independently
- Fast and cheap for most cases

**Tier 2: The Expert Arbiter**
- Only called when the team disagrees (high variance in scores)
- Or when any judge reports low confidence
- A more powerful model (like `gemini-2.5-pro`) reviews everything
- Makes the final decision with full context

**The key insight**: Most cases are clear-cut and don't need the expensive expert. When there's genuine ambiguity, escalate to the specialist.

### When Escalation Happens

The system escalates to Tier 2 when either:
- **High disagreement**: Standard deviation of scores exceeds the threshold (default: 0.15)
- **Low confidence**: Any judge reports high uncertainty (default: >0.3)

Otherwise, it uses the most confident judgment from Tier 1—saving both time and money.

## The Four Judges

Each judge evaluates a different dimension of quality:

| Judge | What It Asks | Why It Matters |
| :--- | :--- | :--- |
| **AccuracyJudge** | "Is the answer correct?" | Core quality check for all tasks |
| **HelpfulnessJudge** | "Did teaching make it better?" | Measures actual improvement from guidance |
| **ProcessJudge** | "Is the reasoning sound?" | Rewards good problem-solving approach |
| **DiagnosticJudge** | "Did we identify the real problem?" | Validates the teacher's assessment |

### How Judges Work

Each judge is required to provide a structured rationale:

**AccuracyJudge** generates evaluation principles first:
```json
{
  "principles": [
    {"name": "Correctness", "weight": 0.5, "description": "..."},
    {"name": "Completeness", "weight": 0.3, "description": "..."}
  ],
  "score": 0.85,
  "explanation": "The response correctly solves...",
  "uncertainty": 0.1
}
```

**HelpfulnessJudge** provides evidence list:
```json
{
  "evidence": ["Student initially missed key concept", "Teaching provided specific guidance"],
  "score": 0.75,
  "explanation": "Teaching improved response quality by...",
  "uncertainty": 0.15
}
```

This makes every score fully auditable—you can see exactly why a judgment was made.

## Configuration Essentials

The reward system is configured via YAML, but you only need to understand a few key settings:

### Core Settings

```yaml
# configs/rim_config.yaml
rim:
  # Diversity: More temperatures = more diverse initial opinions
  temperatures: [0.2, 0.5, 0.8]

  # Escalation sensitivity
  variance_threshold: 0.15  # Lower = more escalations to expert

  # Which dimensions to evaluate
  active_judges:
    accuracy: true
    helpfulness: true
    process: true
    diagnostic: true
```

### Key Tuning Knobs

**Want more precision?**
- Lower `variance_threshold` to 0.10 → More cases go to the expert model

**Need faster/cheaper evaluation?**
- Raise `variance_threshold` to 0.20 → Trust the initial team more often
- Reduce `temperatures` to `[0.3, 0.7]` → Fewer ensemble members

**Different use cases?**
- **Offline training**: Disable `accuracy` and `diagnostic`, focus on `helpfulness` and `process`
- **Online evaluation**: Enable all four judges for comprehensive scoring

## Reward System in the Atlas SDK

The SDK runtime uses the same reward philosophy to control its execution loop. The `rim` block in [`configs/examples/openai_agent.yaml`](https://github.com/Arc-Computer/atlas-sdk/blob/main/configs/examples/openai_agent.yaml) wires up the scorekeepers and escalation model:

```yaml
# configs/examples/openai_agent.yaml
rim:
  small_model:
    provider: openai
    model: gpt-4o-mini
    api_key_env: OPENAI_API_KEY
    max_output_tokens: 512
  large_model:
    provider: openai
    model: gpt-4o
    api_key_env: OPENAI_API_KEY
    max_output_tokens: 768
  active_judges:
    process: true
    helpfulness: true
  variance_threshold: 0.15
  uncertainty_threshold: 0.3
```

During orchestration, this configuration tells the runtime how to behave:

1. After a step is executed, the `Teacher` hands the output to the Reward System.
2. Enabled judges (`process`, `helpfulness`) sample scores with the **small model**.
3. If the judge scores disagree (variance > 0.15) or any judge reports high uncertainty (> 0.3), the system escalates to the **large model** to reconcile the decision.
4. The final score is returned to the orchestrator; scores below `0.6` (the built-in retry threshold) trigger guidance and a retry. Higher scores allow the workflow to continue.

<Tip>
Want a stricter runtime? Lower `variance_threshold` or disable a judge in `active_judges`. See the [`SDK Configuration Reference`](/sdk/configuration#reward-system-the-rim-block) for complete syntax.
</Tip>

This mirrors the training world: the runtime uses rewards to keep the agent on track, while the training process uses the same signals to improve the underlying models.

## Using the Reward System

### In Training (Offline RL)

The reward system integrates seamlessly with the GRPO trainer:

```python
from trainers.grpo import GRPOTrainer
from RIM.reward_adapter import RIMReward
from datasets import load_dataset

# 1. Instantiate reward system
reward_system = RIMReward(config_path='configs/rim_config.yaml')

# 2. Pass to trainer
trainer = GRPOTrainer(
    model="path/to/your/teacher_model",
    args=grpo_config,
    reward_funcs=[reward_system],  # Just pass it in
    train_dataset=train_dataset
)

# 3. Train - the reward system runs automatically
trainer.train()
```

The trainer handles calling the reward system with batches of data during the RL loop. You don't need to manage it manually.

### For Ad-hoc Evaluation

Quick evaluation of teaching effectiveness:

```python
from RIM.reward_adapter import RIMReward

# Create reward system
reward = RIMReward(config_path='configs/rim_config.yaml')

# Evaluate a single interaction
result = reward.evaluate({
    'question': 'What is 2+2?',
    'baseline_response': 'It is 4',
    'taught_response': 'The answer is 4 because 2 plus 2 equals 4',
    'teaching': 'Explain your reasoning step by step'
})

print(f"Accuracy: {result['accuracy']}")
print(f"Helpfulness: {result['helpfulness']}")
print(f"Improvement: {result['helpfulness'] - result['baseline_accuracy']}")
```

### In Online Optimization

The reward system serves as the fitness function for prompt evolution:

```python
from RIM.reward_adapter import RIMReward
from trainers.prompt_adapter import ATLASGEPAAdapter

# Reward system scores teaching quality
reward_system = RIMReward(config_path='configs/rim_config.yaml')

# Adapter uses rewards to evolve prompts
adapter = ATLASGEPAAdapter(
    teacher_model=teacher_model,
    student_model=student_model,
    reward_function=reward_system  # Drives optimization
)

# Optimize teaching prompts based on reward signals
optimized_prompts = adapter.optimize()
```

## Customizing Judges

### Modifying Existing Judges

Judge behavior is controlled by their prompts in `RIM/judges.py`. To change what AccuracyJudge prioritizes:

```python
# RIM/judges.py
class AccuracyJudge:
    def _build_prompt(self, inputs: Dict[str, Any]) -> str:
        # Customize this string to change evaluation criteria
        return f"""Evaluate these responses.

Prompt: {inputs.get('prompt', '')}
Response A: {inputs.get('response_a', '')}
Response B: {inputs.get('response_b', '')}

Step 1: Generate 2-3 evaluation principles with weights (must sum to 1.0)
Step 2: Score both responses against each principle
Step 3: Provide final scores (0.0 to 1.0)

Output JSON only: {{"principles": [...], "score_a": float, "score_b": float, "uncertainty": float}}"""
```

### Adding a New Judge

**Step 1: Create judge class** (`RIM/judges.py`):

```python
class CreativityJudge:
    def __init__(self):
        self.name = 'creativity'

    def evaluate(self, inputs: Dict[str, Any], model_fn, temperature: float):
        prompt = f"""Score creativity (0.0 = formulaic, 1.0 = highly creative).
        Response: {inputs.get('response', '')}
        Output JSON: {{"score": float, "rationale": str, "uncertainty": float}}"""

        response = model_fn(prompt, temperature)
        return json.loads(response)
```

**Step 2: Register in reward adapter** (`RIM/reward_adapter.py`):

```python
from RIM.judges import AccuracyJudge, HelpfulnessJudge, CreativityJudge

class RIMReward:
    def __init__(self, ...):
        self.judges = {
            'accuracy': AccuracyJudge(),
            'helpfulness': HelpfulnessJudge(),
            'creativity': CreativityJudge()  # Add here
        }
```

**Step 3: Enable in config** (`configs/rim_config.yaml`):

```yaml
active_judges:
  accuracy: true
  helpfulness: true
  creativity: true  # Enable new judge
```

## Performance

### RewardBench V2 Results

The ensemble-and-escalation architecture achieves **93.7% overall accuracy**, significantly outperforming individual models:

- **Component model** (`gemini-2.5-flash`): 77.7% on its own
- **System performance**: 93.7% (+16 points)

The architecture creates a result greater than the sum of its parts.

<div align="center">
  <img src="/images/reward-leaderboard.png" alt="ATLAS Reward System Leaderboard" width="800" />
</div>

### Category Breakdown

<div align="center">
  <img src="/images/atlas-categories.png" alt="Performance by Category" width="800" />
</div>

See the complete [Reward System Technical Report](https://www.arc.computer/blog/ATLAS-Reward-System) for full analysis.

## Monitoring Rewards During Training

The training logs include reward system outputs:

```python
# Example log entry
{
  'step': 150,
  'rim_rewards': {
    'accuracy': 0.85,
    'helpfulness': 0.72,
    'process': 0.78,
    'diagnostic': 0.80
  },
  'rim_explanations': {
    'accuracy': 'Response correctly solves the problem with proper units',
    'helpfulness': 'Teaching improved reasoning structure significantly'
  },
  'escalation_rate': 0.23  # 23% of cases went to Tier 2
}
```

Monitor these to:
- Spot prompt regressions (dropping helpfulness scores)
- Identify misconfigured thresholds (escalation rate too high/low)
- Validate teaching improvements (rising scores over time)

## Next Steps

<CardGroup cols="2">
  <Card title="How ATLAS Works" icon="arrows-rotate" href="/concepts/adaptive-teaching-protocol">
    See how the two-pass teaching protocol works
  </Card>
  <Card title="GRPO Training" icon="graduation-cap" href="/training/offline/grpo-training">
    Use the reward system to train teacher models
  </Card>
  <Card title="Online Optimization" icon="lightbulb" href="/training/online/optimize-with-atlas">
    Evolve prompts using reward signals
  </Card>
  <Card title="Training Configs API" icon="code" href="/api-reference/training-configs">
    Complete configuration options
  </Card>
</CardGroup>

## References

- [Reward System Technical Report](https://www.arc.computer/blog/ATLAS-Reward-System) - Complete methodology and benchmarks
- [ATLAS Technical Report](/reference/technical-report) - How rewards integrate with training
- [RewardBench V2](https://huggingface.co/spaces/allenai/reward-bench) - Benchmark leaderboard
