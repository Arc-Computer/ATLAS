---
title: The ATLAS Reward System
description: How the ATLAS reward ensemble evaluates accuracy, teaching quality, process, and diagnostics.
sidebarTitle: Reward System
icon: trophy
---

The ATLAS reward system is an evaluation framework designed for robustness, explainability, and cost-efficiency. It replaces a single, monolithic reward model with a multi-agent ensemble of "judges" that work together to score agent performance. The system operates on a two-tier protocol: an ensemble of efficient, small language models performs an initial evaluation, and any contentious or low-confidence results are automatically escalated to a larger, more capable arbiter model.

This architecture provides developers with clear, principle-driven rationales for every score, calibrated uncertainty estimates, and state-of-the-art accuracy, as validated by its **93.7%** performance on the RewardBenchV2 benchmark.

## Architectural Flow

The reward system's evaluation process is a multi-step workflow designed to balance cost and accuracy, as implemented in `RIM/rim.py`.

<Steps>
  <Step title="1. Ensemble Evaluation">
    An evaluation begins with the **Tier 1 Small-Judge Ensemble**. As defined in `configs/rim_config.yaml`, a pool of efficient models (e.g., `gemini/gemini-2.5-flash`) are called in parallel. Each judge receives the same input but is run at a different temperature (e.g., `[0.2, 0.5, 0.8]`) to produce a diverse set of initial judgments.
  </Step>

  <Step title="2. Principle-Guided Rationale">
    A core feature of the system is its explainability. Each judge is required by its prompt (defined in `RIM/judges.py`) to first generate a set of evaluation principles and weights (e.g., "Factual Accuracy: 0.6, Conciseness: 0.4") before assigning a score. This ensures every judgment is auditable and based on a clear, explicit rationale.
  </Step>

  <Step title="3. Uncertainty & Disagreement Check">
    Once the ensemble judgments are collected, the system checks for ambiguity. Escalation to the next tier is triggered if either of these conditions is met:
    - **High Disagreement:** The standard deviation of scores from the ensemble exceeds the `variance_threshold` (e.g., `0.15`).
    - **Low Confidence:** Any individual judge reports a high self-assessed `uncertainty` score (e.g., `> 0.3`).
  </Step>

  <Step title="4. Escalation to Arbiter">
    If a case is flagged, it is escalated to the **Tier 2 Arbiter**. A detailed **meta-prompt** is constructed, containing the original inputs plus the full set of principles, scores, and rationales from the Tier 1 ensemble. This gives the larger, more capable model (e.g., `gemini/gemini-2.5-pro`) complete context to make a final, informed decision. If no flags are raised, the most confident judgment from the Tier 1 ensemble is used, saving cost and time.
  </Step>
</Steps>

![ATLAS Reward System Architecture](/images/reward-system-design.png)

## Reward Dimensions

The system is designed to be flexible, evaluating interactions across four key dimensions. Each dimension is implemented as a separate `Judge` class in `RIM/judges.py` and can be toggled in the configuration.

| Judge | Purpose | ATLAS Use Case |
| :--- | :--- | :--- |
| **`AccuracyJudge`** | Measures factual correctness and alignment with ground truth. | Foundational check for all tasks; primary judge for pairwise benchmarking. |
| **`HelpfulnessJudge`** | Assesses if teacher guidance caused a positive change in student performance. | Core reward signal for training the Teacher model to be effective. |
| **`ProcessJudge`** | Evaluates the quality and coherence of a student's plan or reasoning steps. | Rewards the Teacher for guiding students toward better problem-solving strategies. |
| **`DiagnosticJudge`** | Judges whether the teacher correctly identified the student's underlying flaws. | Rewards the Teacher for accurate diagnostic capabilities. |

## Configuration for Developers

Developers can tune the reward system's behavior via `configs/rim_config.yaml` without changing code. This allows for easy adaptation to different tasks and cost constraints.

```yaml title="configs/rim_config.yaml"
rim:
  # Controls the diversity of the Tier 1 ensemble
  temperatures: [0.2, 0.5, 0.8]

  # Sets the sensitivity for escalation due to judge disagreement
  variance_threshold: 0.15

  # Defines the models for the two-tier system
  models:
    small_model: "gemini/gemini-2.5-flash"
    large_model: "gemini/gemini-2.5-pro"

  # Toggle which reward dimensions to compute for a given run
  active_judges:
    accuracy: true
    helpfulness: true
    process: true
    diagnostic: true

  # Configure parallel execution for the ensemble
  parallel_execution:
    max_workers: 8
```
For different use cases, such as offline training where only process and helpfulness matter, you can create or use an alternative configuration file (e.g., `rim_offline_config.yaml`) that sets the `accuracy` and `diagnostic` flags to `false`.

## Using the Reward System in Code

The primary entry point for developers is the `RIMReward` class in `RIM/reward_adapter.py`. This class wraps the entire evaluation workflow.

```python
from trainers.reward_adapter import RIMReward
from trainers.grpo_config import GRPOConfig

# The reward system is typically initialized within a trainer
# This example shows a simplified standalone usage

# 1. Initialize the reward system with a configuration
# This will load the models and settings from the specified YAML
reward_system = RIMReward(config_path='configs/rim_config.yaml')

# 2. Prepare the data for evaluation
# The system expects a "trajectory" of the interaction
trajectory = {
    "prompt": "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?",
    "response_a": "The ball costs $0.10.", # The 'chosen' or better response
    "response_b": "The ball costs $0.05."  # The 'rejected' or worse response
}

# 3. Call the evaluation function
# In a real scenario, this is called by the trainer with batches of data
# The system automatically handles which judges to run based on the config
# and whether the trajectory is pairwise (has response_a/b) or single-response
evaluation_result = reward_system.rim.evaluate(trajectory)

# 4. Inspect the results
# The result contains scores and rich explanations for each active judge
print(evaluation_result)
# {
#   'rewards': {'accuracy': 0.9},
#   'explanations': {'accuracy': 'Response B correctly identifies the price of the ball...'}
# }
```
When used within the ATLAS training loop, the `RIMReward` callable returns a final scalar reward for the RL algorithm and a detailed `info_dict` containing all per-judge scores and text rationales, which are logged for experiment tracking.

## Extending the System

The prompt-based design makes the reward system highly extensible. To add a new evaluation dimension (e.g., "Creativity"), a developer only needs to:
1.  Create a new `CreativityJudge` class in `RIM/judges.py` that defines the evaluation prompt.
2.  Add `creativity: true` to the `active_judges` section in the YAML configuration.

No changes to the core `rim.py` evaluation logic are needed. This allows for rapid adaptation to new domains and evaluation criteria.

## Performance Snapshot

Our reward system achieves a state-of-the-art **93.7%** overall accuracy on the RewardBenchV2 benchmark. This accuracy stems from our systems-based architecture, which outperforms single-model evaluators. 

The value of the architecture is best seen in the performance uplift it provides to its components. The system's Tier 1 judge, `gemini-2.5-flash`, scores 77.7% on its own. By integrating it into our ensemble-and-escalation framework, the system achieves a **16-point performance increase**. This highlights how the system's design creates a result that is greater than the sum of its parts.

![ATLAS Reward System Performance on RewardBench V2](/images/Atlas-reward-system-bench.png)

For a full analysis, see the complete [Technical Report](../REWARD_SYSTEM_TECHNICAL_REPORT).