---
title: Teacher-Student Paradigm
description: The foundational concept enabling adaptive performance enhancement
sidebarTitle: Teacher-Student Concept
icon: chalkboard-user
---

The teacher-student paradigm is the core idea behind ATLAS: a specialized teacher model enhances any student model's performance without modifying the student's weights.

Think of it like hiring an expert tutor for your existing team. The tutor (teacher) doesn't replace your team members (student models)—instead, it helps them perform better on specific tasks through real-time guidance.

## Core Concept

**Student**: Any LLM you want to enhance (GPT, Claude, Llama, your custom model, etc.)

**Teacher**: A specialized 8B model trained to provide adaptive guidance

**Result**: Student produces better outputs without any retraining

### Key Advantage: Model Agnostic

Unlike fine-tuning or RLHF, which require modifying model weights, ATLAS works with any model through inference-time guidance:

| Traditional Approach | ATLAS Approach |
|---------------------|----------------|
| Retrain the student model | Keep student model frozen |
| Requires model access and compute | Works with API-only models |
| Risk of capability loss | Preserves all original capabilities |
| Weeks to deploy changes | Hours to deploy improvements |

For implementation details, see [`Student & Teacher Roles`](/sdk/student-teacher-roles) for the runtime personas and [`Offline Training`](/training/offline/grpo-training) to learn how new teachers are trained.

## Why This Works

### 1. Asymmetric Specialization

The teacher model is smaller (8B parameters) but specialized for one job: generating helpful teaching. The student model can be any size and handles the actual task.

**Analogy**: A coding interview coach (teacher) doesn't need to be a better programmer than you (student). They just need to be excellent at identifying gaps in your approach and providing targeted feedback.

### 2. Inference-Time Enhancement

Teaching happens through prompting at inference time:
1. Teacher analyzes the task and student capability
2. Teacher generates guidance as text
3. Guidance is added to student's prompt
4. Student generates improved response

No gradient updates, no fine-tuning, no model modification required.

### 3. Adaptive Intensity

The teacher adjusts guidance based on student capability:
- **Strong student**: Minimal intervention (50-100 tokens)
- **Weak student**: Comprehensive help (200-300 tokens)

This saves both cost and context length while ensuring safety.

## Model Requirements

| Component | Specification | Purpose |
|-----------|--------------|---------|
| Teacher Model | 8B parameters, RL-trained | Generates adaptive guidance |
| Student Model | Any size (4B-70B+), any provider | Executes enhanced reasoning |
| Context Window | 4096-32768 tokens | Accommodates teaching interaction |
| Inference Overhead | +30% latency | Two-pass protocol cost |

## Performance on τ²-bench

State-of-the-art results on complex multi-step tasks (mms_issue subset):

| System | Pass@1 Rate | Notes |
|--------|-------------|-------|
| **ATLAS Teacher-Student** | **24.0%** | Minimal degradation across attempts |
| GPT-4.1 | 18.0% | -8pt drop from Pass@1 to Pass@4 |
| Claude 3.7 Sonnet | 18.0% | -16pt drop from Pass@1 to Pass@4 |
| o4-mini | 12.0% | -10pt drop from Pass@1 to Pass@4 |
| Qwen3-8B (Student Only) | 4.1% | No teacher guidance |

**Key observations:**
- **6x performance lift**: Teacher guidance improves Qwen3-8B from 4.1% to 24.0%
- **Consistency advantage**: ATLAS shows minimal degradation across multiple attempts
- **Cross-domain transfer**: Math-trained teacher successfully guides telecom debugging tasks

### Aggregate Metrics
- **Average accuracy improvement (runtime + GRPO)**: 15.7% across all benchmarks
- **Continual learning (SDK runtime)**: Rapid, task-specific adaptation powered by the atlas-sdk export + training workflow
- **Maximum observed improvement**: 29.6% on specific domains (closed-loop baseline)
- **Non-degradation rate**: 97% (safe to deploy)
- **Token efficiency**: 50% reduction (4k → 2k tokens)
- **Completion rate**: 31% improvement (69% → 100%)

## Advantages Over Alternatives

### vs. Fine-tuning
- **No retraining required**: Works with frozen student models
- **Preserves capabilities**: No catastrophic forgetting
- **Instant deployment**: No training time or compute cost

### vs. Prompt Engineering
- **Adaptive**: Adjusts to student capability automatically
- **Consistent**: Systematic improvement, not trial-and-error
- **Efficient**: Optimized token usage based on need

### vs. Ensemble Methods
- **Lower latency**: Single student inference (not multiple models)
- **Lower cost**: No redundant model calls
- **Better interpretability**: Clear teaching rationale

## How It's Trained

The teacher model undergoes two-phase training:

**Phase 1: Supervised Fine-Tuning (SFT)**
- Learns basic teaching patterns
- Establishes foundational reasoning
- ~4-6 hours on 8× H100 GPUs

**Phase 2: Reinforcement Learning (GRPO)**
- Optimizes for student performance improvement
- Learns adaptive capability assessment
- ~24-36 hours on 8× H100 GPUs

The reward signal comes from measuring actual student improvement, creating a teacher that's incentivized to make students succeed.

## Integration Patterns

### Pattern 1: SDK Runtime Orchestration

Define your agent connector in the atlas-sdk (HTTP, Python callable, CLI, or OpenAI Assistant) and enable teaching plus reward scoring. The runtime collects traces that Atlas Core consumes for GRPO training.

### Pattern 2: Offline GRPO Training

Export those traces and run `python scripts/run_offline_pipeline.py --export-path <export>.jsonl` to produce an updated teacher checkpoint. Point the SDK runtime at the new weights to close the loop.

## Best Practices

<AccordionGroup>
  <Accordion title="Teacher Model Selection">
    Choose based on task type:
    - **ATLAS-8B-Thinking**: Mathematical and logical reasoning
    - **ATLAS-8B-Instruct**: Code generation and technical tasks
    - **Custom trained**: Domain-specific requirements
  </Accordion>

  <Accordion title="Student Model Compatibility">
    Verify student model supports:
    - System prompts or instruction following
    - Sufficient context length (>4K tokens)
    - Deterministic generation (temperature control)
  </Accordion>

  <Accordion title="Performance Optimization">
    - Cache teacher guidance for repeated queries
    - Batch similar tasks together
    - Use streaming for interactive applications
    - Monitor token usage for cost control
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols="2">
  <Card title="How ATLAS Works" icon="arrows-rotate" href="/concepts/adaptive-teaching-protocol">
    Detailed two-pass protocol and implementation
  </Card>
  <Card title="Deploy to Production" icon="plug" href="/integration/inference-only">
    Integrate teacher-student system into your pipeline
  </Card>
  <Card title="Reward System" icon="trophy" href="/concepts/reward-design">
    How teaching effectiveness is measured
  </Card>
  <Card title="Performance Benchmarks" icon="chart-line" href="/examples/sota-on-tau-squared-bench">
    Detailed τ²-bench results and analysis
  </Card>
  <Card title="Offline Training" icon="dumbbell" href="/training/offline/grpo-training">
    Train custom teachers with SFT + GRPO
  </Card>
</CardGroup>

## References

- [ATLAS Technical Report](/reference/technical-report) – Complete methodology and architecture
- [Offline Training Guide](/training/offline/grpo-training) – Hands-on teacher training walkthrough
- [SRE Case Study](/examples/sre-root-cause-analysis) – Applying the runtime loop and GRPO to incident response
