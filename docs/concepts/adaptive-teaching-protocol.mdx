---
title: How ATLAS Works
description: The adaptive teaching method that enhances any AI model at inference time
sidebarTitle: How ATLAS Works
icon: arrows-rotate
---

ATLAS improves your AI model's responses through adaptive teaching. Like a doctor doing a check-up before prescribing treatment, ATLAS first diagnoses your model's capability on a specific task, then provides the right level of guidance to help it succeed.

This happens in two quick passes—no retraining required.

## The Two-Pass Process

<div align="center">
  <img src="/images/adaptive-teaching.png" alt="ATLAS Adaptive Teaching Flow" width="800" />
  <p><em>Pass 1 diagnoses capability, Pass 2 provides adaptive guidance</em></p>
</div>

### Pass 1: Quick Diagnosis (≤50 tokens)

The teacher sends a brief probe to assess the student model's capability on the specific task.

**What happens:**
- Teacher generates a targeted diagnostic question
- Student responds with up to 50 tokens
- Teacher analyzes the response for reasoning depth, domain knowledge, and confidence

**Example probe for math problem:**
```
"What approach would you use to solve this problem?"
```

The student's answer reveals whether they understand the core concept or need help.

### Pass 2: Adaptive Guidance (≤200 tokens)

Based on the diagnosis, the teacher provides exactly the right amount of help—not too much, not too little.

**What happens:**
- Teacher selects a guidance strategy based on capability assessment
- Guidance is injected into the student's context
- Student generates an enhanced response using the teaching

**Example guidance for weak capability:**
```
"This is exponential growth. Steps:
1. Find doubling periods: 15 ÷ 3 = 5
2. Apply formula: Initial × 2^periods
3. Calculate: 100 × 2^5 = 3200"
```

## Three Guidance Strategies

ATLAS adapts its teaching based on what the student needs:

| Student Capability | Teaching Strategy | What It Provides |
|-------------------|------------------|------------------|
| **Struggling** | Comprehensive | Like a detailed tutorial—breaks down every step, explains prerequisites, points out common mistakes |
| **Partial Understanding** | Targeted | Like a helpful hint—identifies the key insight you're missing, corrects misconceptions |
| **Already Capable** | Minimal | Like a safety net—only flags edge cases, keeps you on track |

### How It Adapts

**Weak student (comprehensive guidance):**
- Full problem decomposition
- Step-by-step methodology
- Common pitfalls to avoid
- Verification strategy

**Moderate student (targeted guidance):**
- Critical insight they're missing
- Correction for specific misconception
- Key principle to apply

**Strong student (minimal guidance):**
- Edge case awareness
- Optimization suggestion
- Or nothing at all if they're on the right track

## Case Study: Debugging Task

**Task:** "Service returns 503 errors intermittently in production"

<Tabs>
  <Tab title="Weak Student">
    **Diagnostic probe response:**
    > "Check if service is running"

    **Capability score:** 0.25 (struggling)

    **Adaptive guidance (178 tokens):**
    ```
    Systematic debugging approach:
    1. Check service mesh configuration (istioctl analyze)
    2. Verify mTLS policies aren't conflicting
    3. Examine traffic routing rules
    4. Check resource limits and scaling policies
    5. Review recent deployments

    Start with: kubectl get virtualservice,destinationrule -A
    This reveals routing configuration issues that cause intermittent failures.
    ```
  </Tab>

  <Tab title="Strong Student">
    **Diagnostic probe response:**
    > "Analyze Istio configurations and check Envoy proxy logs"

    **Capability score:** 0.85 (capable)

    **Adaptive guidance (62 tokens):**
    ```
    Circuit breaker configuration may be too aggressive.
    Check: outlierDetection.consecutiveErrors threshold in DestinationRule.
    Common cause of intermittent 503s with partial traffic.
    ```
  </Tab>
</Tabs>

Notice the dramatic difference: weak students get comprehensive scaffolding (178 tokens), while strong students get minimal intervention (62 tokens). This saves both cost and context length.

## Why This Matters

### Saves Money
Adaptive guidance means you only use the tokens you need. Strong models get 50-100 tokens of guidance, weak models get 200-300. Average across tasks: **50% token reduction** compared to always providing full guidance.

### Safe to Deploy
97% non-degradation rate across benchmarks. When a student is already capable, minimal guidance means minimal risk of making things worse.

### Works with Any Model
Because teaching happens at inference time through prompting, ATLAS works with any student model:
- Closed APIs (GPT, Claude, Gemini)
- Open-source models (Llama, Qwen, Mistral)
- Your custom fine-tuned models
- Even your existing agents and workflows

No retraining, no weight modifications, no model access required.

## Performance Results

Measured on τ²-bench (complex multi-step tasks):

| System | Pass@1 Rate | Degradation |
|--------|-------------|-------------|
| **ATLAS Teacher-Student** | **24.0%** | Minimal (1.6pt) |
| GPT-4.1 | 18.0% | High (8pt drop) |
| Claude 3.7 Sonnet | 18.0% | Severe (16pt drop) |
| o4-mini | 12.0% | High (10pt drop) |
| Student Only (no teacher) | 4.1% | — |

ATLAS provides a **6x performance lift** while maintaining consistency across multiple attempts. Other systems show severe degradation from Pass@1 to Pass@4, while ATLAS stays stable.

### Key Metrics Across Benchmarks
- **Closed-loop accuracy improvement**: 15.7% baseline lift
- **Continual learning (atlas-sdk)**: Rapid task-specific adaptation powered by the runtime export + GRPO workflow
- **Maximum observed improvement**: 29.6% on specific datasets (closed-loop baseline)
- **Completion rate**: 31% improvement (69% → 100%)
- **Token efficiency**: 50% reduction (4k → 2k tokens)
- **Non-degradation rate**: 97%

## Implementation Patterns

### Real-time Enhancement

Use the atlas-sdk runtime adapters (HTTP, Python callable, CLI, OpenAI Assistant) to wrap your agent with the teacher/student loop. The runtime streams guidance, telemetry, and reward scores while capturing traces for offline training.

### Batch Processing

Schedule `arc-atlas --database-url postgresql://... --output traces.jsonl` to dump accumulated traces to JSONL. Atlas Core consumes those exports via `scripts/run_offline_pipeline.py`, producing new teacher checkpoints through GRPO.

### Streaming Applications

Leverage the SDK's streaming APIs to interleave guidance with live tool usage. Persist the trace outputs so that high-signal conversations feed the next GRPO training cycle.

## How Diagnosis Works

The teacher uses multiple signals to assess student capability:

**Keyword presence** (20%): Does the response mention relevant domain terms?

**Reasoning structure** (30%): Is there logical flow in the approach?

**Specificity** (20%): Are they vague or concrete?

**Correctness** (30%): Is the approach fundamentally sound?

These combine into a capability score from 0.0 to 1.0:
- **< 0.3**: Weak (comprehensive guidance)
- **0.3 - 0.7**: Moderate (targeted guidance)
- **> 0.7**: Strong (minimal guidance)

## Next Steps

<CardGroup cols="2">
  <Card title="Reward System" icon="trophy" href="/concepts/reward-design">
    How ATLAS measures teaching effectiveness
  </Card>
  <Card title="Deploy to Production" icon="code" href="/integration/inference-only">
    Integrate adaptive teaching into your pipeline
  </Card>
  <Card title="SDK Runtime" icon="workflow" href="/sdk/quickstart">
    Export traces and monitor agents in production
  </Card>
  <Card title="Performance Analysis" icon="chart-line" href="/benchmarks/evaluation-methodology">
    Detailed benchmark results and methodology
  </Card>
</CardGroup>

## References

- [ATLAS Technical Report](/reference/technical-report) - Section 3.2 on two-pass protocol
- [Teacher-Student Paradigm](/concepts/teacher-student-paradigm) - Conceptual foundation
- [Offline Training](/training/offline/grpo-training) - Hands-on protocol training
