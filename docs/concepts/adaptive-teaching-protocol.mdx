---
title: How ATLAS Works
description: The adaptive teaching method that enhances any AI model at inference time
sidebarTitle: How ATLAS Works
icon: arrows-rotate
---

ATLAS improves your model’s responses by turning each task into a structured learning episode. Before the Student executes anything, the runtime collects triage context, probes capability, and then chooses how tightly to supervise the work. The entire process runs at inference time—no finetuning or weight edits required.

## Adaptive Episode Flow

<div align="center">
  <img src="/images/adaptive-teaching.png" alt="ATLAS Adaptive Teaching Flow" width="800" />
  <p><em>Triage → capability probe → adaptive lane → reward + telemetry</em></p>
</div>

1. **Triage dossier** – A triage adapter (default: `atlas.utils.triage.default_build_dossier`) normalises metadata, risks, and persona hints for the task.  
2. **Capability probe** – A lightweight LLM prompt inspects the dossier plus recent history and returns `{mode, confidence, evidence}`.  
3. **Adaptive lane** – The orchestrator records the probe payload and routes the Student into one of four execution modes.  
4. **Execution & reward** – The Student and Teacher collaborate according to the lane’s supervision level, then the Reward System aggregates judges and learning notes.  
5. **Telemetry & memory** – The runtime emits an `adaptive_summary`, captures persona usage/updates, and persists everything for export.

### The Four Lanes

| Lane | When it triggers | Supervision profile | What gets recorded |
|------|------------------|---------------------|--------------------|
| `auto` | High confidence history | Student executes once, Teacher skips validation | Lane selection + confidence for telemetry; reward may be skipped |
| `paired` | Certification required or fingerprint unseen | Student executes once, Teacher validates the final answer | Certification flag, validation verdict, reward reused from certification |
| `coach` | Medium confidence | Reviewed plan collapses into a single step with validation + optional retry | Adaptive summary stores probe evidence; guidance stays concise |
| `escalate` | Low confidence or manual override | Full stepwise execution with detailed guidance and retries | Rich per-step metadata, guidance transcripts, persona promotions |

The lane determines how much help the Teacher provides, how many retries are allowed, and how the final reward is computed. Switching lanes is automatic—the probe score and recent session history drive the choice.

### Telemetry and Exports

Every run writes adaptive context into the execution metadata:

- `adaptive_summary` captures the active lane, probe confidence, certification marker, evidence, and recent decisions.
- `triage_dossier` stores the structured risk/signal payload that seeded the episode.
- `personas_used` and `persona_updates` expose which memories influenced the run and what new candidates were proposed.
- `session_reward` and per-step traces mirror the judges and guidance the Teacher delivered.

When you export sessions with `arc-atlas`, those same fields appear in JSONL so downstream analytics and training pipelines can reason about lane-specific behaviour.

## Example Episode: Production Incident

**Task:** “Service returns 503 errors intermittently in production.”

1. **Triage** tags the task with `domain:sre`, notes recent incident history, and references an Istio persona.  
2. **Probe** sees a fresh fingerprint and moderate risk → selects `paired` for certification.  
3. **Execution** runs single-shot, the Teacher validates the final answer, and the certification verdict becomes the reward.  
4. **Follow-up runs** reuse the new fingerprint; once confidence climbs, the probe shifts later requests to `coach`, enabling quick single-step executions with compact guidance.

Atlas tracks the entire journey: the adaptive summary logs the lane change, persona promotion records the helpful guidance, and exports capture both sessions so training teams can see how the runtime escalated supervision.

## Why This Matters

### Saves Money
The runtime only escalates when needed. Familiar tasks stay in `auto`/`paired`, consuming minimal tokens, while unfamiliar or risky work is escalated to `coach`/`escalate` to guarantee quality. Across benchmarks this yields ~50 % token savings versus always-on supervision.

### Safe to Deploy
By certifying new fingerprints (`paired`) and capturing reward rationales, Atlas sustains a 97 % non-degradation rate. Risky tasks receive the same stepwise oversight you’d expect from a human reviewer.

### Works with Any Agent
Because the teacher/student loop runs at inference time, you can wrap closed APIs (GPT, Claude, Gemini), open-source models (Llama, Qwen, Mistral), or your own agents. No model weights or managed infrastructure required—just provide a config and let the runtime handle triage, probing, and telemetry.

## Performance Results

Measured on τ²-bench (complex multi-step tasks):

| System | Pass@1 Rate | Degradation |
|--------|-------------|-------------|
| **ATLAS Teacher-Student** | **24.0%** | Minimal (1.6pt) |
| GPT-4.1 | 18.0% | High (8pt drop) |
| Claude 3.7 Sonnet | 18.0% | Severe (16pt drop) |
| o4-mini | 12.0% | High (10pt drop) |
| Student Only (no teacher) | 4.1% | — |

ATLAS provides a **6x performance lift** while maintaining consistency across multiple attempts. Other systems show severe degradation from Pass@1 to Pass@4, while ATLAS stays stable.

### Key Metrics Across Benchmarks
- **Closed-loop accuracy improvement**: 15.7% baseline lift
- **Continual learning (atlas-sdk)**: Rapid task-specific adaptation powered by the runtime export + GRPO workflow
- **Maximum observed improvement**: 29.6% on specific datasets (closed-loop baseline)
- **Completion rate**: 31% improvement (69% → 100%)
- **Token efficiency**: 50% reduction (4k → 2k tokens)
- **Non-degradation rate**: 97%

## Implementation Patterns

### Real-time Enhancement

Use the atlas-sdk runtime adapters (HTTP, Python callable, CLI, OpenAI Assistant) to wrap your agent with the teacher/student loop. The runtime streams guidance, telemetry, and reward scores while capturing traces for offline training.

### Batch Processing

Schedule `arc-atlas --database-url postgresql://... --output traces.jsonl` to dump accumulated traces to JSONL. Atlas Core consumes those exports via `scripts/run_offline_pipeline.py`, producing new teacher checkpoints through GRPO.

### Streaming Applications

Leverage the SDK's streaming APIs to interleave guidance with live tool usage. Persist the trace outputs so that high-signal conversations feed the next GRPO training cycle.

## How Diagnosis Works

The teacher uses multiple signals to assess student capability:

**Keyword presence** (20%): Does the response mention relevant domain terms?

**Reasoning structure** (30%): Is there logical flow in the approach?

**Specificity** (20%): Are they vague or concrete?

**Correctness** (30%): Is the approach fundamentally sound?

These combine into a capability score from 0.0 to 1.0:
- **< 0.3**: Weak (comprehensive guidance)
- **0.3 - 0.7**: Moderate (targeted guidance)
- **> 0.7**: Strong (minimal guidance)

## Next Steps

<CardGroup cols="2">
  <Card title="Reward System" icon="trophy" href="/concepts/reward-design">
    How ATLAS measures teaching effectiveness
  </Card>
  <Card title="Deploy to Production" icon="code" href="/integration/inference-only">
    Integrate adaptive teaching into your pipeline
  </Card>
  <Card title="SDK Runtime" icon="workflow" href="/sdk/quickstart">
    Export traces and monitor agents in production
  </Card>
</CardGroup>

## References

- [ATLAS Technical Report](/reference/technical-report) - Section 3.2 on two-pass protocol
- [Teacher-Student Paradigm](/concepts/teacher-student-paradigm) - Conceptual foundation
- [Offline Training](/training/offline/grpo-training) - Hands-on protocol training
