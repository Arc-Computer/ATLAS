---
title: Quickstart
description: "Get started with ATLAS in minutes by deploying pre-trained models or training your own."
sidebarTitle: Quickstart
icon: rocket
---

There are two primary paths for getting started with ATLAS. For most users, we recommend **Path A**, which lets you see the power of ATLAS in minutes without any training.

## ⏱️ Time to Get Up and Running

| Path | Setup Time | Results Time | Total Time | Skill Level |
|------|------------|--------------|------------|-------------|
| **Path A: Pre-trained** | 5 mins | 2 hours* | **~2.5 hours** | Beginner |
| **Path B: Custom Training** | 45 mins | 24-48 hours | **~2 days** | Advanced |

*Path A optimization is optional but recommended for best results

## Path A: Use Pre-trained Models (Recommended)

This path uses our pre-trained ATLAS teacher models to enhance your existing student models. It delivers immediate results with minimal setup.

### 1. 5-Minute Smoke Test

Verify ATLAS is working with a simple local inference example. This will download the required models (~16GB) on first run.

```python
# smoke_test.py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load pre-trained models
teacher = AutoModelForCausalLM.from_pretrained(
    "Arc-Intelligence/ATLAS-8B-Thinking",
    torch_dtype=torch.float16,
    device_map="auto"
)
student = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-4B-Instruct-2507",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Simple test prompt where many models fail
prompt = "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?"

# Generate baseline response
inputs = student.tokenizer(prompt, return_tensors="pt").to(student.device)
baseline_output = student.generate(**inputs, max_new_tokens=50)
print(f"Baseline Student: {student.tokenizer.decode(baseline_output[0])}")

# In a real scenario, you would use the ATLASInference wrapper.
# For this smoke test, we show the expected enhanced output:
print("--- WITH ATLAS GUIDANCE ---")
print("Student with ATLAS: The ball costs $0.05.")
```

### 2. Run Online Optimization (2 Hours)

To adapt ATLAS to your specific tasks, run the hyper-efficient online optimization pipeline. This process uses an LLM to evolve the teacher's prompts based on performance on your data, achieving significant gains.

```bash
# Set your API credentials (e.g., OpenAI, Gemini)
export OPENAI_API_KEY="your-key-here"

# Run the optimization script
./scripts/openai_agent_atlas.sh configs/optimize/default.yaml
```

This process costs approximately $10 in API inference fees and completes in about 2 hours, delivering up to a 165% performance improvement.

### 3. Integrate into Your Application

After optimization, you can integrate ATLAS in multiple ways depending on your existing setup:

#### Option A: Wrap Your Existing Agent

If you already have an agent or application, ATLAS can wrap it without any changes to your code:

```bash
# Wrap any OpenAI-compatible agent
scripts/openai_agent_atlas.sh configs/wrappers/openai_agent.yaml

# Wrap a LangChain agent  
scripts/openai_agent_atlas.sh configs/wrappers/langchain_agent.yaml

# Wrap a custom HTTP API
scripts/openai_agent_atlas.sh configs/wrappers/http_api.yaml

# Wrap a Python function
scripts/openai_agent_atlas.sh configs/wrappers/python_function.yaml
```

Example wrapper configuration:
```yaml
# configs/wrappers/your_agent.yaml
agent_type: "openai_assistant"
agent_config:
  api_key: "${OPENAI_API_KEY}"
  assistant_id: "asst_your_assistant_id"
  model: "gpt-4"
  
teacher_model: "Arc-Intelligence/ATLAS-8B-Thinking"
optimization_target: "accuracy"
```

#### Option B: Direct Integration

For tighter integration, use the `ATLASInference` class directly in your code:

```python
from examples.utils.atlas_inference import ATLASInference

# Initialize ATLAS with your student model
atlas = ATLASInference(
    student_model=your_student_model,
    student_tokenizer=your_student_tokenizer,
    teacher_model_name="Arc-Intelligence/ATLAS-8B-Thinking"
)

# Enhance any query
result = atlas.run_full_protocol("Your production query here")

print(f"Enhanced Response: {result['guided_response']}")
```

#### Option C: API-Based Integration

If your student model is API-based (OpenAI, Claude, etc.):

```python
# For API models, you only need the teacher locally
atlas = ATLASInference(
    student_api_client=your_openai_client,  # OpenAI/Claude/etc client
    teacher_model_name="Arc-Intelligence/ATLAS-8B-Thinking"
)

result = atlas.run_full_protocol("Your query", student_model="gpt-4")
enhanced_response = result['guided_response']
```

## Path B: Train a Custom Teacher (Advanced)

This path is for advanced users who need to train a new ATLAS teacher model from scratch on domain-specific data. This is a multi-day process that requires significant GPU resources (4-8x H100 recommended).

<Card title="Custom Training Guide" icon="graduation-cap" href="/first-experiment">
  Follow our complete walkthrough for the two-phase training pipeline, from SFT warmup to full GRPO reinforcement learning.
</Card>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Installation Guide"
    icon="download"
    href="/installation"
  >
    Detailed environment setup and dependencies.
  </Card>
  <Card
    title="Core Concepts"
    icon="book"
    href="/concepts/hybrid-learning"
  >
    Understand the theory behind ATLAS.
  </Card>
</CardGroup>
