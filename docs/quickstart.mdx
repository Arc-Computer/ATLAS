---
title: Quickstart
description: "Wrap your agent with the Atlas runtime and optionally bridge exported traces into the training stack."
sidebarTitle: Quickstart
icon: rocket
---

ATLAS is designed for production AI engineers. In under an hour you can plug the Teacher–Student loop into your existing agent, stream telemetry to your terminal, export training traces, and feed them into the GRPO trainer.

<Info>
**Choose your path:**
- **Runtime orchestration only** — Complete Steps 1–4 to wrap your agent with Atlas quality control and export traces (requires only `pip install arc-atlas`).
- **Runtime + training pipeline** — Continue through Step 5 to feed exported traces into GRPO training (requires the [Atlas Core training repository](https://github.com/Arc-Computer/ATLAS)).
</Info>

*End-to-end flow:* Install SDK → Configure BYOA → Run Teacher/Student/Reward loop → Export traces → Train.

## Step 1 — Install the runtime and set credentials

```bash
python -m pip install --upgrade arc-atlas
```

Create a `.env` (or use your secret manager) with the LLM keys you plan to call:

```bash
OPENAI_API_KEY=sk-...
# optional extras
GEMINI_API_KEY=...
AZURE_OPENAI_API_KEY=...
```

Load those variables before you orchestrate runs.

## Step 2 — Point ATLAS at your agent (Bring Your Own Agent, BYOA, in 5 minutes)

Atlas ships YAML templates for the three adapter families. Copy the closest example from the SDK (`configs/examples/`) and edit the `agent` block. For instance, to wrap a local HTTP service:

```yaml
# configs/my_http_agent.yaml
agent:
  type: http_api
  name: my-http-agent
  system_prompt: |
    You are the Atlas Student. Solve the task carefully and justify decisions.
  tools: []
  transport:
    base_url: http://localhost:8080/agent
    timeout_seconds: 60

student:
  max_plan_tokens: 1024
  max_step_tokens: 1024
  max_synthesis_tokens: 1024

teacher:
  llm:
    provider: openai
    model: gpt-4o-mini
    api_key_env: OPENAI_API_KEY
```

Swap the adapter for OpenAI (`type: openai`) or a local Python callable (`type: python`) if that fits your stack. The rest of the file—`student`, `teacher`, `rim`, `orchestration`—ships with sensible defaults. See [`SDK Configuration`](/sdk/configuration) for every field.

## Step 3 — Run the adaptive runtime with telemetry

Every call to `atlas.core.run` now begins with a triage dossier and capability probe. Those signals decide which lane the orchestrator chooses for the task:

- `auto` keeps latency low with a single-shot execution and no validation when confidence is high.
- `paired` runs single-shot but routes through a one-time certification check (ideal for first-run fingerprints).
- `coach` condenses the reviewed plan into a single step with validation plus an optional retry for lightweight supervision.
- `escalate` keeps the full stepwise plan, enabling rich guidance, retries, and detailed telemetry.

```python
from atlas.core import run

result = run(
    task="Summarize the latest AI news",
    config_path="configs/my_http_agent.yaml",
    stream_progress=True,
)
```

When `stream_progress=True` and stdout is interactive, `atlas.runtime.telemetry.ConsoleTelemetryStreamer` auto-enables to print the accepted plan (if any), retries, tool calls, lane choice, probe confidence, and reward scores as they happen. Set `stream_progress=False` to silence streaming, or leave it unset and Atlas will auto-detect.

<Note>
Need persistence? Add a `storage:` block in your YAML (see below) so every session lands in Postgres for export.
</Note>

## Step 4 — Persist and export traces

Enable storage:

```yaml
storage:
  database_url: postgresql://atlas:atlas@localhost:5432/atlas
  min_connections: 1
  max_connections: 5
```

After a run (or a batch of runs) export the traces to JSONL:

```bash
arc-atlas \
  --database-url postgresql://atlas:atlas@localhost:5432/atlas \
  --output traces/aime-batch.jsonl \
  --trajectory-event-limit 500 \
  --status succeeded
```

<Note>
Ensure Postgres is running and reachable before exporting. You can launch it with `docker compose up -d postgres` (using the repo's compose file) or `brew services start postgresql`.
</Note>

<Tip>
If another tool has claimed the `atlas` command on your machine, invoke the exporter with `python -m atlas.cli.export ...` or ensure `arc-atlas` resolves first on `PATH`.
</Tip>

Each JSONL record maps to the shared runtime schema (`task`, `adaptive_summary`, triage dossier, plan/step traces, per-step `reward` with nested judges, guidance history, validation, tool metadata, plus optional `artifacts` and `deliverable` payloads). Preview a single record with `jq`:

```bash
jq '.' traces/aime-batch.jsonl | head
```

---

## Training Integration (Optional)

<Note>
**Training integration (optional):** The remaining steps assume you have cloned the Atlas training repository and provisioned the required GPU resources. If you only need runtime orchestration, you can stop after Step 4.
</Note>

## Step 5 — Train or score with the runtime dataset

Point the core training stack at the exported file using the provided loader:

```yaml
# configs/data/runtime_traces.yaml
make_dataset_fn:
  _target_: custom_data.runtime_trace_data.get_runtime_trace_dataset
  export_path: traces/aime-batch.jsonl
  eval_split_ratio: 0.1
```

Now launch GRPO or a scoring job exactly as you would with any other dataset. The trainer sees the same reward structure that the runtime produced, so step-level improvements translate directly into policy updates. See [`Training Your Own Teacher`](/training/offline/grpo-training) for the full recipe and the [`Training Configuration`](/training/configuration) guide for Hydra overrides.

Run the helper CLI to kick off training without assembling overrides:

```bash
python scripts/run_offline_pipeline.py --export-path traces/aime-batch.jsonl
```

Pass `--wandb-project`, `--output-dir`, or additional `--override` flags as needed.

## Optional — Advanced optimisation

Once the loop is running end-to-end, you can layer on additional optimization strategies:

- **Custom judges** for domain-specific scoring.
- **Full GRPO retraining** when you need a bespoke teacher checkpoint.
- **SDK continual learning** if you need rapid, task-specific adaptation between offline runs.

Each of these uses the same exported traces, so you can adopt them incrementally as your workloads grow.

## Checklist

**SDK runtime (required):**
- [x] Installed `arc-atlas` and exported API keys
- [x] Wrapped the agent with a YAML config
- [x] Streamed telemetry for a real task
- [x] Enabled Postgres storage and exported JSONL traces

**Training integration (optional):**
- [x] Consumed traces in the training stack

You now have the full Atlas workflow configured. Completing Steps 1–4 gives you runtime orchestration with exportable traces; finishing Step 5 bridges those traces into the training stack for GRPO training.
