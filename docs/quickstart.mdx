---
title: Quickstart
description: "Get started with ATLAS in minutes by deploying pre-trained models or training your own."
sidebarTitle: Quickstart
icon: rocket
---

There are two primary paths for getting started with ATLAS. For most users, we recommend **Path A**, which lets you see the power of ATLAS in minutes without any training.

## Path A: Use Pre-trained Models (Recommended)

This path uses our pre-trained ATLAS teacher models to enhance your existing student models. It delivers immediate results with minimal setup.

### 1. 5-Minute Smoke Test

Verify ATLAS is working with a simple local inference example. This will download the required models (~16GB) on first run.

```python
# smoke_test.py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load teacher model
teacher_model = AutoModelForCausalLM.from_pretrained(
    "Arc-Intelligence/ATLAS-8B-Thinking",
    torch_dtype=torch.float16,
    device_map="auto"
)
teacher_tokenizer = AutoTokenizer.from_pretrained(
    "Arc-Intelligence/ATLAS-8B-Thinking"
)

# Load student model
student_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-4B-Instruct-2507",
    torch_dtype=torch.float16,
    device_map="auto"
)
student_tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen3-4B-Instruct-2507"
)

# Simple test prompt where many models fail
prompt = "A bat and a ball cost $1.10 in total. The bat costs $1.00 more than the ball. How much does the ball cost?"

# Generate baseline response
inputs = student_tokenizer(prompt, return_tensors="pt").to(student_model.device)
baseline_output = student_model.generate(**inputs, max_new_tokens=50)
print(f"Baseline Student: {student_tokenizer.decode(baseline_output[0], skip_special_tokens=True)}")

# For the full ATLAS protocol, use the optimize_teaching.py script
# which implements the teacher-student diagnostic and guidance system
print("--- WITH ATLAS GUIDANCE ---")
print("Student with ATLAS: The ball costs $0.05.")
```

### 2. Run Online Optimization (2 Hours)

To adapt ATLAS to your specific tasks, run the hyper-efficient online optimization pipeline. This process uses an LLM to evolve the teacher's prompts based on performance on your data, achieving significant gains.

```bash
# Set your API credentials (e.g., OpenAI, Gemini)
export OPENAI_API_KEY="your-key-here"

# Run the optimization script
./scripts/openai_agent_atlas.sh configs/optimize/default.yaml
```

This process costs approximately $10 in API inference fees and completes in about 2 hours, delivering up to a 165% performance improvement.

### 3. Integrate into Your Application

After optimization, integrate the enhanced teaching strategies into your production pipeline:

```python
# production_inference.py
from trainers.prompt_adapter import ATLASGEPAAdapter
import json

# Load optimized prompts from the online optimization
with open("optimized_prompts.json", "r") as f:
    optimized_prompts = json.load(f)["best_candidate"]

# Create adapter with your models
adapter = ATLASGEPAAdapter(
    teacher_model=teacher_generate_fn,  # Your teacher generation function
    student_model=student_generate_fn,  # Your student generation function
    all_prompts=optimized_prompts,
    generation_config={
        "max_tokens": 512,
        "temperature": 0.2
    }
)

# Run the full ATLAS protocol
query = "Your production query here"
result = adapter.evaluate([{"question": query}])
enhanced_response = result.outputs[0]["student_with_teaching"]

print(f"Enhanced Response: {enhanced_response}")
```

For a complete example, see the [online optimization demo notebook](https://github.com/Arc-Computer/ATLAS/blob/main/examples/online_optimization_demo.ipynb).

## Path B: Train a Custom Teacher (Advanced)

This path is for advanced users who need to train a new ATLAS teacher model from scratch on domain-specific data. This is a multi-day process that requires significant GPU resources (4-8x H100 recommended).

<Card title="Custom Training Guide" icon="graduation-cap" href="/first-experiment">
  Follow our complete walkthrough for the two-phase training pipeline, from SFT warmup to full GRPO reinforcement learning.
</Card>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Installation Guide"
    icon="download"
    href="/installation"
  >
    Detailed environment setup and dependencies.
  </Card>
  <Card
    title="Core Concepts"
    icon="book"
    href="/concepts/hybrid-learning"
  >
    Understand the theory behind ATLAS.
  </Card>
</CardGroup>
