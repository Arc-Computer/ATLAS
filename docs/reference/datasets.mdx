---
title: Datasets
description: Official ATLAS training and evaluation datasets
sidebarTitle: Datasets
icon: database
---

## Available Datasets

ATLAS provides curated datasets for training adaptive teachers and evaluating system performance.

## Primary Dataset

### Arc-ATLAS-Teach-v0

<Card title="View on Hugging Face" icon="hugging-face" href="https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v0">
  Comprehensive teaching interaction dataset for RL training
</Card>

**Purpose:** Train teacher models to provide adaptive guidance across diverse tasks

**Statistics:**
- **Total examples**: 100,000+ teaching interactions
- **Task domains**: Mathematics, reasoning, coding, debugging
- **Formats**: SFT and RL training splits
- **Languages**: English

**Data Schema:**

```json
{
  "prompt": "The problem or task requiring solution",
  "ground_truth": "Correct answer or solution",
  "student_response": "Initial student attempt",
  "teaching": "Adaptive guidance provided",
  "enhanced_response": "Student response after teaching",
  "baseline_score": 0.3,
  "with_teaching_score": 0.9,
  "reward": 0.6,
  "problem_id": "unique_identifier",
  "student_level": "weak|moderate|strong",
  "domain": "math|reasoning|code|debug"
}
```

**Loading the Dataset:**

```python
from datasets import load_dataset

# Load for supervised fine-tuning
sft_data = load_dataset(
    "Arc-Intelligence/Arc-ATLAS-Teach-v0",
    "sft",
    split="train"
)

# Load for reinforcement learning
rl_data = load_dataset(
    "Arc-Intelligence/Arc-ATLAS-Teach-v0",
    "rl",
    split="train"
)

# Load validation set
val_data = load_dataset(
    "Arc-Intelligence/Arc-ATLAS-Teach-v0",
    "rl",
    split="validation"
)
```

**File Structure:**
```
Arc-ATLAS-Teach-v0/
├── training/
│   ├── sft.jsonl         # Supervised fine-tuning data
│   └── rl.jsonl          # Reinforcement learning data
└── validation/
    └── rl.jsonl          # Held-out validation
```

## Domain-Specific Subsets

### Mathematics Subset

**Focus:** Step-by-step mathematical reasoning

**Example:**
```json
{
  "prompt": "Sarah has 24 apples. She gives 1/3 to her brother...",
  "ground_truth": "12",
  "teaching": "Break down: 1) Calculate 1/3 of 24 = 8..."
}
```

**Filtering:**
```python
math_data = dataset.filter(lambda x: x['domain'] == 'math')
```

### Code Generation Subset

**Focus:** Programming tasks and debugging

**Example:**
```json
{
  "prompt": "Write a function to validate email addresses",
  "ground_truth": "def validate_email(email):...",
  "teaching": "Consider regex pattern, edge cases like..."
}
```

**Filtering:**
```python
code_data = dataset.filter(lambda x: x['domain'] == 'code')
```

### SRE/Debugging Subset

**Focus:** System reliability and debugging scenarios

**Example:**
```json
{
  "prompt": "Service returns 503 errors intermittently",
  "ground_truth": "Check service mesh configuration...",
  "teaching": "Systematic approach: 1) Check Istio configs..."
}
```

**Filtering:**
```python
sre_data = dataset.filter(lambda x: x['domain'] == 'debug')
```

## Data Quality Metrics

### Coverage Statistics

| Domain | Examples | Avg Length | Unique Patterns |
|--------|----------|------------|-----------------|
| Mathematics | 35,000 | 250 tokens | 500+ |
| Code Generation | 30,000 | 400 tokens | 800+ |
| Reasoning | 25,000 | 300 tokens | 600+ |
| Debugging | 10,000 | 350 tokens | 400+ |

### Performance Baselines

| Metric | Baseline | w/ Dual-Agent Loop | Improvement |
|--------|----------|-------------------------|-------------|
| Accuracy | 62.3% | 78.0% | +15.7% |
| Completion | 69% | 100% | +31% |
| Token Efficiency | 100% | 50% | -50% |

<Note>
  These figures reflect the closed-loop runtime plus GRPO baseline. Online continual learning now lives in the [`atlas-sdk`](https://github.com/Arc-Computer/atlas-sdk) runtime if you need task-specific adaptation between offline training runs.
</Note>

## Creating Custom Datasets

### Data Format Requirements

Your dataset should follow this structure:

```python
{
    "prompt": str,           # Required: Task description
    "ground_truth": str,     # Required: Correct solution
    "metadata": {            # Optional: Additional context
        "difficulty": str,
        "source": str,
        "tags": List[str]
    }
}
```

### Preprocessing Pipeline (JSONL exports)

Use the runtime helpers that ship in this repository to turn SDK exports into trainer-ready splits:

```python
from transformers import AutoTokenizer
from custom_data.runtime_trace_data import get_runtime_trace_dataset

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
splits = get_runtime_trace_dataset(
    tokenizer=tokenizer,
    export_path="traces/runtime.jsonl",  # Generated via `arc-atlas export …`
    eval_split_ratio=0.1,
    dataset_max_samples=5000,
)

train_ds = splits["train_dataset"]
eval_ds = splits["eval_dataset"]
```

For Postgres-backed workflows, query the SDK database directly and convert records with `trainers.runtime_dataset`:

```python
from atlas.training_data import get_training_sessions
from trainers.runtime_dataset import sessions_to_rl_records

sessions = get_training_sessions(
    db_url="postgresql://atlas:atlas@localhost:5433/atlas",
    min_reward=0.8,
    learning_key="security-review",
)

records = sessions_to_rl_records(sessions)
```

`records` is a list of dictionaries that any Hugging Face `Dataset` can ingest (the same structure Hydra configs consume via `custom_data.runtime_trace_data`). See the [Training Data Pipeline](/training/offline/training-data-pipeline) guide for additional filters and batching helpers.

> **GKD alignment note:** Every conversation record now carries `prompt_text` (serialized messages excluding the final assistant turn) and `completion_text` (the assistant response the student learns to mimic). These fields let the distillation pipeline re-render prompts with both the student and teacher tokenizers so cross-tokenizer KL is computed in each model’s native chat template.

### Quality Validation

Inspect coverage with standard Python tooling—you already have `datasets` installed for training:

```python
from collections import Counter
from datasets import Dataset

dataset = Dataset.from_list(records)
lengths = [len(example["step_trace"].split()) for example in dataset]
domains = Counter(example["session_metadata"].get("domain", "unknown") for example in dataset)

print(f"Examples: {len(dataset)}")
print(f"Avg step length: {sum(lengths)/len(lengths):.1f} tokens")
print(f"Domains: {domains}")
```

Pair these quick checks with any in-house validators your team already maintains. The key is to keep the format (`prompt`, `student_response`, guidance, rewards) identical to what the SDK emits so Atlas Core can reuse the traces without custom glue code.

## Contributing Data

We welcome contributions to improve ATLAS datasets:

1. **Format your data** according to the schema
2. **Validate quality** using provided tools
3. **Test with models** to ensure compatibility
4. **Submit PR** with data and documentation

See [Contributing Guidelines](https://github.com/Arc-Computer/ATLAS/blob/main/CONTRIBUTING.md) for details.

## License and Citation

Datasets are released under Apache 2.0 license.

If you use these datasets, please cite:

```bibtex
@dataset{atlas_teach_v0,
  title={Arc-ATLAS-Teach-v0: Adaptive Teaching Dataset},
  author={Arc Intelligence Team},
  year={2024},
  publisher={Hugging Face},
  url={https://huggingface.co/datasets/Arc-Intelligence/Arc-ATLAS-Teach-v0}
}
```

## Next Steps

<CardGroup cols="2">
  <Card title="Training Guide" icon="graduation-cap" href="/training/offline/grpo-training">
    Train models with datasets
  </Card>
  <Card title="Models" icon="robot" href="/reference/models">
    Pre-trained ATLAS models
  </Card>
  <Card title="Adaptive Tool Use" icon="wrench" href="/examples/adaptive-tool-use">
    Production example with MCP integration
  </Card>
  <Card title="Technical Report" icon="file-pdf" href="/reference/technical-report">
    Dataset methodology
  </Card>
</CardGroup>
