---
title: Configuration Guide
description: Practical guide to customizing ATLAS for your use case
sidebarTitle: Configuration
icon: sliders
---

<Note>
**Looking for SDK runtime configuration?** See the [SDK Configuration Reference](/sdk/configuration) for YAML keys that control `atlas.core.run`.

This page focuses on training and optimization configuration for the ATLAS teacher stack.
</Note>

## Quick Reference

ATLAS uses configuration files to customize behavior without code changes. Here's what each directory contains:

```
configs/
‚îú‚îÄ‚îÄ wrappers/          üéØ START HERE - Wrap your existing agent
‚îú‚îÄ‚îÄ optimize/          üìà Optimization settings (API or vLLM)
‚îú‚îÄ‚îÄ examples/          ‚ö° Ready-to-run configs
‚îú‚îÄ‚îÄ data/              üíæ Dataset configurations
‚îú‚îÄ‚îÄ demo/              üî¨ Full demo scenarios
‚îú‚îÄ‚îÄ rim_config.yaml    üèÜ Reward system configuration
‚îú‚îÄ‚îÄ model/             ü§ñ Model architectures (advanced training)
‚îú‚îÄ‚îÄ run/               üî¨ Training experiments (advanced training)
‚îî‚îÄ‚îÄ trainer/           ‚öôÔ∏è RL algorithms (advanced training)
```

<Note>
**For Getting Started:** Start with `wrappers/` and `optimize/`. The `model/`, `run/`, and `trainer/` directories are for advanced RL training (see [Architecture docs](/architecture/config-system)).

**Root-level configs:**
- `rim_config.yaml` - Reward system settings (judges, models, thresholds). See [Reward Design](/concepts/reward-design).
- `train.yaml` - Global training defaults (advanced users only)
</Note>

## Common Configurations

### Wrapping Your Existing Agent

A common starting point is integrating ATLAS with your existing agent. All integration configs live in `configs/wrappers/`.

<Tabs>
  <Tab title="HTTP API">
    Wrap any REST API endpoint with ATLAS teaching:

    ```yaml
    # configs/wrappers/my_api_agent.yaml
    user_agent:
      type: custom
      config:
        integration_type: http_api
        endpoint: "http://localhost:8000/chat"
        prompt_field: "message"
        response_field: "response"
        headers:
          Authorization: "Bearer YOUR_API_KEY"
        timeout: 300

    # ATLAS teacher model
    teacher_model: Arc-Intelligence/ATLAS-8B-Thinking

    # Dataset to test on
    trainset: arc-atlas-rl
    max_examples: 10

    # Enable compatibility mode (recommended for custom agents)
    compatibility_mode: true

    generation_config:
      max_tokens: 2048
      temperature: 0.7
      diagnostic_max_tokens: 500
    ```

    **How to run:**
    ```bash
    ./scripts/openai_agent_atlas.sh configs/wrappers/my_api_agent.yaml
    ```
  </Tab>

  <Tab title="Python Function">
    Integrate a Python function directly:

    ```yaml
    # configs/wrappers/my_python_agent.yaml
    user_agent:
      type: custom
      config:
        integration_type: python_function
        module_path: "/path/to/your/agent.py"
        function_name: "generate"

    teacher_model: Arc-Intelligence/ATLAS-8B-Thinking
    trainset: arc-atlas-rl
    max_examples: 10
    compatibility_mode: true

    generation_config:
      max_tokens: 2048
      temperature: 0.7
    ```

    Your Python function should have this signature:
    ```python
    # agent.py
    def generate(prompt: str) -> str:
        """Your agent's generation function."""
        # Your agent logic here
        return response_text
    ```
  </Tab>

  <Tab title="CLI Command">
    Wrap a command-line tool:

    ```yaml
    # configs/wrappers/my_cli_agent.yaml
    user_agent:
      type: custom
      config:
        integration_type: cli_command
        command: "python agent.py '{prompt}'"

    teacher_model: Arc-Intelligence/ATLAS-8B-Thinking
    trainset: arc-atlas-rl
    max_examples: 10
    compatibility_mode: true
    ```

    The `{prompt}` placeholder will be replaced with the actual prompt.
  </Tab>

  <Tab title="Real Working Example">
    Here's the complete `openai_existing_agent.yaml` from the codebase:

    ```yaml
    # Test YOUR existing agent with ATLAS teaching

    # Your agent configuration
    user_agent:
      type: custom
      config:
        # Choose ONE integration method:

        # HTTP API
        integration_type: http_api
        endpoint: "http://localhost:8000/chat"
        prompt_field: "message"
        response_field: "response"
        request_template:
          message: ""  # Will be filled with prompt
        headers:
          Authorization: "Bearer YOUR_KEY"
        timeout: 300

        # Python Function
        # integration_type: python_function
        # module_path: "/path/to/your/agent.py"
        # function_name: "generate"

        # CLI Command
        # integration_type: cli_command
        # command: "python agent.py '{prompt}'"

    # ATLAS teacher (optimized for teaching)
    teacher_model: Arc-Intelligence/ATLAS-8B-Thinking

    # Dataset to test on
    trainset: arc-atlas-rl
    max_examples: 10

    # Compatibility mode settings
    compatibility_mode: true

    # Teaching optimization
    max_metric_calls: 100
    reflection_lm: gpt-4
    trace_storage: traces/compatibility_traces.jsonl
    output: compatibility_results.json

    # vLLM settings (optional)
    use_vllm_client: false
    vllm_host: localhost
    vllm_port: 8765

    # Worker settings
    max_litellm_workers: 10

    generation_config:
      max_tokens: 2048
      temperature: 0.7
      diagnostic_max_tokens: 500
      timeout: 300
      request_timeout: 300

    # Seed prompts for compatibility mode
    seed_prompts:
      teacher_adaptive_template: "You are an expert teacher. The student gave this response: {baseline_response}\n\nTo the question: {question}\n\nProvide focused teaching to help them improve. Wrap teaching in <teaching> tags."

    # GEPA optimization settings
    gepa_config:
      candidate_selection_strategy: pareto
      skip_perfect_score: false
      reflection_minibatch_size: 5
      perfect_score: 1.0
      module_selector: single
      display_progress_bar: true

    # WandB logging (optional)
    wandb:
      enabled: false
      project: atlas-compatibility-testing
      run_name: null
      tags:
        - compatibility
        - existing-agent
      notes: "Testing existing agent with ATLAS teaching"
    ```
  </Tab>
</Tabs>

### Online Optimization Settings

Once you have your agent wrapped, configure GEPA optimization in `configs/optimize/`.

<Tabs>
  <Tab title="API Models (No GPU)">
    Use API models for optimization (recommended for getting started):

    ```yaml
    # configs/optimize/my_api_optimization.yaml
    defaults:
      - default

    student_model: gpt-4o
    teacher_model: Arc-Intelligence/ATLAS-8B-Thinking
    reflection_lm: gpt-5

    max_metric_calls: 50
    ```

    **Cost:** ~$10 for 50 iterations with 10 examples

    **Time:** ~2 hours

    **Run it:**
    ```bash
    python optimize_teaching.py --config-name my_api_optimization
    ```
  </Tab>

  <Tab title="vLLM Server (Local GPU)">
    Use local models via vLLM for cost-free optimization:

    ```yaml
    # configs/optimize/my_vllm_optimization.yaml
    defaults:
      - default

    student_model: Qwen/Qwen3-4B
    teacher_model: Arc-Intelligence/ATLAS-8B-Thinking
    reflection_lm: gpt-4

    use_vllm_client: true
    vllm_host: localhost
    vllm_port: 8765

    max_metric_calls: 100
    ```

    **Setup vLLM first:**
    ```bash
    # Start vLLM server
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen3-4B \
      --port 8765
    ```

    **Requirements:** GPU with 16GB+ VRAM
  </Tab>

  <Tab title="Default Config">
    Here's the base `optimize/default.yaml` that other configs inherit from:

    ```yaml
    max_examples: 10

    student_model: Qwen/Qwen3-4B
    teacher_model: Arc-Intelligence/ATLAS-8B-Thinking

    reflection_lm: gpt-5

    max_metric_calls: 50

    trace_storage: traces/optimize_traces.jsonl
    output: optimized_prompts.json

    use_vllm_client: false
    vllm_host: localhost
    vllm_port: 8765

    max_litellm_workers: 100

    generation_config:
      max_tokens: 2048
      temperature: 0.7
      diagnostic_max_tokens: 500
      timeout: 300
      request_timeout: 300

    seed_prompts:
      teacher_adaptive_template: "You are an expert teacher helping a student solve a problem.\n\nQuestion: {question}\n\nStudent's initial approach: {approach}\n\nAnalyze the student's approach and provide focused teaching that addresses their specific misconceptions or gaps. Wrap your teaching in <teaching> tags.\n\n<teaching>\nYour adaptive teaching here\n</teaching>"
      student_diagnostic_template: "You are a student working through a problem step by step.\n\nQuestion: {question}\n\nBriefly outline your approach to solving this problem. Focus on the key steps and reasoning you would use:"
      student_with_teaching_template: "You received helpful teaching from an expert. Apply this teaching to solve the problem.\n\nQuestion: {question}\n\nTeaching provided: {teaching}\n\nNow solve the problem using the teaching above. Show your work and provide the final answer:"

    fixed_prompts:
      student_baseline_template: "Solve the following problem step by step. Show your work clearly and provide the final answer.\n\nQuestion: {question}\n\nSolution:"

    gepa_config:
      candidate_selection_strategy: pareto
      skip_perfect_score: false
      reflection_minibatch_size: 5
      perfect_score: 1.0
      module_selector: all
      display_progress_bar: false

    wandb:
      enabled: false
      project: atlas-teaching-optimization
      run_name: null
      tags:
        - gepa
        - teaching-optimization
      notes: "ATLAS teaching prompt optimization using GEPA"
    ```
  </Tab>
</Tabs>

### Dataset Configuration

Configure which dataset to use for optimization in `configs/data/`.

<Tabs>
  <Tab title="Built-in Datasets">
    Use ATLAS curated datasets from HuggingFace:

    ```yaml
    # configs/data/my_dataset.yaml
    dataset_id_or_path: Arc-Intelligence/Arc-ATLAS-Teach-v0
    dataset_split: rl
    dataset_max_samples: null
    eval_split_ratio: 0.1

    data_log_name: my_dataset

    make_dataset_fn:
      _target_: custom_data.arc_atlas_rl_data.get_arc_atlas_rl_dataset
      dataset_id_or_path: ${dataset_id_or_path}
      dataset_split: ${dataset_split}
      dataset_max_samples: ${dataset_max_samples}
      eval_split_ratio: ${eval_split_ratio}
    ```

    **Available datasets:**
    - `arc-atlas-rl`: RL training data (default)
    - `arc-atlas-sft`: SFT training data
    - See `configs/data/` for more options
  </Tab>

  <Tab title="Custom Dataset">
    Point to your own dataset:

    ```yaml
    # configs/data/custom_dataset.yaml
    dataset_id_or_path: your-username/your-dataset
    dataset_split: train
    dataset_max_samples: 100  # Limit for testing
    eval_split_ratio: 0.1

    data_log_name: custom_dataset

    # Use custom data loader function
    make_dataset_fn:
      _target_: custom_data.your_loader.load_custom_data
      dataset_id_or_path: ${dataset_id_or_path}
      dataset_split: ${dataset_split}
    ```

    Your dataset should have these fields:
    ```json
    {
      "prompt": "The task or question",
      "ground_truth": "Expected answer (optional)",
      "metadata": {}
    }
    ```
  </Tab>

  <Tab title="Use in Wrapper">
    Reference your dataset in wrapper config:

    ```yaml
    # configs/wrappers/my_agent.yaml
    defaults:
      - openai_existing_agent

    # Override dataset
    trainset: custom_dataset  # References configs/data/custom_dataset.yaml
    max_examples: 20
    ```
  </Tab>
</Tabs>

## Configuration Patterns

### Pattern 1: Quick Start with Defaults

Use the quickstart config which inherits sensible defaults:

```yaml
# configs/examples/quickstart.yaml
defaults:
  - ../wrappers/openai_existing_agent

# Override only what you need
# teacher_model: gpt-4o
# student_model: gpt-4o-mini
# max_metric_calls: 40
```

**Run it:**
```bash
./scripts/openai_agent_atlas.sh configs/examples/quickstart.yaml
```

### Pattern 2: Environment Variables

Use environment variables for secrets and dynamic values:

```yaml
# configs/wrappers/secure_agent.yaml
user_agent:
  type: custom
  config:
    integration_type: http_api
    endpoint: "${API_ENDPOINT:http://localhost:8000}"  # Fallback to localhost
    headers:
      Authorization: "Bearer ${API_KEY}"  # Must be set in environment

vllm_host: ${VLLM_HOST:localhost}
vllm_port: ${VLLM_PORT:8765}
```

**Set them before running:**
```bash
export API_ENDPOINT="https://api.example.com"
export API_KEY="your-secret-key"
export VLLM_HOST="10.0.0.5"
export VLLM_PORT="8000"

./scripts/openai_agent_atlas.sh configs/wrappers/secure_agent.yaml
```

### Pattern 3: Compose Configs with Hydra

Create task-specific configs that inherit from base configs:

```yaml
# configs/my_custom_experiment.yaml
defaults:
  - wrappers/openai_existing_agent
  - optimize/api_models
  - data/arc_atlas_rl

# Override specific fields
max_examples: 50
max_metric_calls: 100

generation_config:
  max_tokens: 4096
  temperature: 0.9

wandb:
  enabled: true
  project: my-atlas-project
  tags:
    - experiment-1
```

## Key Configuration Fields

<AccordionGroup>
  <Accordion title="Model Selection">
    **`student_model`**: The model being improved
    - API models: `gpt-4o`, `gpt-4o-mini`, `claude-3-opus`
    - Local models: `Qwen/Qwen3-4B`, `meta-llama/Llama-3-8B`

    **`teacher_model`**: The model providing guidance
    - Recommended: `Arc-Intelligence/ATLAS-8B-Thinking` (reasoning tasks)
    - Alternative: `Arc-Intelligence/ATLAS-8B-Instruct` (coding tasks)
    - API: `gpt-5`, `gpt-4o` (requires API access)

    **`reflection_lm`**: Model for GEPA prompt optimization
    - Recommended: `gpt-5` or `gpt-4o` (highest quality)
    - Budget: `gpt-4o-mini` (faster, cheaper)
  </Accordion>

  <Accordion title="Generation Settings">
    **`generation_config.max_tokens`**: Maximum output length (default: 2048)

    **`generation_config.temperature`**: Sampling randomness (0.0-1.0)
    - Lower (0.3-0.5): Focused, deterministic responses
    - Higher (0.7-0.9): Creative, diverse responses

    **`generation_config.diagnostic_max_tokens`**: Max tokens for diagnostic probe (default: 500)

    **`generation_config.timeout`**: Request timeout in seconds (default: 300)
  </Accordion>

  <Accordion title="GEPA Parameters">
    **`max_metric_calls`**: Optimization budget (default: 50)
    - Development: 20-50 iterations (~$5-10)
    - Production: 100-200 iterations (~$20-40)

    **`gepa_config.candidate_selection_strategy`**: How to select candidates
    - `pareto`: Multi-objective optimization (recommended)
    - `greedy`: Single best candidate

    **`gepa_config.module_selector`**: Which prompts to optimize
    - `all`: Optimize all teaching prompts (default)
    - `single`: Optimize only teacher_adaptive_template (compatibility mode)

    **`gepa_config.reflection_minibatch_size`**: Examples for reflection (default: 5)
  </Accordion>

  <Accordion title="Integration Settings">
    **`compatibility_mode`**: Enable for custom agents (recommended: `true`)

    **`use_vllm_client`**: Use local vLLM server (default: `false`)
    - `true`: Connect to vLLM at `vllm_host:vllm_port`
    - `false`: Use API models

    **`max_litellm_workers`**: Parallel API workers (default: 100)
    - Higher: Faster but more API rate limit risk
    - Lower: Safer for rate limits

    **`trace_storage`**: Where to save interaction traces (default: `traces/optimize_traces.jsonl`)

    **`output`**: Where to save optimized prompts (default: `optimized_prompts.json`)
  </Accordion>

  <Accordion title="Monitoring Settings">
    **`wandb.enabled`**: Enable Weights & Biases logging (default: `false`)

    **`wandb.project`**: W&B project name

    **`wandb.tags`**: Tags for this run (list)

    **`wandb.notes`**: Run description

    Example:
    ```yaml
    wandb:
      enabled: true
      project: atlas-development
      tags:
        - experiment-1
        - baseline
      notes: "Testing ATLAS with our custom agent"
    ```
  </Accordion>
</AccordionGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Error: Could not find wrappers.my_agent">
    **Problem:** Config file not found in expected location

    **Solution:** Ensure config is in correct directory
    ```bash
    # Correct structure
    configs/wrappers/my_agent.yaml  ‚úÖ

    # Wrong - won't be found
    my_agent.yaml                   ‚ùå
    configs/my_agent.yaml          ‚ùå
    ```
  </Accordion>

  <Accordion title="Error: KeyError: 'response_field'">
    **Problem:** Your API returns different field name than configured

    **Debug:** Check actual API response
    ```bash
    curl -X POST http://localhost:8000/chat \
      -H "Content-Type: application/json" \
      -d '{"message": "test"}'

    # If response is {"data": {"text": "..."}}, update config:
    response_field: "data.text"  # Use dot notation for nested fields
    ```
  </Accordion>

  <Accordion title="Error: Timeout after 60s">
    **Problem:** Default timeout too short for slow models

    **Solution:** Increase timeout in generation_config
    ```yaml
    generation_config:
      timeout: 600  # 10 minutes
      request_timeout: 600
    ```
  </Accordion>

  <Accordion title="Error: API rate limit exceeded">
    **Problem:** Too many parallel workers hitting rate limits

    **Solution:** Reduce worker count
    ```yaml
    max_litellm_workers: 10  # Lower from default 100
    ```
  </Accordion>

  <Accordion title="Teaching not improving performance">
    **Problem:** Teaching prompts not effective for your task

    **Solutions:**
    1. Verify teacher model is stronger than student
    2. Customize seed_prompts for your domain
    3. Increase max_metric_calls for more optimization
    4. Check that examples in trainset are relevant
  </Accordion>

  <Accordion title="Out of memory with vLLM">
    **Problem:** Model too large for GPU

    **Solutions:**
    ```yaml
    # Use smaller model
    student_model: Qwen/Qwen3-4B  # Instead of 8B or 70B

    # Or use quantization (add to vLLM server launch)
    # --quantization awq
    ```
  </Accordion>
</AccordionGroup>

## Quick Decision Guide

**Which config directory do I need?**

```
‚îå‚îÄ I want to integrate my existing agent
‚îÇ  ‚îî‚îÄ configs/wrappers/
‚îÇ
‚îå‚îÄ I want to optimize teaching prompts
‚îÇ  ‚îî‚îÄ configs/optimize/
‚îÇ     ‚îú‚îÄ Use API models ‚Üí api_models.yaml
‚îÇ     ‚îî‚îÄ Have local GPU ‚Üí vllm_server.yaml
‚îÇ
‚îå‚îÄ I want to use a different dataset
‚îÇ  ‚îî‚îÄ configs/data/
‚îÇ     ‚îú‚îÄ Use built-in ‚Üí arc_atlas_rl.yaml
‚îÇ     ‚îî‚îÄ Use custom ‚Üí create new yaml
‚îÇ
‚îå‚îÄ I want to train teacher from scratch (advanced)
‚îÇ  ‚îî‚îÄ configs/run/ + configs/trainer/
‚îÇ     ‚îî‚îÄ See Architecture docs for details
‚îÇ
‚îî‚îÄ I just want to test quickly
   ‚îî‚îÄ configs/examples/quickstart.yaml
```

## Next Steps

<CardGroup cols="2">
  <Card title="Wrap Your Agent" icon="plug" href="/examples/online_optimization">
    See complete example of wrapping an existing agent
  </Card>
  <Card title="Run Optimization" icon="bolt" href="/training/online/optimize-with-atlas">
    Use GEPA to improve teaching prompts
  </Card>
  <Card title="Advanced Config System" icon="gears" href="/architecture/config-system">
    Deep dive into Hydra composition for training
  </Card>
  <Card title="Production Deployment" icon="rocket" href="/integration/production-deployment">
    Deploy optimized configs at scale
  </Card>
</CardGroup>
