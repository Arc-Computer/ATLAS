---
title: Inference-Only Integration
description: Deploy ATLAS teaching capabilities using API models without training infrastructure
sidebarTitle: Inference Only
icon: play
---

<Note>
**Setup time**: 5 minutes • **Reading time**: 10 minutes • **Difficulty**: Beginner
</Note>

## Overview

The simplest way to leverage ATLAS is through API-based inference. This approach uses the ATLAS protocol with any API-compatible models (OpenAI, Anthropic, Gemini, etc.) to enhance your agent's performance without requiring local infrastructure or training.

This is the fastest path to seeing ATLAS in action and is production-ready for API-based agents.

## Prerequisites

- Python 3.11+
- OpenAI API key (or other provider)
- Gemini API key (for RIM reward scoring)
- ~$10 in API credits for evaluation and optimization

## Quick Start (5 Minutes)

<Steps>
  <Step title="Install the managed SDK">
    Install the packaged runtime and exporter from PyPI:

    ```bash
    python -m pip install --upgrade arc-atlas
    ```

    <Note>
    Clone the repository and install `requirements-py312.txt` only if you need to modify the offline training stack or extend reward tooling.
    </Note>
  </Step>

  <Step title="Set API Keys">
    Configure your API credentials:

    ```bash
    export OPENAI_API_KEY="sk-..."
    export GEMINI_API_KEY="your-gemini-key"
    ```

    <Info>
    OpenAI is used for teacher/student models. Gemini is used for the RIM reward judges that score improvements.
    </Info>
    <Tip>
    Prefer storing credentials in a `.env` file and loading them with your process manager so they never land in shell history.
    </Tip>
  </Step>

  <Step title="Run an adaptive episode">
    Call the SDK runtime so each task flows through triage, capability probing, and lane routing:

    ```python
    from atlas import core

    result = core.run(
        task="Investigate intermittent 503 errors in checkout",
        config_path="configs/examples/http_agent.yaml",
        stream_progress=True,
    )

    print("Final answer:", result.final_answer)
    ```

    The console stream shows:

    1. Triage dossier metadata (risks, signals, persona hints).
    2. Capability probe decision (`mode=coach confidence=0.55 evidence=[...]`).
    3. Lane-specific execution (`auto`/`paired` single-shot versus `coach`/`escalate` stepwise retries).
    4. Session reward plus `adaptive_summary`, `personas_used`, and any promotion updates.
  </Step>

  <Step title="Persist and export runs">
    Capture runs for analysis or training by enabling storage and exporting JSONL traces:

    ```bash
    atlas storage up                         # optional helper to launch Postgres
    arc-atlas \
      --database-url postgresql://atlas:atlas@localhost:5432/atlas \
      --output traces/runtime.jsonl \
      --trajectory-event-limit 500 \
      --status succeeded
    ```

    Each record includes the triage dossier, adaptive summary (lane, confidence, certification), plan/step traces, persona usage/update events, and the reward breakdown—exactly what the training stack expects.
  </Step>
</Steps>

## Patterns for Production Integration

- **Inline orchestration** — Call `atlas.core.run` directly from your service and work with the returned `Result` object (plan, per-step traces, final answer). `ExecutionContext.get().metadata["adaptive_summary"]` exposes lane, confidence, and certification flags in real time.
- **Job runner / queue** — For batched workloads, wrap `core.run` inside a worker that records adaptive summaries and reward scores to your observability stack. Schedule `arc-atlas` to materialise datasets for the training team.
- **Custom adapters** — If your agent is exposed via HTTP, scaffold a dossier builder with `atlas triage init --domain sre --output sre_dossier.py`, point `adaptive_teaching.triage_adapter` at it, and let the probe choose the right lane per request.

## Best Practices

1. **Watch adaptive summaries** — Lane changes, probe evidence, and certification flags surface in telemetry and exports. Forward them to your observability stack to track risk trends.
2. **Keep triage light** — Start with the default dossier builder, then enrich it with domain signals (alerts, customer metadata) once you see where probes need more context.
3. **Instrument rewards** — `session_reward` and per-step judge scores explain why the runtime retried. Use them to tune prompts or escalate risky tasks to humans.
4. **Export regularly** — Nightly `arc-atlas` runs capture adaptive summaries, persona updates, and telemetry so the training team can retrain judges or students without extra plumbing.
5. **Automate guardrails** — Trigger alerts if a fingerprint stays in `escalate` for too long or if certification attempts fail repeatedly.

## Troubleshooting

<AccordionGroup>
  <Accordion title="AttributeError: 'OpenAI' object has no attribute 'responses'">
    **Problem**: You don't have access to the Responses API (required for gpt-5)

    **Solution**: Switch to the Chat Completions API for the teacher/student models or request Responses API access from OpenAI.
  </Accordion>

  <Accordion title="RIM judges returning all 0.5 scores">
    **Problem**: Reward judges can't differentiate quality

    **Solutions**:
    1. Check that responses contain substantive content (not just "yes" or "no")
    2. Verify ground truths are provided when available
    3. Ensure baseline comparisons are included
    4. Check RIM config has correct judge models
  </Accordion>

  <Accordion title="Teaching not improving student responses">
    **Problem**: Delta scores are near zero or negative

    **Solutions**:
    1. Verify teacher model is stronger than student (e.g., gpt-5 > gpt-4o-mini)
    2. Check teaching actually contains guidance (not just repeating the question)
    3. Ensure student prompt includes the teaching content
    4. Export traces and review them with the reward system to locate failure patterns before scheduling GRPO training
  </Accordion>
</AccordionGroup>
