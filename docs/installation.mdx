---
title: Installation
description: Set up ATLAS environment with validated dependencies
sidebarTitle: Installation
icon: download
---

<Note>
**Time required**: 10-15 minutes • **Difficulty**: Beginner
</Note>

**TL;DR:** Use the automated install script for your Python version, then run the smoke test in the Verification section to confirm PyTorch, vLLM, and model downloads are configured correctly.

This guide covers installing ATLAS with Python 3.11/3.12, PyTorch 2.6.0, and vLLM 0.8.3 (the high-throughput inference engine).

## System Requirements

<CardGroup cols={2}>
  <Card title="Minimum Requirements" icon="desktop">
    - 2× NVIDIA GPUs with CUDA support (for RL training)
    - 1× GPU minimum for inference only
    - 32GB+ system RAM
    - 100GB+ disk space
    - Python 3.11 or 3.12
  </Card>
  <Card title="Recommended Setup" icon="server">
    - 4×H100 or 8×H100 GPUs (40GB+ VRAM each)
    - 128GB+ system RAM
    - 200GB+ NVMe storage
    - Ubuntu 22.04 LTS
  </Card>
</CardGroup>

## Prerequisites

<Steps>
  <Step title="CUDA Setup">
    Ensure NVIDIA drivers and CUDA are installed and compatible with PyTorch 2.6.0:
    ```bash
    nvidia-smi  # Verify CUDA version
    ```
  </Step>

  <Step title="Python Environment">
    Verify Python version (3.11 or 3.12 required):
    ```bash
    python --version
    ```
  </Step>

  <Step title="HuggingFace Authentication">
    Authenticate for model and dataset access:
    ```bash
    huggingface-cli login
    ```
  </Step>
</Steps>

## Installation Methods

<Tabs>
  <Tab title="Automated (Recommended)">
    Use our validated installation scripts for the smoothest setup:

    **For Python 3.11:**
    ```bash
    bash scripts/install_py311.sh
    ```

    **For Python 3.12:**
    ```bash
    bash scripts/install_py312.sh
    ```

    These scripts automatically:
    - Install PyTorch with CUDA 12.4 support
    - Configure vLLM 0.8.3
    - Set up Flash Attention
    - Install all dependencies
  </Tab>

  <Tab title="Manual Installation">
    For custom environments or debugging:

    ```bash
    # Install PyTorch with CUDA support
    python -m pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/cu124

    # Install vLLM and TensorBoard
    python -m pip install vllm==0.8.3 tensorboard

    # Install Flash Attention (for optimal performance)
    python -m pip install flash-attn --no-build-isolation

    # Install FlashInfer
    python -m pip install flashinfer-python -i https://flashinfer.ai/whl/cu124/torch2.6/

    # Install remaining dependencies
    python -m pip install --upgrade -r requirements-py311.txt  # or requirements-py312.txt
    ```
  </Tab>

  <Tab title="Conda Environment">
    Create isolated environment with Conda:

    ```bash
    # Create environment
    conda create -n atlas python=3.11
    conda activate atlas

    # Install PyTorch
    conda install pytorch==2.6.0 pytorch-cuda=12.4 -c pytorch -c nvidia

    # Run installation script
    bash scripts/install_py311.sh
    ```
  </Tab>
</Tabs>

## Environment Configuration

### API Keys and Tracking

Configure authentication for various services:

```bash
# Required: HuggingFace for models
export HF_TOKEN="your-huggingface-token"

# Optional: Weights & Biases for experiment tracking
export WANDB_API_KEY="your-wandb-key"

# Optional: OpenAI/Gemini for online optimization
export OPENAI_API_KEY="your-openai-key"
export GEMINI_API_KEY="your-gemini-key"
```

<Note>
  The training script automatically sets `HF_HUB_ENABLE_HF_TRANSFER=1` to speed up model downloads.
</Note>

### Disable Tracking

To disable Weights & Biases tracking:

```bash
# In command line
python train.py report_to=null

# Or in config file
report_to: null
```

## Verification

After installation, verify your setup:

### 3-Minute Smoke Test

<Note>
Run this once to confirm CUDA, vLLM, and model downloads are working before you invest in longer training jobs.
</Note>

```bash
python - <<'PY'
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load teacher model
teacher = AutoModelForCausalLM.from_pretrained(
    "Arc-Intelligence/ATLAS-8B-Thinking",
    device_map="auto",
    torch_dtype=torch.float16
)
teacher_tokenizer = AutoTokenizer.from_pretrained(
    "Arc-Intelligence/ATLAS-8B-Thinking"
)

print("CUDA available:", torch.cuda.is_available())
print("GPU count:", torch.cuda.device_count())
print("Teacher model loaded:", teacher.config.model_type)
print("Model device:", next(teacher.parameters()).device)
PY
# Expected output: CUDA available: True, GPU count: 2+ (for RL training), model type shown
```

<CodeGroup>
  ```python "Quick Test"
  # Verify core dependencies
  import torch
  import transformers
  import datasets
  import vllm

  print(f"PyTorch: {torch.__version__}")
  print(f"CUDA available: {torch.cuda.is_available()}")
  print(f"GPU count: {torch.cuda.device_count()}")
  print(f"Transformers: {transformers.__version__}")
  print(f"vLLM: {vllm.__version__}")
  ```

  ```bash "CLI Verification"
  # Check accelerate installation
  accelerate --version

  # Verify CUDA
  python -c "import torch; print(torch.cuda.is_available())"

  # Test model access
  huggingface-cli download Arc-Intelligence/ATLAS-8B-Thinking \
    --include "*.json" \
    --exclude "*.safetensors"
  ```
</CodeGroup>

## GPU Memory Management

For different GPU configurations:

<AccordionGroup>
  <Accordion title="Single GPU Setup" icon="microchip">
    Single GPU is supported for inference only. For RL training, use model offloading:
    ```bash
    # Inference only with single GPU
    python examples/quickstart/evaluate.py  # Quick evaluation test

    # For training with limited VRAM (requires 2+ GPUs)
    python train.py +offload

    # Or use Zero-1 optimization
    python train.py +zero1
    ```
  </Accordion>

  <Accordion title="Multi-GPU Setup" icon="layer-group">
    For distributed training across multiple GPUs:
    ```bash
    # Minimum 2 GPUs for RL training (1 for vLLM, 1 for training)
    scripts/launch_with_server.sh 1 1 configs/run/teacher_rcl.yaml

    # Production setup with 4 GPUs (2 for vLLM, 2 for training)
    scripts/launch_with_server.sh 2 2 configs/run/teacher_rcl.yaml

    # Full 8 GPU setup
    scripts/launch_with_server.sh 4 4 configs/run/teacher_rcl.yaml
    ```
  </Accordion>

  <Accordion title="Memory Optimization" icon="memory">
    Reduce memory usage with these settings:
    ```yaml
    # In config file
    per_device_train_batch_size: 1
    gradient_checkpointing: true
    fp16: true  # or bf16 for A100/H100
    ```
  </Accordion>
</AccordionGroup>

## Security Best Practices

<Warning>
  Follow these security guidelines to protect sensitive information:
</Warning>

- **Never commit secrets**: Keep tokens, `.env` files, and API keys out of version control
- **Use environment variables**: Store `HF_TOKEN`, `WANDB_API_KEY`, etc. as environment variables
- **Gitignore protection**: Ensure `results/`, `logs/`, `wandb/` remain in `.gitignore`
- **Least privilege**: Restrict dataset access permissions
- **Logout on shared machines**: Run `huggingface-cli logout` after use

## Platform-Specific Notes

<Tabs>
  <Tab title="Linux">
    Tested on Ubuntu 20.04/22.04 LTS:
    - Ensure CUDA toolkit matches PyTorch requirements
    - May need `sudo` for system package installations
  </Tab>

  <Tab title="macOS">
    Limited support for Apple Silicon:
    - CPU-only mode available
    - Use MPS backend where supported
    - vLLM may not be available
  </Tab>

  <Tab title="Windows WSL2">
    Run through WSL2 for best compatibility:
    - Install CUDA toolkit in WSL2
    - Use Linux installation instructions
    - Ensure WSL2 has GPU passthrough enabled
  </Tab>
</Tabs>

## Troubleshooting

<AccordionGroup>
  <Accordion title="CUDA Version Mismatch" icon="triangle-exclamation">
    If you see CUDA errors:
    ```bash
    # Check CUDA version
    nvidia-smi
    nvcc --version

    # Reinstall PyTorch with correct CUDA version
    pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/cu118  # For CUDA 11.8
    ```
  </Accordion>

  <Accordion title="Out of Memory Errors" icon="memory">
    Reduce memory usage:
    ```bash
    # Use gradient checkpointing
    python train.py gradient_checkpointing=true

    # Reduce batch size
    python train.py per_device_train_batch_size=1

    # Enable CPU offloading
    python train.py +offload
    ```
  </Accordion>

  <Accordion title="HuggingFace Access Denied" icon="lock">
    Ensure proper authentication:
    ```bash
    # Re-authenticate
    huggingface-cli logout
    huggingface-cli login

    # Verify token
    huggingface-cli whoami
    ```
  </Accordion>

  <Accordion title="vLLM Installation Fails" icon="xmark">
    Common vLLM issues:
    ```bash
    # Install build dependencies
    sudo apt-get install python3-dev

    # Try pre-built wheel
    pip install https://github.com/vllm-project/vllm/releases/download/v0.8.3/vllm-0.8.3-cp311-cp311-linux_x86_64.whl
    ```
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Quickstart"
    icon="rocket"
    href="/quickstart"
  >
    Deploy ATLAS with pre-trained models
  </Card>
  <Card
    title="First Experiment"
    icon="flask"
    href="/first-experiment"
  >
    Run your first ATLAS training experiment
  </Card>
</CardGroup>
