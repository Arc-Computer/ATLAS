---
title: Custom Implementation Guide
description: Build your own ATLAS integration for domain-specific tasks
sidebarTitle: Custom Implementation
icon: code
---

## Overview

This guide shows how to integrate ATLAS into your applications, create custom evaluators, and adapt the system for domain-specific tasks. The pattern is simple: load models, run the protocol, evaluate results.

![Adaptive Teaching Process](/images/adaptive-teaching.png)

## Core Integration Pattern

The fundamental ATLAS protocol works with any models via API calls:

```python
from openai import OpenAI
from RIM.reward_adapter import RIMReward

client = OpenAI()
reward = RIMReward(config_path='configs/rim_config.yaml')

def run_atlas_protocol(task: str, teacher_model: str = "gpt-5", student_model: str = "gpt-4o-mini"):
    """Run the full ATLAS teaching protocol."""

    # Step 1: Get baseline student response
    baseline_response = client.responses.create(
        model=student_model,
        input=[{"role": "user", "content": task}],
        max_output_tokens=512
    ).output[0].text

    # Step 2: Teacher provides guidance
    teaching_response = client.responses.create(
        model=teacher_model,
        input=[{
            "role": "user",
            "content": f"Task: {task}\n\nStudent attempt:\n{baseline_response}\n\nProvide focused teaching in <teaching> tags."
        }],
        max_output_tokens=200
    ).output[0].text

    # Step 3: Student applies teaching
    enhanced_response = client.responses.create(
        model=student_model,
        input=[{
            "role": "user",
            "content": f"Task: {task}\n\nTeaching:\n{teaching_response}\n\nApply the teaching step-by-step."
        }],
        max_output_tokens=512
    ).output[0].text

    # Step 4: Measure improvement with RIM
    baseline_eval = reward.evaluate(prompt=task, response=baseline_response)
    enhanced_eval = reward.evaluate(
        prompt=task,
        response=enhanced_response,
        baseline_solutions=baseline_response,
        teacher_traces=teaching_response
    )

    return {
        'baseline_response': baseline_response,
        'guided_response': enhanced_response,
        'teaching': teaching_response,
        'baseline_score': baseline_eval.score,
        'enhanced_score': enhanced_eval.score,
        'improvement_score': enhanced_eval.score - baseline_eval.score
    }

# Use it
task = "Debug intermittent 503 errors in production Kubernetes cluster"
result = run_atlas_protocol(task)

print(f"Improvement: {result['improvement_score']:+.3f}")
print(f"Enhanced response:\n{result['guided_response']}")
```

<Note>
For Chat Completions API (gpt, Claude, etc.), replace `client.responses.create()` with `client.chat.completions.create()`. See [inference-only integration](/integration/inference-only) for fallback patterns.
</Note>

## Building Domain-Specific Evaluators

<Steps>
  <Step title="Define Success Metrics">
    Create evaluators that understand your domain:

    ```python
    class DomainEvaluator:
        """
        Custom evaluator for your specific domain
        """

        def __init__(self, domain_config):
            self.config = domain_config
            self.success_patterns = self.load_patterns()

        def evaluate(self, response, ground_truth=None):
            """
            Evaluate response quality for your domain

            Returns:
                score: float between 0 and 1
                details: dict with specific metrics
            """
            score = 0.0
            details = {}

            # Domain-specific checks
            if self.has_required_structure(response):
                score += 0.3
                details['structure'] = True

            if self.contains_key_concepts(response):
                score += 0.3
                details['concepts'] = True

            if self.validates_constraints(response):
                score += 0.4
                details['valid'] = True

            return score, details
    ```
  </Step>

  <Step title="Implement Diagnostic Strategies">
    Customize how the teacher assesses student capability:

    ```python
    class CustomDiagnostic:
        """
        Domain-specific diagnostic probing
        """

        def create_probe(self, task):
            """
            Generate diagnostic questions for your domain
            """
            task_type = self.classify_task(task)

            probes = {
                'technical': "What's your approach to solving this?",
                'analytical': "What data would you need?",
                'creative': "What constraints should we consider?",
                'debugging': "What would you check first?"
            }

            return probes.get(task_type, "How would you approach this?")

        def assess_response(self, response):
            """
            Evaluate student's diagnostic response
            """
            indicators = {
                'understanding': self.check_comprehension(response),
                'methodology': self.check_approach(response),
                'expertise': self.check_domain_knowledge(response)
            }

            # Map to capability level
            score = sum(indicators.values()) / len(indicators)

            if score < 0.3:
                return 'weak', score
            elif score < 0.7:
                return 'moderate', score
            else:
                return 'strong', score
    ```
  </Step>

  <Step title="Create Teaching Strategies">
    Define how guidance adapts to student capability:

    ```python
    class AdaptiveTeachingStrategy:
        """
        Domain-specific teaching strategies
        """

        def generate_guidance(self, task, capability_level, diagnostic_details):
            """
            Create targeted teaching based on assessment
            """
            strategies = {
                'weak': self.comprehensive_guidance,
                'moderate': self.targeted_guidance,
                'strong': self.minimal_guidance
            }

            strategy_func = strategies.get(capability_level, self.targeted_guidance)
            return strategy_func(task, diagnostic_details)

        def comprehensive_guidance(self, task, details):
            """Full scaffolding for weak students"""
            return f"""
            Let's break this down step-by-step:
            1. First, understand the problem: {self.explain_problem(task)}
            2. Identify key components: {self.identify_components(task)}
            3. Apply this methodology: {self.suggest_approach(task)}
            4. Check your work: {self.verification_steps(task)}
            """

        def targeted_guidance(self, task, details):
            """Strategic hints for moderate students"""
            missing = self.identify_gaps(details)
            return f"""
            Key insight: {self.provide_missing_piece(task, missing)}
            Consider: {self.suggest_improvement(task)}
            """

        def minimal_guidance(self, task, details):
            """Light touch for strong students"""
            edge_case = self.find_edge_case(task)
            if edge_case:
                return f"Don't forget: {edge_case}"
            return ""
    ```
  </Step>

  <Step title="Integrate with Production Systems">
    Deploy ATLAS in your application:

    ```python
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    import asyncio

    app = FastAPI()

    class TaskRequest(BaseModel):
        task: str
        use_teaching: bool = True
        return_diagnostics: bool = False

    class ATLASService:
        def __init__(self):
            self.client = OpenAI()
            self.reward = RIMReward(config_path='configs/rim_config.yaml')
            self.evaluator = DomainEvaluator(your_config)
            self.cache = {}
            self.teacher_model = "gpt-5"
            self.student_model = "gpt-4o-mini"

        async def process_task(self, request: TaskRequest):
            """
            Process task with optional teaching enhancement
            """
            # Check cache
            cache_key = hash(request.task)
            if cache_key in self.cache:
                return self.cache[cache_key]

            # Run ATLAS protocol
            if request.use_teaching:
                result = await self.run_with_teaching(request.task)
            else:
                result = await self.run_baseline(request.task)

            # Evaluate quality
            score, details = self.evaluator.evaluate(result['response'])

            # Cache if high quality
            if score > 0.8:
                self.cache[cache_key] = result

            # Add diagnostics if requested
            if request.return_diagnostics:
                result['diagnostics'] = {
                    'capability': result.get('capability_level'),
                    'strategy': result.get('teaching_strategy'),
                    'score': score,
                    'details': details
                }

            return result

    # Initialize service
    atlas_service = ATLASService()

    @app.post("/enhance")
    async def enhance_task(request: TaskRequest):
        """API endpoint for task enhancement"""
        try:
            result = await atlas_service.process_task(request)
            return result
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    ```
  </Step>
</Steps>

## Domain-Specific Patterns

### Pattern 1: Code Generation

```python
class CodeGenerationATLAS:
    """
    ATLAS for code generation tasks using the teaching protocol
    """

    def __init__(self):
        self.client = OpenAI()
        self.reward = RIMReward(config_path='configs/rim_config.yaml')

    def enhance_code_generation(self, specification):
        """
        Generate code with teaching enhancement
        """
        # Add context for better teaching
        prompt = f"""
        Task: {specification}
        Requirements:
        - Include error handling
        - Follow best practices
        - Add type hints
        - Include docstrings
        """

        # Run ATLAS protocol
        result = run_atlas_protocol(prompt, teacher_model="gpt-5", student_model="gpt-4o-mini")

        # Post-process to extract code
        code = self.extract_code_blocks(result['guided_response'])

        # Validate syntax
        if self.validate_syntax(code):
            return code
        else:
            # Retry with more specific guidance
            return self.retry_with_corrections(specification, code, result['teaching'])

    def validate_syntax(self, code):
        """Check if generated code is syntactically valid"""
        try:
            compile(code, '<string>', 'exec')
            return True
        except SyntaxError:
            return False
```

### Pattern 2: Data Analysis

```python
class DataAnalysisATLAS:
    """
    ATLAS for data analysis tasks
    """

    def __init__(self):
        self.client = OpenAI()

    def enhance_analysis(self, data_description, question):
        """
        Enhance analytical reasoning
        """
        # Structure the task for optimal teaching
        task = f"""
        Data Context: {data_description}
        Analysis Question: {question}

        Required approach:
        1. State assumptions
        2. Choose appropriate methods
        3. Interpret results
        4. Address limitations
        """

        result = run_atlas_protocol(task, teacher_model="gpt-5", student_model="gpt-4o-mini")

        # Extract structured insights
        analysis = self.parse_analysis(result['guided_response'])

        return {
            'assumptions': analysis.get('assumptions', []),
            'methodology': analysis.get('methods', ''),
            'findings': analysis.get('findings', ''),
            'limitations': analysis.get('limitations', []),
            'confidence': self.assess_confidence(analysis)
        }
```

### Pattern 3: Debugging Assistant

```python
class DebuggingATLAS:
    """
    ATLAS for systematic debugging
    """

    def __init__(self):
        self.client = OpenAI()

    def debug_issue(self, error_description, context):
        """
        Systematic debugging with teaching
        """
        # Structure for systematic investigation
        task = f"""
        Error: {error_description}
        System Context: {context}

        Debug systematically:
        1. Identify error pattern
        2. Form hypotheses
        3. Design tests
        4. Find root cause
        """

        result = run_atlas_protocol(task, teacher_model="gpt-5", student_model="gpt-4o-mini")

        # Parse debugging steps
        debug_plan = self.parse_debug_plan(result['guided_response'])

        return {
            'hypotheses': debug_plan['hypotheses'],
            'tests': debug_plan['test_commands'],
            'root_cause': debug_plan['root_cause'],
            'fix': debug_plan['suggested_fix'],
            'prevention': debug_plan['prevention_measures']
        }
```

## Performance Optimization

### Caching Strategies

```python
import hashlib
import time
import json
from openai import OpenAI

class CachedATLAS:
    """
    ATLAS with intelligent caching
    """

    def __init__(self):
        self.client = OpenAI()
        self.result_cache = {}
        self.ttl = 3600  # 1 hour

    def get_cached_result(self, task):
        """
        Cache full ATLAS protocol results for similar tasks
        """
        # Create semantic hash
        task_hash = self.semantic_hash(task)

        # Check cache
        if task_hash in self.result_cache:
            cached = self.result_cache[task_hash]
            if time.time() - cached['timestamp'] < self.ttl:
                return cached['result']

        # Run full protocol
        result = run_atlas_protocol(task)

        # Cache it
        self.result_cache[task_hash] = {
            'result': result,
            'timestamp': time.time()
        }

        return result

    def semantic_hash(self, task):
        """
        Create hash based on semantic content
        """
        # Extract key features
        features = {
            'length': len(task),
            'keywords': self.extract_keywords(task),
            'complexity': self.estimate_complexity(task)
        }

        # Create stable hash
        feature_str = json.dumps(features, sort_keys=True)
        return hashlib.md5(feature_str.encode()).hexdigest()
```

### Batch Processing

```python
import asyncio

async def batch_enhance(tasks, batch_size=4):
    """
    Process multiple tasks efficiently using ATLAS protocol
    """
    results = []

    for i in range(0, len(tasks), batch_size):
        batch = tasks[i:i + batch_size]

        # Process batch in parallel
        batch_results = await asyncio.gather(*[
            asyncio.to_thread(run_atlas_protocol, task) for task in batch
        ])

        results.extend(batch_results)

    return results
```

## Monitoring and Analytics

Track your custom implementation's performance:

```python
class ATLASAnalytics:
    """
    Track ATLAS performance in production
    """

    def __init__(self):
        self.metrics = {
            'total_requests': 0,
            'improvements': [],
            'degradations': [],
            'teaching_strategies': {},
            'average_latency': 0
        }

    def track_result(self, result):
        """
        Track enhancement results
        """
        self.metrics['total_requests'] += 1

        improvement = result['improvement_score']
        if improvement > 0:
            self.metrics['improvements'].append(improvement)
        elif improvement < 0:
            self.metrics['degradations'].append(improvement)

        strategy = result.get('teaching_strategy', 'unknown')
        self.metrics['teaching_strategies'][strategy] = \
            self.metrics['teaching_strategies'].get(strategy, 0) + 1

    def get_summary(self):
        """
        Generate performance summary
        """
        return {
            'requests': self.metrics['total_requests'],
            'avg_improvement': np.mean(self.metrics['improvements']),
            'non_degradation_rate': len(self.metrics['improvements']) /
                                   max(1, len(self.metrics['improvements']) +
                                       len(self.metrics['degradations'])),
            'strategy_distribution': self.metrics['teaching_strategies']
        }
```

## Best Practices

<AccordionGroup>
  <Accordion title="Model Selection">
    Choose models based on your domain:
    - **Technical tasks**: Use ATLAS-8B-Instruct
    - **Reasoning tasks**: Use ATLAS-8B-Thinking
    - **Code tasks**: Pair with CodeLlama or StarCoder
    - **Domain-specific**: Fine-tune on your data
  </Accordion>

  <Accordion title="Error Handling">
    Implement robust error handling for API calls:
    ```python
    from openai import OpenAI, APIError

    def safe_atlas_protocol(task):
        try:
            result = run_atlas_protocol(task)
            return result
        except APIError as e:
            # Handle API errors
            logger.error(f"API error: {e}")
            # Fall back to baseline without teaching
            client = OpenAI()
            baseline = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": task}]
            ).choices[0].message.content
            return {'guided_response': baseline, 'improvement_score': 0}
        except Exception as e:
            logger.error(f"ATLAS failed: {e}")
            raise
    ```
  </Accordion>

  <Accordion title="Performance Tuning">
    Optimize for your use case:
    - Cache guidance for repeated task types
    - Use batch processing for throughput
    - Implement selective enhancement for simple tasks
    - Monitor token usage to control costs
  </Accordion>

  <Accordion title="Testing Strategy">
    Test your implementation thoroughly:
    ```python
    from RIM.reward_adapter import RIMReward

    def test_enhancement():
        reward = RIMReward(config_path='configs/rim_config.yaml')
        test_cases = load_test_cases()

        for case in test_cases:
            result = run_atlas_protocol(case['input'])

            # Score with RIM
            eval_result = reward.evaluate(
                prompt=case['input'],
                response=result['guided_response'],
                ground_truths=[case['expected']]
            )

            assert eval_result.score > case['min_score'], \
                   f"Failed: {case['name']}"
            assert result['improvement_score'] >= 0, \
                   "Degradation detected"
    ```
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Offline Training" icon="dumbbell" href="/training/offline/grpo-training">
    Train custom teacher models
  </Card>
  <Card title="Online Optimization" icon="bolt" href="/training/online/optimize-with-atlas">
    Optimize for your domain
  </Card>
  <Card title="Production Deployment" icon="rocket" href="/integration/production-deployment">
    Deploy at scale
  </Card>
  <Card title="API Reference" icon="code" href="/api-reference/atlas-inference">
    Complete API documentation
  </Card>
</CardGroup>